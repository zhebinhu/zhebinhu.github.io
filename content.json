{"meta":{"title":"huzb的小书斋","subtitle":"念念不忘，必有回响","description":"一枚前行的小码农","author":"huzb","url":"http://yoursite.com"},"pages":[{"title":"关于","date":"2019-03-09T04:59:43.829Z","updated":"2019-03-09T04:59:43.828Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"12345678910111213141516171819202122/** * $= * =@.$ * $- $ &amp;@ =&amp;$$$$$$$$$. * $ B$- =&amp; $#&amp;$$$&amp;- -$$$$$$$$$$$&amp; 电子科技大学计算机在读，一枚奋进的Java程序员 * $ $$ .#@$$$@#. .$$$$$$$$$#$$ 念念不忘，必有回响 * .$ @$$$$$$$$$$$ * .$ B$$$# $$$$$$$$$$ * -$ %@. $$$$$$$ * B$ =$$$= * $$ .%$$&amp;= * $&amp; $- * $- @% * &amp;$ %$ * $% B$ * =$ #$ * $$ =$ * -$. $ =$ * $$ #@ B$ * .$- &amp;% @$ * $@ $= $% */"},{"title":"Repositories","date":"2019-03-08T00:40:27.819Z","updated":"2019-03-08T00:40:27.819Z","comments":false,"path":"repository/index.html","permalink":"http://yoursite.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-03-08T00:40:27.822Z","updated":"2019-03-08T00:40:27.822Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"redo log,undo log 和 binlog","slug":"redo-undo和binlog","date":"2019-04-24T06:38:43.000Z","updated":"2019-04-26T13:57:20.076Z","comments":true,"path":"2019/04/24/redo-undo和binlog/","link":"","permalink":"http://yoursite.com/2019/04/24/redo-undo和binlog/","excerpt":"","text":"MySQL 中有很多日志，其中 redo log,undo log 和 binlog 是使用比较多的日志。redo log 和 undo log 是 innodb 层面的日志，用于实现数据库引擎中的事务。其中，redo log 保证了事务的持久性，undo log 保证了事务的一致性，redo log 和 undo log 共同保证了事务的原子性。binlog 是 mysql server 层面的日志，它主要用于主库和从库之间的数据同步。下面我们来依次介绍一下这三种日志： 1、binlogbinlog 中记录了对 MySQL 数据库执行更改的所有操作，但不包括 SELECT 和 SHOW 这类操作，因为这类操作对数据本身没有修改。在 innodb 中存储引擎会为每个事务分配一个默认大小为 32K 的缓冲区，事务在提交之前会把二进制日志写入缓冲区中，等事务提交时再将缓冲区中的日志写入文件中。但写入文件不代表刷新到硬盘，因为操作系统对写操作会有一个缓冲。通过修改 sync_binlog = [N] 可以设置每提交多少次事务就强制刷盘。默认 sync_binlog = 0，表示刷盘时机由操作系统决定。 binlog 中有三种格式： STATEMENT：记录的是每一条会修改数据的 SQL 语句。优点是体积小，缺点是无法记录特定函数，比如 UUID()、USER() 等。 ROW：记录的是每一条被修改的记录行的修改情况。优点是不会再有无法记录特定函数的问题，缺点是体积大。 MIXED：此格式下默认采用 STATEMENT 格式进行记录，在特殊情况（比如涉及到无法记录的特定函数时）下，会采用 ROW 的格式记录。 目前版本（5.7）下默认使用 ROW 格式来记录二进制日志。 1.1 主从复制binlog 可以实现 MySQL 主从服务器之间的复制功能，具体有以下几步： 在主服务器上创建一个具有复制权限的用户。 依次在主从服务器配置唯一的 serverid。 从服务器设置连接主服务器的信息，然后执行start slave，开启主从复制开关。 从服务器会创建两个线程：IO 线程和 SQL 线程。IO 线程会通过主服务器上授权的有复制权限的用户请求连接主服务器，并请求从指定 binlog 日志文件的指定位置之后发送 binlog 日志内容。（日志文件名和位置在上一步设置连接信息时已指定） 主服务器接收到来自从服务器的请求后，会创建一个专门的 IO 线程，此 IO 线程会根据从服务器的 IO 线程请求的信息，读取指定 binlog 日志文件指定位置之后的 binlog 日志信息，然后返回给从端的 IO 线程。返回的信息中除了 binlog 日志内容外，还有本次返回日志内容后在主服务器端的新的 binlog 文件名以及在 binlog 中的下一个指定更新位置。 当从服务器的 IO 线程获取来自主服务器上 IO 线程发送的日志内容及日志文件和位置点后，将 binlog 日志内容依次写入到从端自身的 relay log（即中继日志）文件的最末端，并将新的 binlog 文件名和位置记录到 master-info 文件中，以便下一次读取主端新 binlog 日志时，能告诉主服务器需要从新 binlog 日志的哪个文件哪个位置开始请求新的 binlog 日志内容。 从服务器端的 SQL 线程会实时检测本地 relay log 中新增加的日志内容，然后根据日志内容更新从库的数据。到此一轮复制操作就完成了。 2、redo logredo log 由两部分组成：一是内存中的重做缓存日志（redo log buffer），其是易失的；二是重做日志文件（redo log file），其是持久的。redo log 通过 Write Ahead Log 和 Force Log at Commit 机制来实现事务的持久性，即当修改内存中的数据页时，先修改内存中的日志；当事务提交时，必须先将该事务的所有 redo log 持久化。在这两个机制之下，当系统发生宕机时，redo log 保证了：如果一个事务的 redo log 已经全部刷入磁盘，那么该事务一定可以恢复；如果一个事务的 redo log 没有全部刷入磁盘，那么就通过 undo log 将这个事务恢复到执行之前。 redo log 中记录的是每一次修改的物理日志，即数据库中每个页的修改，这个页既包括聚簇索引，也包括二级索引。举个例子，当我们执行 SQL 语句 INSERT INTO t VALUES(1,2) 时，其记录的重做日志大致为： page(2,3), offset 32, value 1,2 # 聚簇索引page(2,4), offset 64, value 2 # 二级索引 可以看到 redo log 中的记录是物理的，记录的是在哪个数据页偏移量多少的地方写入什么值，同时这种记录也是幂等的，也就是说无论执行多少次恢复操作，最终的结果都是一样的。而 binlog 就不一样，尽管 binlog 也可以按行记录，但这种记录是逻辑的。比如对于插入操作而言，它的记录可能是：在xx行插入一条xxx的数据。对这条记录而言，重复执行就会插入多条重复数据。 除此之外，两种日志记录写入磁盘的时间点也不一样。每个事务会为二进制日志分配一个缓冲区，缓冲区中的日志只在当前事务提交时一次性刷入磁盘；而 redo log 的缓冲区由所有事务共享，缓冲区中的数据刷入磁盘的时机受很多条件的影响。这表现为 redo log 并不按事务提交的顺序记录日志。如图所示： T1、T2、T3 表示事务的记录。T1、T2、*T3表示事务提交时的日志。T1 的提交发生在 T2之后，但 T2 提交时会把已经记录的 T1 相关的部分刷到磁盘。那么 redo log 何时刷入磁盘呢？具体有三个时机： 有事务提交时 当 log buffer 中有一半的内存空间已经被使用时 checkpoint 时（checkpoint 会将内存中的部分脏页刷入磁盘，要确保脏页在刷入磁盘之前对应的 redo log 已经刷盘完成） 为了确保每次日志都能写入到事务日志文件中，在每次将 redo log buffer 中的日志写入日志文件的过程中都会调用一次操作系统的 fsync 操作。 2.1 LSN数据库宕机后可以根据 redo log 恢复，但并不是所有数据都需要恢复。在宕机之前就已经刷入到磁盘的数据可以不用恢复。这个判断是由 LSN 来完成的。LSN 表示的是日志序列号，它是一个单调递增的 8 字节数字，代表的是事务写入 redo log 的字节的总量。例如当前 redo log 的 LSN 为 1000，事务 T1 写入了 100 字节到 redo log，那么 LSN 就变成了 1100，若又有事务 T2 写入了 200 字节到 redo log，那么 LSN 就变成了 1300。可以看出，LSN 相当于游标，指示了 redo log 中的某个位置。 在 innodb 中有以下几种 LSN： Log sequence number：当前写入 redo log 的总量 Log flushed up to：当前刷入磁盘的 redo log 的总量 FIL_PAGE_LSN：存在于每个数据页的头部，表示该页最后刷新时 LSN 的大小，通过比较这个参数可以判断该页刷新的时间 Last checkpoint at：上次脏页刷盘后，内存中仍存在的脏页中最小的 LSN。这个参数的意思是，页 LSN 小于该 LSN 的数据页都已经刷入了磁盘（但不代表大于该 LSN 的页都没有刷入，redo log 的幂等性确保了重复恢复的一致性），该参数会保存两份交替写入，避免了因介质失败而导致无法找到可用的 checkpoint。 当数据恢复时，只需要应用 checkpoint 之后的日志，且对于某一页，只需要应用页 LSN 之后的日志。这样加快了恢复的速度。而且 redo log 中小于 checkpoint 的部分可以写入新的数据，循环利用，节省空间。 3、undo logundo log 的设计目的在于回滚，在 innodb 中还顺带实现了 MVCC 的功能。undo log 位于共享表空间中，事务需要在 undo log segment 中申请相应的页并写入。这个过程同样需要写入 redo log。undo log 的逻辑结构如下： T0、T1、T2 表示创建 undo log 的事务。undo log 中保存的是每一行数据在每一个事务版本下的逻辑数据。而恢复也很简单，就是将某个版本的数据设置为当前数据就好。这里可能会有一个问题，就是数据库中可能同时有数千个并发的事务，当我们回滚某一个事务时，如何保障其它事务的修改仍然有效？答案很简单，当我们回滚某一个事务时，该事物还未提交，而如果该事务还未提交，则当前数据行会加锁，因此对某一行数据而言，不存在回滚会同时擦除其它事务修改的可能。而对整个数据页而言，如果使用物理日志，就会发生这种擦除的情况。因此 undo log 使用逻辑日志。 事务为 undo log 分配的页不会一直存在，当事务提交时，undo log 就没有了回滚的价值。但仍不能立马删除 undo log 的所在页，因为可能还有其它事务通过 MVCC 访问之前的版本。故事务提交时将 undo log 放入一个链表中，是否可以最终删除 undo log 及 undo log 所在页由 purge 线程判断。 4、事务写入过程1、事务开启2、undo log’s redo log 写入3、undo log 写入4、redo log 写入5、数据页写入6、redo log 刷盘7、2-6 重复若干8、事务提交9、某个时间脏页刷盘 5、崩溃恢复过程1、找到持久化的 checkpoint2、从 checkpoint 向后恢复，跳过页 LSN 大于当前 LSN 的页，通过 redo log 也可以恢复 undo log3、如果页已损坏，则在共享表空间的 doublewrite 找到对应的页，恢复4、通过 undo log 回滚未提交的事务 6、binlog 和 redo log 的一致性MySQL 主从复制之间依赖 binlog，而 binlog 文件的写入在 commit 之前，如果写完 binlog 文件后主库宕机，再次启动时会回滚事务。但此时从库已经执行，则会造成主备数据不一致。所以在开启binlog 后，如何保证 binlog 和 redo log 的一致性呢？为此，MySQL 引入二阶段提交（two phase commit or 2pc），MySQL 内部会自动将普通事务当做一个 XA 事务（内部分布式事物）来处理： – 自动为每个事务分配一个唯一的ID（XID）。 – COMMIT 会被自动的分成 Prepare 和 Commit 两个阶段。 – Binlog 会被当做事务协调者(Transaction Coordinator)，Binlog 的每条日志会被当做协调者日志。 Binlog 在 2PC 中充当了事务的协调者（Transaction Coordinator）。由 Binlog 来通知 InnoDB 引擎来执行 prepare，commit 或者 rollback 的步骤。事务提交的整个过程如下： 以上的图片中可以看到，事务的提交主要分为两个主要步骤： 1、准备阶段（Storage Engine（InnoDB） Transaction Prepare Phase） 此时SQL已经成功执行，并生成xid信息及redo和undo的内存日志。然后调用prepare方法完成第一阶段，papare方法实际上什么也没做，将事务状态设为TRX_PREPARED，并将redo log刷磁盘。 2、提交阶段(Storage Engine（InnoDB）Commit Phase) 2.1 记录协调者日志，即Binlog日志。 如果事务涉及的所有存储引擎的prepare都执行成功，则调用TC_LOG_BINLOG::log_xid方法将SQL语句写到binlog（write()将binary log内存日志数据写入文件系统缓存，fsync()将binary log文件系统缓存日志数据永久写入磁盘）。此时，事务已经铁定要提交了。否则，调用ha_rollback_trans方法回滚事务，而SQL语句实际上也不会写到binlog。 2.2 告诉引擎做commit。 3、最后，调用引擎的commit完成事务的提交。会清除undo信息，刷redo日志，将事务设为TRX_NOT_STARTED状态。 由上面的二阶段提交流程可以看出，一旦步骤 2 中的操作完成，就确保了事务的提交，即使在执行步骤 3 时数据库发送了宕机。此外需要注意的是，每个步骤都需要进行一次 fsync 操作才能保证上下两层数据的一致性。步骤 2 的 fsync 参数由 sync_binlog=1 控制，步骤 3 的 fsync 由参数 innodb_flush_log_at_trx_commit=1 控制，俗称“双1”，是保证日志一致性的根本。 事务的两阶段提交协议保证了无论在任何情况下，事务要么同时存在于存储引擎和 binlog 中，要么两个里面都不存在，这就保证了主库与从库之间数据的一致性。如果数据库系统发生崩溃，当数据库系统重新启动时会进行崩溃恢复操作，存储引擎中处于 prepare 状态的事务会去查询该事务是否也同时存在于 binlog 中，如果存在就在存储引擎内部提交该事务（因为此时从库可能已经获取了对应的 binlog 内容），如果 binlog 中没有该事务，就回滚该事务。例如：当崩溃发生在第一步和第二步之间时，明显处于 prepare 状态的事务还没来得及写入到 binlog 中，所以该事务会在存储引擎内部进行回滚，这样该事务在存储引擎和 binlog 中都不会存在；当崩溃发生在第二步和第三步之间时，处于 prepare 状态的事务存在于 binlog 中，那么该事务会在存储引擎内部进行提交，这样该事务就同时存在于存储引擎和 binlog 中。 7、参考资料MySQL 中Redo与Binlog顺序一致性问题","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"Spring AOP 源码浅析——事务的实现","slug":"Spring-AOP源码浅析——事务的实现","date":"2019-03-28T08:03:54.000Z","updated":"2019-04-03T14:42:35.666Z","comments":true,"path":"2019/03/28/Spring-AOP源码浅析——事务的实现/","link":"","permalink":"http://yoursite.com/2019/03/28/Spring-AOP源码浅析——事务的实现/","excerpt":"","text":"Spring AOP 的诸多应用中，事务无疑是最常使用的工具之一。对于 Spring 事务的有些特性我们很熟悉，比如事务的四大特性 ACID，比如具体的实现委托给底层 DB。但有些特性我们又有些陌生，比如事务传播行为，事务实现的原理。这也是这篇文章要阐述的内容。 一、基础知识1.1 传播行为传播行为就是指在一个方法中调用另一个声明了事务的方法，被调用方法的事务的执行策略，先看两段代码：12345678910public void service()&#123; serviceA(); serviceB();&#125;@TransactionalserviceA();@TransactionalserviceB(); 1234567891011@Transactionalpublic void service()&#123; serviceA(); serviceB();&#125;@TransactionalserviceA();@TransactionalserviceB(); 如上，当我们在其它地方调用开启了事务的 serviceA() 和 serviceB() 时，就发生了事务的传播。这里面会有很多种情况，比如调用方当前存不存在事务，如果存在事务的话，是要加入当前的事务还是自己创建一个；如果不存在的话，是自己创建一个还是抛异常。这些策略的选择，统称为 Spring 的事务传播行为。 org.springframework.transaction.annotation 包下的枚举类 Propagation 为 Spring 定义了七种传播行为，我们简单了解一下： 事务传播行为 说明 PROPAGATION_REQUIRED 支持当前事务，如果不存在事务，创建一个事务，这是默认的传播属性值。 PROPAGATION_SUPPORTS 支持当前事务，如果不存在事务，则在无事务环境执行。 PROPAGATION_MANDATORY 支持当前事务，如果不存在事务，则抛出异常。 PROPAGATION_REQUIRES_NEW 不支持当前事务，挂起当前事务，创建一个新事务。 PROPAGATION_NOT_SUPPORTED 不支持当前事务，挂起当前事务，在无事务环境上执行。 PROPAGATION_NEVER 不支持当前事务，如果当前存在事务，则抛出异常，否则在无事务环境上执行。 PROPAGATION_NESTED 如果当前存在事务，则嵌套事务执行 这里解释一下挂起和嵌套的概念，假设我们有如下代码：123456789101112131415@Transactionalpublic void service()&#123; do sql serviceA(); serviceB(); 1/0;&#125;@Transactional(propagation=Propagation.NESTED)serviceA()&#123; do sql&#125;@Transactional(propagation=Propagation.NEW)serviceB()&#123; do sql&#125; 很明显，1/0 会抛出异常，然后事务回滚。看 propagation 的值可以知道：serviceA 是嵌套事务执行，而 serviceB 是挂起后新建事务执行。那么当 service 回滚时，嵌套在内层的事务会随着外层事务的提交而提交，随着外层事务的回滚而回滚，而挂起后新建的事务本身看做一个独立的事务，不会受外部事务的提交和回滚影响。而当 serviceA 内部发生错误，需要回滚时，嵌套在内层的事务不会引起外层事务的回滚，同样，挂起后新建的事务回滚也不会引起挂起的事务回滚。 1.2 顶层接口Spring 框架中，最重要的事务管理的接口有三个：TransactionDefinition、PlatformTransactionManager 和 TransactionStatus。 所谓事务管理，实质上就是按照给定的事务规则来执行提交或者回滚操作。其中，“给定的事务规则”是用 TransactionDefinition 表示的，“按照……来执行提交或者回滚操作”是用 PlatformTransactionManager 表示的，而 TransactionStatus 可以看作代表事务本身。 1.2.1 PlatformTransactionManagerSpring 事务策略是通过 PlatformTransactionManager 接口体现的，该接口是 Spring 事务策略的核心。该接口的源代码如下：1234567891011public interface PlatformTransactionManager &#123; // 平台无关的获得事务的方法 TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // 平台无关的事务提交方法 void commit(TransactionStatus status) throws TransactionException; // 平台无关的事务回滚方法 void rollback(TransactionStatus status) throws TransactionException;&#125; 可以看出，PlatformTransactionManager 是一个与任何事务策略分离的接口。PlatformTransactionManager 接口有许多不同的实现类，应用程序面向与平台无关的接口编程，而对不同平台的底层支持由 PlatformTransactionManager 接口的实现类完成，故而应用程序无须与具体的事务 API 耦合。因此使用 PlatformTransactionManager 接口，可将代码从具体的事务 API 中解耦出来。以下是几个常用的实现类： 实现类 说明 org.springframework.jdbc.datasource.DataSourceTransactionManager 使用 spring jdbc 或 ibatis、mybatis 进行持久化数据时使用 org.springframework.orm.hibernate3.HibernateTransactionManager 使用 Hibernate 进行持久化数据时使用 在PlatformTransactionManager 接口内，包含一个 getTransaction（TransactionDefinition definition）方法，该方法根据一个 TransactionDefinition 参数，返回一个 TransactionStatus 对象。TransactionStatus 对象表示一个事务，该事务可能是一个新的事务，也可能是一个已经存在的事务对象，这由 TransactionDefinition 所定义的事务规则所决定。 1.2.2 TransactionDefinition我们通过@Transaction 声明事务时，最终都转换成 TransactionDefinition 来表示。TransactionDefinition 接口用于定义一个事务的规则，它包含了事务的一些静态属性，比如：事务传播行为、超时时间等。同时，Spring 还为我们提供了一个默认的实现类：DefaultTransactionDefinition，该类适用于大多数情况。如果该类不能满足需求，可以通过实现 TransactionDefinition 接口来实现自己的事务定义。接口的定义如下：12345678910public interface TransactionDefinition&#123; // 获取事务隔离级别，和数据库隔离界别一个概念 int getIsolationLevel(); // 获取事务传播行为，上文介绍过了 int getPropagationBehavior(); // 获取超时时间，超时时间指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务 int getTimeout(); // 获取事务的只读属性，事务的只读属性是指，对事务性资源（比如数据源）进行只读操作或者是读写操作。 boolean isReadOnly();&#125; 1.2.3 TransactionStatusPlatformTransactionManager.getTransaction(…) 方法返回一个 TransactionStatus 对象，该对象可能代表一个新的或已经存在的事务（如果在当前调用堆栈有一个符合条件的事务）。TransactionStatus 接口提供了一个简单的控制事务执行和查询事务状态的方法。该接口的源代码如下：12345678public interface TransactionStatus&#123; // 是否是一个新事务 boolean isNewTransaction(); // Spring 中默认是通过抛出运行时异常来回滚，如果不想抛出异常，可以设置这个参数手动回滚 void setRollbackOnly(); // 判断当前是否设置了回滚标记 boolean isRollbackOnly();&#125; 这个接口有一个默认实现类：DefaultTransactionStatus。这是整个事务框架最重要的状态对象，它贯穿于事务拦截器，Spring 抽象框架和底层具体事务实现框架之间，它的重要任务是在新建，挂起，提交事务的过程中保存对应事务的属性。在 AbstractPlatformTransactionManager 中，每个事物流程都会创建这个对象。DefaultTransactionStatus 会持有一个 DataSourceTransactionObject，这是底层JDBC具体框架使用的对象，其中包含 ConnectionHolder，它又持有了 Connection，表示一个实际的数据库连接。 1.3 AOP 套件Spring 的事务是通过 AOP 的机制实现的，因此在阅读源码之前对一些 AOP 组件的了解是有必要的。 1.3.1 事务通知器事务通知器的实现类是 BeanFactoryTransactionAttributeSourceAdvisor，如果开启了事务功能，它会自动注入容器中。和其它通知器一样它的内部也有一组切点和通知，只是它的切点匹配方式和我们常见的通知器不一样，不是根据 AspectJ 表达式，而是根据有没有@Transaction 注解来匹配的。类继承关系如下：可以看出继承了 PointcutAdvisor 和 BeanFactoryAware 接口，表明它是一个通知器，而且能获取到容器对象。该类的定义如下：1234567891011121314151617181920212223public class BeanFactoryTransactionAttributeSourceAdvisor extends AbstractBeanFactoryPointcutAdvisor &#123; // TransactionAttributeSource 用于获取方法的 TransactionDefinition 信息 private TransactionAttributeSource transactionAttributeSource; // 匿名内部类实现的切点，和常见的切点实现类的不同之处在于它可以获取 TransactionAttributeSource 对象 private final TransactionAttributeSourcePointcut pointcut = new TransactionAttributeSourcePointcut() &#123; @Override @Nullable protected TransactionAttributeSource getTransactionAttributeSource() &#123; return transactionAttributeSource; &#125; &#125;; // 设置 TransactionAttributeSource public void setTransactionAttributeSource(TransactionAttributeSource transactionAttributeSource)&#123;...&#125; // 类过滤器，用于通知器匹配合适的类 public void setClassFilter(ClassFilter classFilter) &#123;...&#125; // 获取切点 @Override public Pointcut getPointcut() &#123;...&#125;&#125; 1.3.2 事务切点TransactionAttributeSourcePointcut 对应 AOP 中的切点。它是一个抽象接口，具体的实现在 BeanFactoryTransactionAttributeSourceAdvisor 里，是一个匿名内部类。它的继承关系如图所示：它实现了两个接口：Pointcut 和 MethodMatcher。MethodMatcher 中的 matches 方法提供了方法的匹配，而 Pointcut 的 getClassFilter 和 getMethodMatcher 方法提供了获取方法匹配器和类过滤器的方法。那么这里为什么没有实现类过滤器呢？因为在抽象类 StaticMethodMatcherPointcut 里直接指定了 private ClassFilter classFilter = ClassFilter.TRUE;。TransactionAttributeSourcePointcut 的定义如下：123456789101112131415161718192021222324abstract class TransactionAttributeSourcePointcut extends StaticMethodMatcherPointcut implements Serializable &#123; // 默认类过滤器 private ClassFilter classFilter = ClassFilter.TRUE; // 方法匹配 @Override public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123;...&#125; // 获取 TransactionAttributeSource，用于获取方法的 TransactionDefinition 信息 @Nullable protected abstract TransactionAttributeSource getTransactionAttributeSource(); // 设置类过滤器 public void setClassFilter(ClassFilter classFilter) &#123;...&#125; // 获取类过滤器 @Override public ClassFilter getClassFilter() &#123;...&#125; // 获取方法匹配器 @Override public final MethodMatcher getMethodMatcher() &#123;...&#125;&#125; 1.3.3 事务拦截器TransactionInterceptor 对应 AOP 中的拦截器，也就是 AspectJ 标准中的通知。TransactionInterceptor 是一个环绕型通知，它的类继承关系如下：主要实现了两个接口：BeanFactoryAware 和 Advice。说明它是一个通知类，同时也能获取容器对象。该类的（部分）定义如下：1234567891011121314151617181920212223public class TransactionInterceptor extends TransactionAspectSupport implements MethodInterceptor, Serializable &#123; // ThreadLocal变量，存放当前事务信息 private static final ThreadLocal&lt;TransactionInfo&gt; transactionInfoHolder; // 事务管理器 private PlatformTransactionManager transactionManager; // 事务信息 private TransactionAttributeSource transactionAttributeSource; // 容器对象 private BeanFactory beanFactory; // 通知的执行方法，在方法执行前后的一些处理 @Override @Nullable public Object invoke(MethodInvocation invocation) throws Throwable &#123;...&#125; // 获取当前事务信息 protected static TransactionInfo currentTransactionInfo()&#123;...&#125; // 获取当前事务状态 public static TransactionStatus currentTransactionStatus()&#123;...&#125; // 在事务中运行方法（核心） protected Object invokeWithinTransaction(Method method, Class&lt;?&gt; targetClass, InvocationCallback invocation) &#123;...&#125; 1.3 事务同步管理器这个组件单独拿出来说是因为在之后的代码中会经常看到。这是一个保存当前事务信息的组件，代码如下：12345678910111213141516171819202122public abstract class TransactionSynchronizationManager &#123; // 保存当前事务的一些资源，比如连接 private static final ThreadLocal&lt;Map&lt;Object, Object&gt;&gt; resources = new NamedThreadLocal&lt;&gt;(\"Transactional resources\"); // 记录当前事务的回调方法，类似于事务的监听器 private static final ThreadLocal&lt;Set&lt;TransactionSynchronization&gt;&gt; synchronizations = new NamedThreadLocal&lt;&gt;(\"Transaction synchronizations\"); // 记录当前事务名 private static final ThreadLocal&lt;String&gt; currentTransactionName = new NamedThreadLocal&lt;&gt;(\"Current transaction name\"); // 记录当前事务读写情况 private static final ThreadLocal&lt;Boolean&gt; currentTransactionReadOnly = new NamedThreadLocal&lt;&gt;(\"Current transaction read-only status\"); // 记录当前事务隔离级别 private static final ThreadLocal&lt;Integer&gt; currentTransactionIsolationLevel = new NamedThreadLocal&lt;&gt;(\"Current transaction isolation level\"); // 记录当前事务是否活跃 private static final ThreadLocal&lt;Boolean&gt; actualTransactionActive = new NamedThreadLocal&lt;&gt;(\"Actual transaction active\"); //...&#125; 可以看到内部统一使用了 ThreadLocal，这是因为当前事务的信息需要在线程内部全局可见，类似于操作系统中的进程上下文。当事务切换时，本质上就是获取一个新的数据库连接然后把事务同步管理器中的 ThreadLocal 变量替换掉。 二、事务实现事务的实现依赖于 Spring 的 AOP 机制，从本质上来讲，下面两段代码是等价的：1234567891011121314151617181920public void service() throws Exception &#123; // 1.获取事务控制管理器 DataSourceTransactionManager transactionManager = applicationContext.getBean( \"transactionManager\", DataSourceTransactionManager.class); // 2.获取事务定义 DefaultTransactionDefinition def = new DefaultTransactionDefinition(); // 3.设置事务隔离级别 def.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRED); // 4.开启事务，获得事务状态 TransactionStatus status = transactionManager.getTransaction(def); try &#123; // 5.具体的数据库操作（多个） do sql... // 6.提交事务 transactionManager.commit(status); &#125; catch (Exception e) &#123; // 7.回滚事务 transactionManager.rollback(status); &#125; &#125; 1234@Transactionalpublic void service() throws Exception &#123; do sql&#125; 从上我们大概可以猜到，Spring 的事务本质上就是一个环绕通知器，在方法运行前开启事务，在方法结束后提交事务，抛出异常时回滚事务。由于之前已经分析过 AOP 相关的源码，因此对事务的实现这里只看一些重要的方法。 2.1 扫描@Transactional 方法Spring 要生成事务的代理对象，首先就要判断当前对象中是否包含了@Transactional 方法，这个过程发生在筛选通知器的过程中。当开启了事务之后，BeanFactoryTransactionAttributeSourceAdvisor 就会被注入容器中，当我们为一个包含了@Transactional 方法的对象筛选通知器的时候，BeanFactoryTransactionAttributeSourceAdvisor 就会被匹配到，然后注解的信息会被保存在通知器中返回。核心代码如下：12345678910111213141516171819202122232425262728293031323334353637protected TransactionAttribute computeTransactionAttribute(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; // 忽略非 public 的方法 if (allowPublicMethodsOnly() &amp;&amp; !Modifier.isPublic(method.getModifiers())) &#123; return null; &#125; // method 代表接口中的 default 方法，specificMethod 代表实现类中的方法 Method specificMethod = AopUtils.getMostSpecificMethod(method, targetClass); // 查看实现方法中是否存在事务声明 TransactionAttribute txAttr = findTransactionAttribute(specificMethod); if (txAttr != null) &#123; return txAttr; &#125; // 查看方法所在类中是否存在事务声明 txAttr = findTransactionAttribute(specificMethod.getDeclaringClass()); if (txAttr != null &amp;&amp; ClassUtils.isUserLevelMethod(method)) &#123; return txAttr; &#125; // 如果存在接口，则到接口中去寻找 if (specificMethod != method) &#123; // 查找接口方法是否存在事务声明 txAttr = findTransactionAttribute(method); if (txAttr != null) &#123; return txAttr; &#125; // 查看接口所在类上面是否存在事务声明 txAttr = findTransactionAttribute(method.getDeclaringClass()); if (txAttr != null &amp;&amp; ClassUtils.isUserLevelMethod(method)) &#123; return txAttr; &#125; &#125; return null;&#125; 如果方法中存在事务属性，则使用方法上的属性，否则使用方法所在的类上的属性，如果方法所在类的属性上还是没有搜寻到对应的事务属性，那么再搜寻接口中的方法，再没有的话，最后尝试搜寻接口的类上面的声明。最终返回一个 TransactionAttribute 类型的结果，匹配到的依据就是返回的对象是否为空，为空就是没匹配到，不为空就是匹配到了。TransactionAttribute 是 TransactionDefinition 的子类，里面保存着事务的定义信息。事务通知器会把它保存 TransactionAttributeSource 内部的一个 Map 中，key是(类名+方法名)，value 是事务定义信息。 2.2 执行事务拦截器在执行动态代理的对象时，对象持有的若干个通知器会抽离出内部持有的拦截器组成拦截器链，事务通知器也不例外。事务拦截器本质上是一个环绕型通知，通过内部的事务定义信息控制事务的创建、提交和回滚。其核心方法如下：1234567891011121314151617181920212223242526272829303132333435363738protected Object invokeWithinTransaction(Method method, @Nullable Class&lt;?&gt; targetClass, final TransactionAspectSupport.InvocationCallback invocation) throws Throwable &#123; // 获取 TransactionAttributeSource TransactionAttributeSource tas = getTransactionAttributeSource(); // 通过 TransactionAttributeSource 获取事务定义信息 final TransactionAttribute txAttr = (tas != null ? tas.getTransactionAttribute(method, targetClass) : null); // 获取容器中的事务管理器 final PlatformTransactionManager tm = determineTransactionManager(txAttr); // 构造方法唯一标识（类.方法，如service.UserServiceImpl.save） final String joinpointIdentification = methodIdentification(method, targetClass, txAttr); // 声明式事务 if (txAttr == null || !(tm instanceof CallbackPreferringPlatformTransactionManager)) &#123; // 根据传播行为创建事务，返回值是事务信息，里面包含事务的定义、事务的状态、指向上一个事务信息的指针等 TransactionAspectSupport.TransactionInfo txInfo = createTransactionIfNecessary(tm, txAttr, joinpointIdentification); Object retVal = null; try &#123; // 调用回调方法（这个看起来好像跟其它拦截器里面的回调方法不一样，但其实是一样的。这个是函数式接口，传入的是 process() 方法） retVal = invocation.proceedWithInvocation(); &#125; catch (Throwable ex) &#123; // 抛出异常，根据当前事务信息决定是回滚还是提交 completeTransactionAfterThrowing(txInfo, ex); // 继续抛出异常，这里有两个目的：如果是内层事务，抛出异常让外层事务去回滚；如果是最外层事务，抛出异常给用户 throw ex; &#125; finally &#123; // 将保存在 ThreadLocal 的当前事务信息替换为上一个事务的信息 cleanupTransactionInfo(txInfo); &#125; // 提交当前事务 commitTransactionAfterReturning(txInfo); return retVal; &#125; // 编程式事务 else &#123;...&#125;&#125; 从代码我们可以看出，拦截器主要做了四件事：获取事务、执行方法、正常处理和异常处理。我们先看一下开启事务的相关方法： 2.2.1 获取事务1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public final TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException &#123; // transaction 代表了当前事务 Object transaction = doGetTransaction(); if (definition == null) &#123; // 如果没有事务定义信息，使用默认定义 definition = new DefaultTransactionDefinition(); &#125; if (isExistingTransaction(transaction)) &#123; // 已经存在事务，根据传播行为确定接下来的处理流程 return handleExistingTransaction(definition, transaction, debugEnabled); &#125; // 当前不存在事务，不同的传播行为不同处理 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_MANDATORY) &#123; // PROPAGATION_MANDATORY 下直接抛出异常 throw new IllegalTransactionStateException(...); &#125; else if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED || definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW || definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; // PROPAGATION_REQUIRED、PROPAGATION_REQUIRES_NEW、PROPAGATION_NESTED 三种情况下创建新事务 SuspendedResourcesHolder suspendedResources = suspend(null); try &#123; // 打开同步开关，除非设置为 SYNCHRONIZATION_NEVER boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); // 更新事务状态 DefaultTransactionStatus status = newTransactionStatus( definition, transaction, true, newSynchronization, debugEnabled, suspendedResources); // 开启事务 doBegin(transaction, definition); // 执行同步，即把当前事务信息写入线程变量 prepareSynchronization(status, definition); return status; &#125; catch (RuntimeException | Error ex) &#123; // 恢复事务 resume(null, suspendedResources); throw ex; &#125; &#125; else &#123; // PROPAGATION_SUPPORTS、PROPAGATION_NOT_SUPPORTED、PROPAGATION_NEVER 三种情况下在无事务环境运行，基本不用做什么操作 if (definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT &amp;&amp; logger.isWarnEnabled()) &#123; logger.warn(...); &#125; // 关闭同步开关，除非设置为 SYNCHRONIZATION_ALWAYS boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); // 创建新的事务状态但不执行同步 return prepareTransactionStatus(definition, null, true, newSynchronization, debugEnabled, null); &#125;&#125; 如上，源码分成了2条处理线： 当前已存在事务：isExistingTransaction() 判断是否存在事务，存在事务 handleExistingTransaction() 根据不同传播机制不同处理 当前不存在事务: 不同传播机制不同处理 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091private TransactionStatus handleExistingTransaction(TransactionDefinition definition, Object transaction, boolean debugEnabled) throws TransactionException &#123; if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NEVER) &#123; // PROPAGATION_NEVER 下直接抛出异常 throw new IllegalTransactionStateException(...); &#125; if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NOT_SUPPORTED) &#123; // PROPAGATION_NOT_SUPPORTED 下先挂起当前事务，再在无事务状态下运行 // 挂起当前事务 Object suspendedResources = suspend(transaction); // 关闭同步开关，除非 SYNCHRONIZATION_ALWAYS boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); // 创建新的事务状态 return prepareTransactionStatus(definition, null, false, newSynchronization, debugEnabled, suspendedResources); &#125; if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW) &#123; // PROPAGATION_NOT_SUPPORTED 下先挂起当前事务，再创建新事务 // 挂起当前事务 SuspendedResourcesHolder suspendedResources = suspend(transaction); try &#123; // 打开同步开关，除非 SYNCHRONIZATION_NEVER boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); // 创建新的事务状态 DefaultTransactionStatus status = newTransactionStatus( definition, transaction, true, newSynchronization, debugEnabled, suspendedResources); // 开启新事务 doBegin(transaction, definition); // 执行同步，把新的事务信息写入线程变量 prepareSynchronization(status, definition); return status; &#125; catch (RuntimeException | Error beginEx) &#123; // 恢复事务 resumeAfterBeginException(transaction, suspendedResources, beginEx); throw beginEx; &#125; &#125; if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; // PROPAGATION_NESTED 下，嵌套执行新事务。嵌套事务本质上是设置了一个保存点，在内部事务中的回滚都是回滚到保存点 if (!isNestedTransactionAllowed()) &#123; throw new NestedTransactionNotSupportedException(...); &#125; // 大多数情况下，通过保存点方式实现 if (useSavepointForNestedTransaction()) &#123; // 创建新的事务状态，但不执行同步（第四个参数） DefaultTransactionStatus status = prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null); // 创建并持有了保存点 status.createAndHoldSavepoint(); return status; &#125; // JTA 情况下，通过新建事务实现 else &#123; // 开启同步开关，除非 SYNCHRONIZATION_NEVER boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); // 创建新的事务状态 DefaultTransactionStatus status = newTransactionStatus( definition, transaction, true, newSynchronization, debugEnabled, null); // 开启新事务 doBegin(transaction, definition); // 执行同步 prepareSynchronization(status, definition); return status; &#125; &#125; // PROPAGATION_REQUIRED、PROPAGATION_SUPPORTS、PROPAGATION_MANDATORY 三种情况都是加入原先的事务 if (isValidateExistingTransaction()) &#123; // 这里会判断新旧事务的隔离级别，如果新事务的隔离级别和旧事务不一样，而且不是 ISOLATION_DEFAULT，抛出异常 if (definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT) &#123; Integer currentIsolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel(); if (currentIsolationLevel == null || currentIsolationLevel != definition.getIsolationLevel()) &#123; Constants isoConstants = DefaultTransactionDefinition.constants; throw new IllegalTransactionStateException(...); &#125; &#125; // 检查新旧事务的只读属性，不一样抛出异常 if (!definition.isReadOnly()) &#123; if (TransactionSynchronizationManager.isCurrentTransactionReadOnly()) &#123; throw new IllegalTransactionStateException(...); &#125; &#125; &#125; // 打开同步开关，除非 SYNCHRONIZATION_NEVER boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); return prepareTransactionStatus(definition, transaction, false, newSynchronization, debugEnabled, null);&#125; 可以看到，获取事务时会根据事务传播行为和当前是否存在事务，来决定以何种方式来创建事务。流程涉及到三个方法：挂起事务、开启事务和恢复事务，我们依次来看一下： 2.2.1.1 挂起事务1234567891011121314151617181920212223242526272829303132333435363738394041protected final SuspendedResourcesHolder suspend(@Nullable Object transaction) throws TransactionException &#123; // 当前存在同步 if (TransactionSynchronizationManager.isSynchronizationActive()) &#123; // 获取当前事务的同步方法（类似于监听器） List&lt;TransactionSynchronization&gt; suspendedSynchronizations = doSuspendSynchronization(); try &#123; Object suspendedResources = null; if (transaction != null) &#123; // 挂起当前事务，获取挂起事务的资源 suspendedResources = doSuspend(transaction); &#125; // 保存和清空一些事务同步管理器中的线程变量 String name = TransactionSynchronizationManager.getCurrentTransactionName(); TransactionSynchronizationManager.setCurrentTransactionName(null); boolean readOnly = TransactionSynchronizationManager.isCurrentTransactionReadOnly(); TransactionSynchronizationManager.setCurrentTransactionReadOnly(false); Integer isolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel(); TransactionSynchronizationManager.setCurrentTransactionIsolationLevel(null); boolean wasActive = TransactionSynchronizationManager.isActualTransactionActive(); TransactionSynchronizationManager.setActualTransactionActive(false); // 将线程变量保存在 SuspendedResourcesHolder 中返回 return new SuspendedResourcesHolder( suspendedResources, suspendedSynchronizations, name, readOnly, isolationLevel, wasActive); &#125; catch (RuntimeException | Error ex) &#123; // 挂起失败，调用事务恢复的同步方法 doResumeSynchronization(suspendedSynchronizations); throw ex; &#125; &#125; // 当前有同步但没有同步方法 else if (transaction != null) &#123; Object suspendedResources = doSuspend(transaction); return new SuspendedResourcesHolder(suspendedResources); &#125; // 当前既没有同步也没有同步方法 else &#123; return null; &#125;&#125; 12345678protected Object doSuspend(Object transaction) &#123; // 数据源事务对象 DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; // 清空数据库连接 txObject.setConnectionHolder(null); // 解绑并返回当前线程的连接持有者 return TransactionSynchronizationManager.unbindResource(this.dataSource);&#125; 挂起线程的核心操作：1）保存并置空当前线程中的事务信息2）清空数据源事务对象中的连接持有者3）线程中解绑并返回当前连接持有者 2.2.1.2 开启事务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354protected void doBegin(Object transaction, TransactionDefinition definition) &#123; // 数据源事务对象 DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; Connection con = null; try &#123; // 当前没有数据库连接 if (!txObject.hasConnectionHolder() || txObject.getConnectionHolder().isSynchronizedWithTransaction()) &#123; // 从数据源获得新连接 Connection newCon = obtainDataSource().getConnection(); txObject.setConnectionHolder(new ConnectionHolder(newCon), true); &#125; txObject.getConnectionHolder().setSynchronizedWithTransaction(true); // 当前数据库的连接 con = txObject.getConnectionHolder().getConnection(); // 给连接设置新的隔离级别、只读 Integer previousIsolationLevel = DataSourceUtils.prepareConnectionForTransaction(con, definition); // 保存旧的隔离级别 txObject.setPreviousIsolationLevel(previousIsolationLevel); // 如果是自动提交切换到手动提交 if (con.getAutoCommit()) &#123; txObject.setMustRestoreAutoCommit(true); // 表示开启事务 con.setAutoCommit(false); &#125; // 如果只读，执行sql设置事务只读 prepareTransactionalConnection(con, definition); txObject.getConnectionHolder().setTransactionActive(true); // 设置事务超时 int timeout = determineTimeout(definition); if (timeout != TransactionDefinition.TIMEOUT_DEFAULT) &#123; txObject.getConnectionHolder().setTimeoutInSeconds(timeout); &#125; // 绑定当前的连接持有者到当前线程 if (txObject.isNewConnectionHolder()) &#123; TransactionSynchronizationManager.bindResource(this.dataSource, txObject.getConnectionHolder()); &#125; &#125; catch (Throwable ex) &#123; if (txObject.isNewConnectionHolder()) &#123; // 释放连接 DataSourceUtils.releaseConnection(con, obtainDataSource()); // 清空连接对象 txObject.setConnectionHolder(null, false); &#125; throw new CannotCreateTransactionException(\"Could not open JDBC Connection for transaction\", ex); &#125;&#125; 开启事务分为两种情况，如果当前没有连接，则从数据源获取一条新连接，等于是新开了一个事务；如果当前已有连接，则使用当前的连接，等于是加入了原先的事务。不过加入原先事务的时候，会修改原先事务的隔离级别、自动提交、超时时间等信息。然后把当前的&lt;数据源,连接持有者&gt;写入线程变量中。 2.2.1.3 恢复事务12345678910111213141516171819202122protected final void resume(@Nullable Object transaction, @Nullable SuspendedResourcesHolder resourcesHolder) throws TransactionException &#123; // resourcesHolder 中是挂起的事务信息 if (resourcesHolder != null) &#123; // 获取挂起的事务资源 Object suspendedResources = resourcesHolder.suspendedResources; if (suspendedResources != null) &#123; // 执行恢复操作 doResume(transaction, suspendedResources); &#125; // 下面的代码是将挂起的事务信息重新写回事务同步管理器中 List&lt;TransactionSynchronization&gt; suspendedSynchronizations = resourcesHolder.suspendedSynchronizations; if (suspendedSynchronizations != null) &#123; TransactionSynchronizationManager.setActualTransactionActive(resourcesHolder.wasActive); TransactionSynchronizationManager.setCurrentTransactionIsolationLevel(resourcesHolder.isolationLevel); TransactionSynchronizationManager.setCurrentTransactionReadOnly(resourcesHolder.readOnly); TransactionSynchronizationManager.setCurrentTransactionName(resourcesHolder.name); // 将回调方法写回事务同步管理器之前会调用事务恢复的回调方法 doResumeSynchronization(suspendedSynchronizations); &#125; &#125;&#125; 1234protected void doResume(@Nullable Object transaction, Object suspendedResources) &#123; // 回绑之前解绑的连接持有者 TransactionSynchronizationManager.bindResource(this.dataSource, suspendedResources);&#125; 可以看出恢复事务基本上是挂起事务的逆操作，就是把挂起时保存的事务信息重新写回到事务同步管理器中。 2.2.2 异常处理12345678910111213141516171819202122232425262728293031protected void completeTransactionAfterThrowing(@Nullable TransactionInfo txInfo, Throwable ex) &#123; if (txInfo != null &amp;&amp; txInfo.getTransactionStatus() != null) &#123; // 事务属性不为空且事务在该异常下会执行回滚 if (txInfo.transactionAttribute != null &amp;&amp; txInfo.transactionAttribute.rollbackOn(ex)) &#123; try &#123; // 执行回滚 txInfo.getTransactionManager().rollback(txInfo.getTransactionStatus()); &#125; catch (TransactionSystemException ex2) &#123; ex2.initApplicationException(ex); throw ex2; &#125; catch (RuntimeException | Error ex2) &#123; throw ex2; &#125; &#125; // 事务在该异常下不执行回滚，就直接提交 else &#123; try &#123; txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); &#125; catch (TransactionSystemException ex2) &#123; ex2.initApplicationException(ex); throw ex2; &#125; catch (RuntimeException | Error ex2) &#123; throw ex2; &#125; &#125; &#125;&#125; 2.2.2.1 回滚事务123456789public final void rollback(TransactionStatus status) throws TransactionException &#123; if (status.isCompleted()) &#123; throw new IllegalTransactionStateException( \"Transaction is already completed - do not call commit or rollback more than once per transaction\"); &#125; DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; // 主要调用了 processRollback processRollback(defStatus, false);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849private void processRollback(DefaultTransactionStatus status, boolean unexpected) &#123; try &#123; // 是否是期望中的回滚 boolean unexpectedRollback = unexpected; try &#123; // 触发 BeforeCompletion 同步方法 triggerBeforeCompletion(status); // 如果有保存点 if (status.hasSavepoint()) &#123; // 回滚到保存点 status.rollbackToHeldSavepoint(); &#125; // 如果是最外层事务 else if (status.isNewTransaction()) &#123; // 执行回滚 doRollback(status); &#125; else &#123; // 当前存在事务，但不是最外层事务 if (status.hasTransaction()) &#123; // 如果已经标记为回滚 或 当加入事务失败时全局回滚（默认true） if (status.isLocalRollbackOnly() || isGlobalRollbackOnParticipationFailure()) &#123; // 将当前数据源事务对象标记为回滚 doSetRollbackOnly(status); &#125; &#125; if (!isFailEarlyOnGlobalRollbackOnly()) &#123; unexpectedRollback = false; &#125; &#125; &#125; catch (RuntimeException | Error ex) &#123; // 触发 AfterCompletion 同步方法 triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); throw ex; &#125; // 触发 AfterCompletion 同步方法 triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK); if (unexpectedRollback) &#123; throw new UnexpectedRollbackException( \"Transaction rolled back because it has been marked as rollback-only\"); &#125; &#125; finally &#123; // 解绑当前线程 cleanupAfterCompletion(status); &#125;&#125; 1234567891011protected void doRollback(DefaultTransactionStatus status) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) status.getTransaction(); Connection con = txObject.getConnectionHolder().getConnection(); try &#123; // 执行底层的回滚 con.rollback(); &#125; catch (SQLException ex) &#123; throw new TransactionSystemException(\"Could not roll back JDBC transaction\", ex); &#125;&#125; 这里可以看出回滚的三种方式：1）如果有保存点，则回滚到保存点；2）如果是最外层事务，则直接回滚；3）如果是内层事务，则给事务状态打一个回滚标记。 2.2.3 正常处理123456protected void commitTransactionAfterReturning(@Nullable TransactionInfo txInfo) &#123; if (txInfo != null &amp;&amp; txInfo.getTransactionStatus() != null) &#123; // 获取事务管理器执行 commit txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); &#125;&#125; 2.2.3.1 提交事务123456789101112131415161718192021public final void commit(TransactionStatus status) throws TransactionException &#123; // 事务执行完了，抛出异常 if (status.isCompleted()) &#123; throw new IllegalTransactionStateException( \"Transaction is already completed - do not call commit or rollback more than once per transaction\"); &#125; DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; // 被标记为本地回滚，回滚 if (defStatus.isLocalRollbackOnly()) &#123; processRollback(defStatus, false); return; &#125; // 被标记为全局回滚，且设置了“不需要在全局回滚时提交”，回滚 if (!shouldCommitOnGlobalRollbackOnly() &amp;&amp; defStatus.isGlobalRollbackOnly()) &#123; processRollback(defStatus, true); return; &#125; // 执行提交 processCommit(defStatus);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private void processCommit(DefaultTransactionStatus status) throws TransactionException &#123; try &#123; boolean beforeCompletionInvoked = false; try &#123; boolean unexpectedRollback = false; prepareForCommit(status); // 触发 BeforeCommit 同步方法 triggerBeforeCommit(status); // 触发 BeforeCompletion 同步方法 triggerBeforeCompletion(status); beforeCompletionInvoked = true; // 如果有保存点 if (status.hasSavepoint()) &#123; unexpectedRollback = status.isGlobalRollbackOnly(); // 释放保存点 status.releaseHeldSavepoint(); &#125; // 如果是最外层事务（新事务） else if (status.isNewTransaction()) &#123; unexpectedRollback = status.isGlobalRollbackOnly(); // 调用事务处理器提交事务 doCommit(status); &#125; else if (isFailEarlyOnGlobalRollbackOnly()) &#123; unexpectedRollback = status.isGlobalRollbackOnly(); &#125; // 非新事务，且全局回滚失败，但提交时没有得到异常，抛出异常 if (unexpectedRollback) &#123; throw new UnexpectedRollbackException( \"Transaction silently rolled back because it has been marked as rollback-only\"); &#125; &#125; catch (UnexpectedRollbackException ex) &#123; // 触发 AfterCompletion 同步方法 triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK); throw ex; &#125; catch (TransactionException ex) &#123; // 提交失败回滚 if (isRollbackOnCommitFailure()) &#123; doRollbackOnCommitException(status, ex); &#125; else &#123; // 触发 AfterCompletion 同步方法 triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); &#125; throw ex; &#125; catch (RuntimeException | Error ex) &#123;...&#125; &#125; finally &#123; cleanupAfterCompletion(status); &#125;&#125; 事务的提交比较简单，如果是最外层事务，则直接提交；如果有保存点，释放保存点；如果是内层事务，这里不提交，等到外层再提交。 三、参考资料Java工匠-Spring事务基础设施介绍Spring事务解析3-增强方法的获取深入Spring:自定义事务管理Spring 事务管理机制概述spring事务(4) 事务同步管理器spring事务详解（三）源码详解","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring MVC 源码浅析——容器创建过程","slug":"Spring-MVC源码浅析——容器创建过程","date":"2019-03-26T15:45:41.000Z","updated":"2019-03-28T05:36:31.997Z","comments":true,"path":"2019/03/26/Spring-MVC源码浅析——容器创建过程/","link":"","permalink":"http://yoursite.com/2019/03/26/Spring-MVC源码浅析——容器创建过程/","excerpt":"","text":"Spring MVC 中会配置两个容器：一个用于加载 Web 层的类，比如 Controller、HandlerMapping、ViewResolver 等，叫 web 容器；另一个容器用于加载业务逻辑相关的类，比如 service、dao 层的一些类，叫业务容器。这两个容器是父子关系，业务容器是 web 容器的父容器。父容器中的 bean 对子容器可见，而子容器中的 bean 对父容器不可见。在初始化时，父容器会先于子容器初始化，这是因为子容器中的一些 bean 可能会依赖父容器。 注：在 Sping Boot 中不再有父子容器的概念，因此这是 Spring MVC 独有的。因为面试会问到这部分概念，所以在这里总结一下。 一、父容器创建web 应用程序启动时，servlet 容器会创建一个全局共享的上下文： ServletContext，然后会读取 web.xml 文件，将读取到的转化为键值对，将读取到的创建出来。如果我们要使用 Spring 的话，就要在 web.xml 中添加如下配置：12345678910111213&lt;web-app&gt; &lt;!-- 省略其他配置 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:application.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- 省略其他配置 --&gt;&lt;/web-app&gt; tomcat 会在启动时创建 ContextLoaderListener 对象，这个对象会监听 ServletContext 的创建，然后在监听回调函数中创建父容器。我们来看一下它创建父容器的代码：12345678910public class ContextLoaderListener extends ContextLoader implements ServletContextListener &#123; // 省略部分代码 @Override public void contextInitialized(ServletContextEvent event) &#123; // 初始化父容器 initWebApplicationContext(event.getServletContext()); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142public WebApplicationContext initWebApplicationContext(ServletContext servletContext) &#123; // ServletContext 中会存储一个&lt;key，父容器&gt;的键值对，如果这个 key 被其它组件使用了，就会抛出异常 if (servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null) &#123; throw new IllegalStateException(...); &#125; //... try &#123; if (this.context == null) &#123; // 创建父容器 this.context = createWebApplicationContext(servletContext); &#125; if (this.context instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context; if (!cwac.isActive()) &#123; if (cwac.getParent() == null) &#123; // 加载当前容器的父容器，一般没有，除非特别配置 ApplicationContext parent = loadParentContext(servletContext); cwac.setParent(parent); &#125; // 配置并刷新父容器，这里的刷新过程就是我们之前分析过的 refresh 方法 configureAndRefreshWebApplicationContext(cwac, servletContext); &#125; &#125; // 在 ServletContext 中创建一个&lt;key，父容器&gt;的键值对，对应方法开头 servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); ClassLoader ccl = Thread.currentThread().getContextClassLoader(); if (ccl == ContextLoader.class.getClassLoader()) &#123; currentContext = this.context; &#125; else if (ccl != null) &#123; currentContextPerThread.put(ccl, this.context); &#125; return this.context; &#125; catch (RuntimeException ex) &#123;...&#125;&#125; 进入创建容器的代码：123456789protected WebApplicationContext createWebApplicationContext(ServletContext sc) &#123; // 判断创建什么类型的容器，默认类型为 XmlWebApplicationContext Class&lt;?&gt; contextClass = determineContextClass(sc); if (!ConfigurableWebApplicationContext.class.isAssignableFrom(contextClass)) &#123; throw new ApplicationContextException(...); &#125; // 通过反射创建容器 return (ConfigurableWebApplicationContext) BeanUtils.instantiateClass(contextClass);&#125; 123456789101112131415161718192021222324252627282930protected Class&lt;?&gt; determineContextClass(ServletContext servletContext) &#123; /* * 读取用户自定义配置，比如： * &lt;context-param&gt; * &lt;param-name&gt;contextClass&lt;/param-name&gt; * &lt;param-value&gt;XXXConfigWebApplicationContext&lt;/param-value&gt; * &lt;/context-param&gt; */ String contextClassName = servletContext.getInitParameter(CONTEXT_CLASS_PARAM); if (contextClassName != null) &#123; try &#123; return ClassUtils.forName(contextClassName, ClassUtils.getDefaultClassLoader()); &#125; catch (ClassNotFoundException ex) &#123;...&#125; &#125; else &#123; /* * 若无自定义配置，则获取默认的容器类型，默认类型为 XmlWebApplicationContext。 * defaultStrategies 读取的配置文件为 ContextLoader.properties， * 该配置文件内容如下： * org.springframework.web.context.WebApplicationContext = * org.springframework.web.context.support.XmlWebApplicationContext */ contextClassName = defaultStrategies.getProperty(WebApplicationContext.class.getName()); try &#123; return ClassUtils.forName(contextClassName, ContextLoader.class.getClassLoader()); &#125; catch (ClassNotFoundException ex) &#123;...&#125; &#125;&#125; 一句话就是根据配置文件的内容获取父容器的类型，然后通过反射创建容器。接下来再看看配置并刷新容器的代码：1234567891011121314151617181920212223242526272829303132protected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac, ServletContext sc) &#123; if (ObjectUtils.identityToString(wac).equals(wac.getId())) &#123; // 从 ServletContext 中获取用户配置的 contextId 属性 String idParam = sc.getInitParameter(CONTEXT_ID_PARAM); if (idParam != null) &#123; // 设置容器 id wac.setId(idParam); &#125; else &#123; // 用户未配置 contextId，则设置一个默认的容器 id wac.setId(ConfigurableWebApplicationContext.APPLICATION_CONTEXT_ID_PREFIX + ObjectUtils.getDisplayString(sc.getContextPath())); &#125; &#125; wac.setServletContext(sc); // 获取 contextConfigLocation 配置 String configLocationParam = sc.getInitParameter(CONFIG_LOCATION_PARAM); if (configLocationParam != null) &#123; wac.setConfigLocation(configLocationParam); &#125; ConfigurableEnvironment env = wac.getEnvironment(); if (env instanceof ConfigurableWebEnvironment) &#123; ((ConfigurableWebEnvironment) env).initPropertySources(sc, null); &#125; customizeContext(sc, wac); // 刷新容器 wac.refresh();&#125; 说白了也很简单，就是从配置文件获取容器 id，没有的话就默认设置一个，然后获取配置文件的路径，放入容器中，刷新容器，之后就是大家熟悉的过程了。 总结一下父容器的创建过程： 创建 listener 节点中的 ContextLoaderListener 实例 创建过程中初始化 webapplicationContext，也即是父容器对象 从 ServletContext 中获取 contextConfigLocation 的值，这是父容器配置文件的路径，把路径放入父容器中 根据配置信息刷新父容器 将父容器保存在 ServletContext 中 二、子容器创建子容器的加载发生在 DispatcherServlet 初始化的时候，而 DispatcherServlet 的初始化发生在第一个请求到达的时候，因此能确保 Servlet 的初始化发生在 listener 初始化之后。初始化 DispatcherServlet 需要在 web.xml 中添加如下配置：1234567891011121314151617181920&lt;web-app&gt; &lt;!-- 省略其他配置 --&gt; &lt;servlet&gt; &lt;servlet-name&gt;springMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 初始化参数，配置springmvc配置文件 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:application-web.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- web容器启动时加载该Servlet --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springMVC&lt;/servlet-name&gt; &lt;!-- 拦截所有请求 --&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!-- 省略其他配置 --&gt;&lt;/web-app&gt; 初始化的入口在 DispatcherServlet 的 init 方法中：1234567891011121314151617181920212223242526public final void init() throws ServletException &#123; if (logger.isDebugEnabled()) &#123;...&#125; // 获取 ServletConfig 中的配置信息 PropertyValues pvs = new ServletConfigPropertyValues(getServletConfig(), this.requiredProperties); if (!pvs.isEmpty()) &#123; try &#123; // 为 DispatcherServlet 对象创建一个 BeanWrapper，方便读/写对象属性。 BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); ResourceLoader resourceLoader = new ServletContextResourceLoader(getServletContext()); bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, getEnvironment())); initBeanWrapper(bw); // 设置配置信息到目标对象中 bw.setPropertyValues(pvs, true); &#125; catch (BeansException ex) &#123; if (logger.isErrorEnabled()) &#123;...&#125; throw ex; &#125; &#125; // 进行后续的初始化 initServletBean(); if (logger.isDebugEnabled()) &#123;...&#125;&#125; 上面的源码主要做的事情是将 web.xml 中的配置信息设置到 DispatcherServlet 对象中，容器的初始化在 initServletBean 中：123456789protected final void initServletBean() throws ServletException &#123; // ... try &#123; // 初始化容器 this.webApplicationContext = initWebApplicationContext(); initFrameworkServlet(); &#125; // ...&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546protected WebApplicationContext initWebApplicationContext() &#123; // 获取父容器 WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; // 检查外部是否已经设置了子容器，Spring MVC 中一般 this.webApplicationContext = null， // 而 Spring Boot 中 this.webApplicationContext 就是父容器，看后面的代码就可以知道，Spring Boot 不会创建子容器， // 只会再刷新一次，初始化一些特殊组件比如 handler、invocation if (this.webApplicationContext != null) &#123; wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; // Spring Boot 中虽然有外部设置的 this.webApplicationContext，但不会进入下面的代码 if (!cwac.isActive()) &#123; if (cwac.getParent() == null) &#123; // 设置父容器 cwac.setParent(rootContext); &#125; // 配置并刷新容器 configureAndRefreshWebApplicationContext(cwac); &#125; &#125; &#125; if (wac == null) &#123; // 尝试从 ServletContext 中获取容器 wac = findWebApplicationContext(); &#125; if (wac == null) &#123; // 创建容器，并设置父容器 wac = createWebApplicationContext(rootContext); &#125; if (!this.refreshEventReceived) &#123; // 刷新容器 onRefresh(wac); &#125; if (this.publishContext) &#123; String attrName = getServletContextAttributeName(); // 将创建好的容器缓存到 ServletContext 中 getServletContext().setAttribute(attrName, wac); if (this.logger.isDebugEnabled()) &#123;...&#125; &#125; return wac;&#125; 以上就是创建子容器的源码，下面总结一下该容器创建的过程，我们分成 Spring MVC 和 Spring Boot 两条路线：Spring MVC 路线： 从 ServletContext 中获取父容器 如果已有外部设置的子容器的话，设置父容器、刷新子容器 尝试从 ServletContext 中获取子容器，若子容器不为空，则无需执行步骤4 创建子容器，并设置父容器 刷新子容器 缓存子容器到 ServletContext 中 Spring Boot 路线： 从 ServletContext 中获取父容器 获取 this.webApplicationContext 的“子”容器，其实就是父容器 刷新容器 缓存容器到 ServletContext 中 这里子容器的创建分为两条路线，Spring MVC 路线和 Spring Boot 路线。不过最终它们都会调用 onRefresh 刷新容器。接下来我们看一下 onRefresh 方法做了什么：123protected void onRefresh(ApplicationContext context) &#123; initStrategies(context);&#125; 1234567891011121314151617181920protected void initStrategies(ApplicationContext context) &#123; // 初始化 MultipartResolver，用于文件上传 initMultipartResolver(context); // 初始化 LocaleResolver，用于解析用户区域 initLocaleResolver(context); // 初始化 ThemeResolver，用于设置主题 initThemeResolver(context); // 初始化 HandlerMappings，用于映射用户的 URL 和对应的处理类 initHandlerMappings(context); // 初始化 HandlerAdapters，用于执行处理类得到 ModelAndView initHandlerAdapters(context); // 初始化 HandlerExceptionResolvers，用于统一异常处理 initHandlerExceptionResolvers(context); // 初始化 RequestToViewNameTranslator，用于在处理器返回的 View 为空时根据请求得到视图名称 initRequestToViewNameTranslator(context); // 初始化 ViewResolvers，用于把视图名称转换为真正的视图对象 initViewResolvers(context); // 初始化FlashMapManager，用于重定向数据保存 initFlashMapManager(context);&#125; 我们重点关注一下 initHandlerMappings 和 initHandlerAdapters 两个方法：1234567891011121314151617181920212223242526272829private void initHandlerMappings(ApplicationContext context) &#123; this.handlerMappings = null; if (this.detectAllHandlerMappings) &#123; // 如果配置的是找到所有 HandlerMapping，则找到容器和父容器中所有的 HandlerMapping Map&lt;String, HandlerMapping&gt; matchingBeans = BeanFactoryUtils.beansOfTypeIncludingAncestors(context, HandlerMapping.class, true, false); if (!matchingBeans.isEmpty()) &#123; // 将所有 HandlerMapping 保存在 DispatcherServlet 中 this.handlerMappings = new ArrayList&lt;&gt;(matchingBeans.values()); // 对 HandlerMapping 排序 AnnotationAwareOrderComparator.sort(this.handlerMappings); &#125; &#125; else &#123; try &#123; // 如果配置的是只找一个 HandlerMapping，则找到容器中名称为 \"handlerMapping\" 的 HandlerMapping HandlerMapping hm = context.getBean(\"handlerMapping\", HandlerMapping.class); // 将找到的 HandlerMapping 保存在 DispatcherServlet 中 this.handlerMappings = Collections.singletonList(hm); &#125; catch (NoSuchBeanDefinitionException ex) &#123;...&#125; &#125; // 如果容器中没有 HandlerMapping，就创建一个默认的保存在 DispatcherServlet 中 if (this.handlerMappings == null) &#123; this.handlerMappings = getDefaultStrategies(context, HandlerMapping.class); &#125;&#125; 123456789101112131415161718192021222324252627282930private void initHandlerAdapters(ApplicationContext context) &#123; this.handlerAdapters = null; if (this.detectAllHandlerAdapters) &#123; // 如果配置的是找到所有 HandlerAdapter，则找到容器和父容器中所有的 HandlerAdapter Map&lt;String, HandlerAdapter&gt; matchingBeans = BeanFactoryUtils.beansOfTypeIncludingAncestors(context, HandlerAdapter.class, true, false); if (!matchingBeans.isEmpty()) &#123; // 将所有 HandlerAdapter 保存在 DispatcherServlet 中 this.handlerAdapters = new ArrayList&lt;&gt;(matchingBeans.values()); // 对 HandlerAdapter 排序 AnnotationAwareOrderComparator.sort(this.handlerAdapters); &#125; &#125; else &#123; try &#123; // 如果配置的是只找一个 HandlerAdapter，则找到容器中名称为 \"handlerAdapter\" 的 HandlerAdapter HandlerAdapter ha = context.getBean(\"handlerAdapter\", HandlerAdapter.class); // 将找到的 HandlerAdapter 保存在 DispatcherServlet 中 this.handlerAdapters = Collections.singletonList(ha); &#125; catch (NoSuchBeanDefinitionException ex) &#123;...&#125; &#125; // 如果容器中没有 HandlerAdapter，就创建一个默认的保存在 DispatcherServlet 中 if (this.handlerAdapters == null) &#123; this.handlerAdapters = getDefaultStrategies(context, HandlerAdapter.class); &#125;&#125; 上面的注释我们可以看到，initHandlerMappings 和 initHandlerAdapters 这两个方法会从容器中找出 HandlerMapping 和 HandlerAdapter 然后保存在 DispatcherServlet 中。说明 DispatcherServlet 中的 onRefresh 方法并不像我们熟悉的容器刷新一样，它只是把容器中的在 DispatcherServlet 中要用到的组件设置到 DispatcherServlet 中。而实际的 HandlerMapping 和 HandlerAdapter 和普通 bean 一样在创建的时候就初始化了。 三、小结我们来总结一下，Sping MVC 中的容器创建分为父容器的创建和子容器的创建。父容器创建的时机在 Servlet 容器上下文创建的时候，通过监听器 ContextLoaderListener 的回调函数创建父容器。子容器创建的时机在 DispatcherServlet 初始化的时候，一般发生在第一个请求到达，由 Servlet 容器初始化。子容器的创建在我们之前熟知的容器创建步骤之后，还会执行 onRefresh 为 DispatcherServlet 设置几个重要组件，这几个组件用于处理和请求相关的工作。 四、参考资料Spring MVC 原理探秘 - 容器的创建过程SpringMVC工作原理之二：HandlerMapping和HandlerAdapterSpring与SpringMVC父子容器的关系与初始化","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring MVC 源码浅析——请求处理流程","slug":"Spring-MVC源码浅析——请求处理流程","date":"2019-03-22T04:42:36.000Z","updated":"2019-03-26T12:05:30.666Z","comments":true,"path":"2019/03/22/Spring-MVC源码浅析——请求处理流程/","link":"","permalink":"http://yoursite.com/2019/03/22/Spring-MVC源码浅析——请求处理流程/","excerpt":"","text":"Spring Boot 中把 Spring MVC 设置为默认的网络组建，配合内置的 tomcat 服务器，让服务器开发不再困难。但是封装代码带来了简便性的同时，也让下层逻辑不再清晰，这里我们就先来看一下，一个 http 报文到达后会经过怎样的流程。 1、背景知识1.1 Servlet说到网络就不得不说到 Java 中的 Servlet 接口。Servlet 是 J2EE 规范之一，在遵守该规范的前提下，我们可将 Web 应用部署在 Servlet 容器下，比如 tomcat。这样做的好处是什么呢？它可以使开发者聚焦业务逻辑，而不用去关心 HTTP 协议方面的事情。开发能支持 HTTP 协议的服务器本身就是一个庞大的工程：我们需要对 HTTP 的文本进行解析、对各种状态码作出响应、还要考虑底层操作系统和硬件情况等等。 如果我们写的 Web 应用不大，不夸张的说，项目中对 HTTP 提供支持的代码会比业务代码还要多，这岂不是得不偿失。当然，在现实中，我们不用自己动手写这部分的代码，我们只要基于 Servlet 规范实现 Web 应用，剩下的 Servlet 容器就会帮我们处理。 下面，我们先来看看 Servlet 接口及其实现类结构，然后再进行更进一步的说明。 我们从上到下顺序进行分析。先来看看最顶层的两个接口是怎么定义的。 1.1.1 Servlet 与 ServletConfig先来看看 Servlet 接口的定义，如下：123456789101112public interface Servlet &#123; public void init(ServletConfig config) throws ServletException; public ServletConfig getServletConfig(); public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException; public String getServletInfo(); public void destroy();&#125; init 方法会在容器启动时由容器调用，也可能会在 Servlet 第一次被使用时调用，调用时机取决 load-on-start 的配置。容器调用 init 方法时，会向其传入一个 ServletConfig 参数。ServletConfig 是什么呢？顾名思义，ServletConfig 是一个和 Servlet 配置相关的接口。举个例子说明一下，我们在配置 Spring MVC 的 DispatcherServlet 时，会通过 ServletConfig 将配置文件的位置告知 DispatcherServlet。比如：12345678&lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:application-web.xml&lt;/param-value&gt; &lt;/init-param&gt;&lt;/servlet&gt; 如上，标签内的配置信息最终会被放入 ServletConfig 实现类对象中。DispatcherServlet 通过 ServletConfig 接口中的方法，就能获取到 contextConfigLocation 对应的值。 Servlet 中的 service 方法用于处理请求。当然，一般情况下我们不会直接实现 Servlet 接口，通常是通过继承 HttpServlet 抽象类编写业务逻辑的。Servlet 中接口不多，也不难理解，这里就不多说了。下面我们来看看 ServletConfig 接口定义，如下：12345678910public interface ServletConfig &#123; public String getServletName(); public ServletContext getServletContext(); public String getInitParameter(String name); public Enumeration&lt;String&gt; getInitParameterNames();&#125; 先来看看 getServletName 方法，该方法用于获取 servlet 名称，也就是标签中配置的内容。getServletContext 方法用于获取 Servlet 上下文。如果说一个 ServletConfig 对应一个 Servlet，那么一个 ServletContext 则是对应所有的 Servlet。ServletContext 代表当前的 Web 应用，可用于记录一些全局变量，当然它的功能不局限于记录变量。我们可通过标签向 ServletContext 中配置信息，比如在配置 Spring 监听器（ContextLoaderListener）时，就可以通过该标签配置 contextConfigLocation。如下：1234&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:application.xml&lt;/param-value&gt;&lt;/context-param&gt; 关于 ServletContext 就先说这么多了，继续介绍 ServletConfig 中的其他方法。getInitParameter 方法用于获取标签中配置的参数值，getInitParameterNames 则是获取所有配置的名称集合，这两个方法用途都不难理解。 以上是 Servlet 与 ServletConfig 两个接口的说明，比较简单。说完这两个接口，我们继续往下看，接下来是 HttpServlet。 1.1.2 HttpServletHttpServlet，从名字上就可看出，这个类是和 HTTP 协议相关。该类的关注点在于怎么处理 HTTP 请求，比如其定义了 doGet 方法处理 GET 类型的请求，定义了 doPost 方法处理 POST 类型的请求等。我们若需要基于 Servlet 写 Web 应用，应继承该类，并覆盖指定的方法。doGet 和 doPost 等方法并不是处理的入口方法，所以这些方法需要由其他方法调用才行。其他方法是哪个方法呢？当然是 service 方法了。下面我们看一下这个方法的实现。如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Overridepublic void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123; HttpServletRequest request; HttpServletResponse response; if (!(req instanceof HttpServletRequest &amp;&amp; res instanceof HttpServletResponse)) &#123; throw new ServletException(\"non-HTTP request or response\"); &#125; request = (HttpServletRequest) req; response = (HttpServletResponse) res; // 调用重载方法，该重载方法接受 HttpServletRequest 和 HttpServletResponse 类型的参数 service(request, response);&#125;protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String method = req.getMethod(); // 处理 GET 请求 if (method.equals(METHOD_GET)) &#123; long lastModified = getLastModified(req); if (lastModified == -1) &#123; // 调用 doGet 方法 doGet(req, resp); &#125; else &#123; long ifModifiedSince = req.getDateHeader(HEADER_IFMODSINCE); if (ifModifiedSince &lt; lastModified) &#123; maybeSetLastModified(resp, lastModified); doGet(req, resp); &#125; else &#123; resp.setStatus(HttpServletResponse.SC_NOT_MODIFIED); &#125; &#125; // 处理 HEAD 请求 &#125; else if (method.equals(METHOD_HEAD)) &#123; long lastModified = getLastModified(req); maybeSetLastModified(resp, lastModified); doHead(req, resp); // 处理 POST 请求 &#125; else if (method.equals(METHOD_POST)) &#123; // 调用 doPost 方法 doPost(req, resp); &#125; else if (method.equals(METHOD_PUT)) &#123; doPut(req, resp); &#125; else if (method.equals(METHOD_DELETE)) &#123; doDelete(req, resp); &#125; else if (method.equals(METHOD_OPTIONS)) &#123; doOptions(req,resp); &#125; else if (method.equals(METHOD_TRACE)) &#123; doTrace(req,resp); &#125; else &#123; String errMsg = lStrings.getString(\"http.method_not_implemented\"); Object[] errArgs = new Object[1]; errArgs[0] = method; errMsg = MessageFormat.format(errMsg, errArgs); resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, errMsg); &#125;&#125; 如上，第一个 service 方法覆盖父类中的抽象方法，并没什么太多逻辑。所有的逻辑集中在第二个 service 方法中，该方法根据请求类型分发请求。我们可以根据需要覆盖指定的处理方法。 以上所述只是 Servlet 规范中的一部分内容，这些内容是和本文相关的内容。对于 Servlet 规范中的其他内容，大家有兴趣可以自己去探索。好了，关于 Servlet 方面的内容，这里先说这么多。 1.2 Spring MVC 组件介绍Spring MVC 利用 Spring 的 IOC 容器管理自己的组件，这些组件之间的协作关系如下图： 我们按照用户发送请求——&gt;用户收到响应来走一遍： 首先用户发送请求——&gt;DispatcherServlet，前端控制器收到请求后自己不进行处理，而是委托给其他的解析器进行处理，作为统一访问点，进行全局的流程控制； DispatcherServlet——&gt;HandlerMapping，HandlerMapping 将会把请求映射为 HandlerExecutionChain 对象（包含一个 Handler 处理器（页面控制器）对象、多个 HandlerInterceptor 拦截器）对象，通过这种策略模式，很容易添加新的映射策略； DispatcherServlet——&gt;HandlerAdapter.Controller（HandlerAdapter 将会把Controller包装为适配器，HandlerAdapter 将会根据适配的结果调用真正的处理器的功能处理方法，完成功能处理）； 业务处理方法（Service） HandlerAdapter.Controller返回一个 ModelAndView 对象（包含模型数据、逻辑视图名）； ModelAndView 的逻辑视图名——&gt; ViewResolver， ViewResolver 将把逻辑视图名解析为具体的 View，通过这种策略模式，很容易更换其他视图技术； View——&gt;渲染，View 会根据传进来的 Model 模型数据进行渲染，此处的 Model 实际是一个 Map 数据结构，因此很容易支持其他视图技术； 返回控制权给 DispatcherServlet，由 DispatcherServlet 返回响应给用户，到此一个流程结束。 以上就是 Spring MVC 处理请求的全过程，上面的流程进行了一定的简化，比如拦截器的执行时机就没说。不过这并不影响大家对主过程的理解。下来来简单介绍一下图中出现的一些组件： 组件 说明 DispatcherServlet 本质上是一个 HttpServlet，Servlet 容器会把请求委托给它。Spring MVC 的核心组件，是请求的入口，负责协调各个组件工作 Handler 处理器，本质上是由实现 Controller 接口的类、实现 HttpRequestHandler 接口的类、注解@RequestMapping 的方法等封装而成的对象 HandlerMapping 内部维护了一些 &lt;访问路径, 处理器&gt; 映射，负责为请求找到合适的处理器 HandlerAdapter 处理器的适配器。Spring 中的处理器的实现多变，比如用户处理器可以实现 Controller 接口，实现 HttpRequestHandler 接口，也可以用 @RequestMapping 注解将方法作为一个处理器等，这就导致 Spring 不知道怎么调用用户的处理器逻辑。所以这里需要一个处理器适配器，由处理器适配器去调用处理器的逻辑 ViewResolver 用于将视图名称解析为视图对象 View。 View 在视图对象用于将模板渲染成 html 或其他类型的文件。比如 InternalResourceView 可将 jsp 渲染成 html。 2、源码分析上面我们知道了一个 HTTP 请求是怎么样被处理的，我们看到了核心部分就是前端控制器 DispatcherServlet。下面我们就从源码的角度对上面的内容进行补充说明，DispatcherServlet 中负责转发的核心方法就是 doDispatch。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // 获取可处理当前请求的处理器 Handler，对应流程图中的步骤2 mappedHandler = getHandler(processedRequest); if (mappedHandler == null || mappedHandler.getHandler() == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // 把 Controller、@RequestMapping 等包装为适配器，对应步骤3 HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // 处理 last-modified 消息头 String method = request.getMethod(); boolean isGet = \"GET\".equals(method); if (isGet || \"HEAD\".equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (logger.isDebugEnabled()) &#123; logger.debug(\"Last-Modified value for [\" + getRequestUri(request) + \"] is: \" + lastModified); &#125; if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; // 执行拦截器 preHandle 方法 if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // 调用处理器逻辑，对应步骤4，这一步得到的 ModelAndView 是数据对象（Map）和视图名称 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; // 如果 controller 未返回视图名称，这里生成默认的视图名称 applyDefaultViewName(processedRequest, mv); // 执行拦截器 postHandle 方法 mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; catch (Throwable err) &#123; dispatchException = new NestedServletException(\"Handler dispatch failed\", err); &#125; // 解析并渲染视图 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125; catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", err)); &#125; finally &#123; if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125; else &#123; if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536private void processDispatchResult(HttpServletRequest request, HttpServletResponse response, HandlerExecutionChain mappedHandler, ModelAndView mv, Exception exception) throws Exception &#123; boolean errorView = false; if (exception != null) &#123; if (exception instanceof ModelAndViewDefiningException) &#123; logger.debug(\"ModelAndViewDefiningException encountered\", exception); mv = ((ModelAndViewDefiningException) exception).getModelAndView(); &#125; else &#123; Object handler = (mappedHandler != null ? mappedHandler.getHandler() : null); mv = processHandlerException(request, response, handler, exception); errorView = (mv != null); &#125; &#125; if (mv != null &amp;&amp; !mv.wasCleared()) &#123; // 渲染视图 render(mv, request, response); if (errorView) &#123; WebUtils.clearErrorRequestAttributes(request); &#125; &#125; else &#123; if (logger.isDebugEnabled()) &#123;... &#125; if (WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; return; &#125; if (mappedHandler != null) &#123; mappedHandler.triggerAfterCompletion(request, response, null); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738protected void render(ModelAndView mv, HttpServletRequest request, HttpServletResponse response) throws Exception &#123; Locale locale = this.localeResolver.resolveLocale(request); response.setLocale(locale); View view; /* * 若 mv 中的 view 是 String 类型，即处理器返回的是模板名称， * 这里将其解析为具体的 View 对象 */ if (mv.isReference()) &#123; // 解析视图，根据视图名找到视图，对应步骤5 view = resolveViewName(mv.getViewName(), mv.getModelInternal(), locale, request); if (view == null) &#123; throw new ServletException(\"Could not resolve view with name '\" + mv.getViewName() + \"' in servlet with name '\" + getServletName() + \"'\"); &#125; &#125; else &#123; view = mv.getView(); if (view == null) &#123; throw new ServletException(\"ModelAndView [\" + mv + \"] neither contains a view name nor a \" + \"View object in servlet with name '\" + getServletName() + \"'\"); &#125; &#125; if (logger.isDebugEnabled()) &#123;...&#125; try &#123; if (mv.getStatus() != null) &#123; response.setStatus(mv.getStatus().value()); &#125; // 将模型中的数据填入视图中，并将结果返回给用户。对应步骤6和7 view.render(mv.getModelInternal(), request, response); &#125; catch (Exception ex) &#123; if (logger.isDebugEnabled()) &#123;...&#125; throw ex; &#125;&#125; 源码分析就到这里，我们再把流程总结一遍： 请求达到 DispatcherServlet 获取可处理当前请求的一个处理器 Handler 和多个拦截器 HandlerInterceptor 为当前获取的 Handler 找到合适的适配器，适配器的作用是提供统一调用接口 执行拦截器 preHandle 方法 执行适配器 handle 方法，即调用 handler 的对应处理方法，返回 ModelAndView，里面包含了模型数据和视图名称 执行拦截器 postHandle 方法 解析视图，根据视图名找到对应 view 对象 渲染视图，view 对象根据传进来的模型和模型数据进行渲染，返回 html 或其他类型的文件 返回响应 3、总结本文分析了 Spring MVC 的请求转发流程，介绍了 HttpServlet 和 DispatcherServlet 的转发流程。Java 底层只提供了 TCP 套接字通信，Http 协议的实现是由上层应用完成的。自己动手写个协议解析器很麻烦，因此 J2EE 规定了 HttpServlet 接口简化了我们的工作。只要我们实现了 HttpServlet 接口，协议解析与封装的事底层的 Servlet 容器帮我们做好了。DispatcherServlet 就是一个的 HttpServlet，也是 Spring MVC 的核心组件。在 SpringBoot 默认的配置中，它会拦截底层 Servlet 所有的 Http 请求，然后调用 IOC 容器中的各种组件响应并返回。 4、参考资料Spring MVC 原理探秘 - 一个请求的旅行过程spring mvc 处理流程整理","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring AOP 源码浅析——拦截器链的执行过程","slug":"Spring-AOP源码浅析——拦截器链的执行过程","date":"2019-03-20T06:02:24.000Z","updated":"2019-03-22T13:26:48.018Z","comments":true,"path":"2019/03/20/Spring-AOP源码浅析——拦截器链的执行过程/","link":"","permalink":"http://yoursite.com/2019/03/20/Spring-AOP源码浅析——拦截器链的执行过程/","excerpt":"","text":"在前面的两篇文章中，我们分别分析了 Spring AOP 是如何为目标 bean 筛选合适的通知器，以及如何创建代理对象的过程。现在我们得到了 bean 的代理对象，且通知也以合适的方式插在了目标方法的前后。接下来要做的事情，就是执行通知逻辑了。通知可能在目标方法前执行，也可能在目标方法后执行。具体的执行时机，取决于用户的配置。当目标方法被多个通知匹配到时，Spring 通过引入拦截器链来保证每个通知的正常执行。在本文中，我们将会通过源码了解到 Spring 是如何支持 expose-proxy 属性的，以及通知与拦截器之间的关系，拦截器链的执行过程等。 1、背景知识关于 expose-proxy，我们先来说说它有什么用，然后再来说说怎么用。Spring 引入 expose-proxy 特性是为了解决目标方法调用同对象中其他方法时，其他方法的切面逻辑无法执行的问题。如下：12345678910111213@Component(\"car\")public class CarImpl implements Car&#123; @Override public void run()&#123; System.out.println(\"car run...\"); &#125; @Override public void stop() &#123; this.run(); System.out.println(\"car stop\"); &#125;&#125; 当我们执行 stop 方法时，会去调用 run 方法。但这里调用的只是原始的 run 方法而不是被 AOP 增强过的。这是因为这里调用 run 方法的主体，是 this 而不是代理对象。但我们又不可能在编写代码的时候就获取到代理对象，因为代理对象是动态生成的。所以 Spring 提供了一种机制可以动态获取到当前的代理对象。如下：123456789@SpringBootApplication@EnableAspectJAutoProxy(exposeProxy = true)public class DemoApplication &#123; public static void main(String[] args) &#123; // ... &#125;&#125; 然后在调用处就可以获取到代理对象：1234567891011121314@Component(\"car\")public class CarImpl implements Car&#123; @Override public void run()&#123; System.out.println(\"car run...\"); &#125; @Override public void stop() &#123; // 获取到代理对象 ((Car)AopContext.currentProxy()).run(); System.out.println(\"car stop\"); &#125;&#125; 如上，AopContext.currentProxy()用于获取当前的代理对象。当 expose-proxy 被配置为 true 时，该代理对象会被放入 ThreadLocal 中，我们就可以获取到了。 2、源码分析本章所分析的源码来自 JdkDynamicAopProxy，至于 CglibAopProxy 中的源码，大家若有兴趣可以自己去看一下。 2.1 JDK 动态代理逻辑分析对于 JDK 动态代理，代理逻辑封装在 InvocationHandler 接口实现类的 invoke 方法中。JdkDynamicAopProxy 实现了 InvocationHandler 接口，下面我们就来分析一下 JdkDynamicAopProxy 的 invoke 方法。如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; MethodInvocation invocation; Object oldProxy = null; boolean setProxyContext = false; TargetSource targetSource = this.advised.targetSource; Class&lt;?&gt; targetClass = null; Object target = null; try &#123; // 省略部分代码 Object retVal; // 如果 expose-proxy 属性为 true，则暴露代理对象 if (this.advised.exposeProxy) &#123; // 向 AopContext 中设置代理对象 oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; // 获取适合当前方法的拦截器 List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); // 如果拦截器链为空，则直接执行目标方法 if (chain.isEmpty()) &#123; Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); // 通过反射执行目标方法 retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); &#125; else &#123; // 创建一个方法调用器，并将拦截器链传入其中 invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // 执行拦截器链 retVal = invocation.proceed(); &#125; // 获取方法返回值类型 Class&lt;?&gt; returnType = method.getReturnType(); if (retVal != null &amp;&amp; retVal == target &amp;&amp; returnType != Object.class &amp;&amp; returnType.isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; // 如果方法返回值为 this，即 return this; 则将代理对象 proxy 赋值给 retVal retVal = proxy; &#125; // 如果返回值类型为基础类型，比如 int，long 等，当返回值为 null，抛出异常 else if (retVal == null &amp;&amp; returnType != Void.TYPE &amp;&amp; returnType.isPrimitive()) &#123; throw new AopInvocationException( \"Null return value from advice does not match primitive return type for: \" + method); &#125; return retVal; &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; targetSource.releaseTarget(target); &#125; if (setProxyContext) &#123; AopContext.setCurrentProxy(oldProxy); &#125; &#125;&#125; 如上，上面的代码我做了比较详细的注释。下面我们来总结一下 invoke 方法的执行流程，如下： 检测 expose-proxy 是否为 true，若为 true，则暴露代理对象 获取适合当前方法的拦截器 如果拦截器链为空，则直接通过反射执行目标方法 若拦截器链不为空，则创建方法调用 ReflectiveMethodInvocation 对象 调用 ReflectiveMethodInvocation 对象的 proceed() 方法启动拦截器链 处理返回值，并返回该值 在以上6步中，我们重点关注第2步和第5步中的逻辑。第2步用于获取拦截器链，第5步则是启动拦截器链。下面先来分析获取拦截器链的过程。 2.2 获取所有的拦截器所谓的拦截器，顾名思义，是指用于对目标方法的调用进行拦截的一种工具。拦截器的源码比较简单，所以我们直接看源码好了。下面以前置通知拦截器为例，如下：123456789101112131415161718public class MethodBeforeAdviceInterceptor implements MethodInterceptor, Serializable &#123; /** 前置通知 */ private MethodBeforeAdvice advice; public MethodBeforeAdviceInterceptor(MethodBeforeAdvice advice) &#123; Assert.notNull(advice, \"Advice must not be null\"); this.advice = advice; &#125; @Override public Object invoke(MethodInvocation mi) throws Throwable &#123; // 执行前置通知逻辑 this.advice.before(mi.getMethod(), mi.getArguments(), mi.getThis()); // 通过 MethodInvocation 调用下一个拦截器，若所有拦截器均执行完，则调用目标方法 return mi.proceed(); &#125;&#125; 如上，前置通知的逻辑在目标方法执行前被执行。这里先简单介绍一下拦截器是什么，关于拦截器更多的描述将放在下一节中。本节我们先来看看如何如何获取拦截器，如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice(Method method, Class&lt;?&gt; targetClass) &#123; MethodCacheKey cacheKey = new MethodCacheKey(method); // 从缓存中获取 List&lt;Object&gt; cached = this.methodCache.get(cacheKey); // 缓存未命中，则进行下一步处理 if (cached == null) &#123; // 获取所有的拦截器 cached = this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice( this, method, targetClass); // 存入缓存 this.methodCache.put(cacheKey, cached); &#125; return cached;&#125;public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice( Advised config, Method method, Class&lt;?&gt; targetClass) &#123; List&lt;Object&gt; interceptorList = new ArrayList&lt;Object&gt;(config.getAdvisors().length); Class&lt;?&gt; actualClass = (targetClass != null ? targetClass : method.getDeclaringClass()); boolean hasIntroductions = hasMatchingIntroductions(config, actualClass); // registry 为 DefaultAdvisorAdapterRegistry 类型 AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance(); // 遍历通知器列表 for (Advisor advisor : config.getAdvisors()) &#123; if (advisor instanceof PointcutAdvisor) &#123; PointcutAdvisor pointcutAdvisor = (PointcutAdvisor) advisor; // 调用 ClassFilter 对 bean 类型进行匹配，无法匹配则说明当前通知器不适合应用在当前 bean 上 if (config.isPreFiltered() || pointcutAdvisor.getPointcut().getClassFilter().matches(actualClass)) &#123; // 将 advisor 中的 advice 转成相应的拦截器 MethodInterceptor[] interceptors = registry.getInterceptors(advisor); MethodMatcher mm = pointcutAdvisor.getPointcut().getMethodMatcher(); // 通过方法匹配器对目标方法进行匹配 if (MethodMatchers.matches(mm, method, actualClass, hasIntroductions)) &#123; // 若 isRuntime 返回 true，则表明 MethodMatcher 要在运行时做一些检测 if (mm.isRuntime()) &#123; for (MethodInterceptor interceptor : interceptors) &#123; interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm)); &#125; &#125; else &#123; interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; &#125; &#125; else if (advisor instanceof IntroductionAdvisor) &#123; IntroductionAdvisor ia = (IntroductionAdvisor) advisor; // IntroductionAdvisor 类型的通知器，仅需进行类级别的匹配即可 if (config.isPreFiltered() || ia.getClassFilter().matches(actualClass)) &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; else &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; return interceptorList;&#125;public MethodInterceptor[] getInterceptors(Advisor advisor) throws UnknownAdviceTypeException &#123; List&lt;MethodInterceptor&gt; interceptors = new ArrayList&lt;MethodInterceptor&gt;(3); Advice advice = advisor.getAdvice(); // 若 advice 是 MethodInterceptor 类型的，直接添加到 interceptors 中即可。比如 AspectJAfterAdvice 就实现了 MethodInterceptor 接口 if (advice instanceof MethodInterceptor) &#123; interceptors.add((MethodInterceptor) advice); &#125; // 对于 AspectJMethodBeforeAdvice 等类型的通知，由于没有实现 MethodInterceptor 接口，所以这里需要通过适配器进行转换 for (AdvisorAdapter adapter : this.adapters) &#123; if (adapter.supportsAdvice(advice)) &#123; interceptors.add(adapter.getInterceptor(advisor)); &#125; &#125; if (interceptors.isEmpty()) &#123; throw new UnknownAdviceTypeException(advisor.getAdvice()); &#125; return interceptors.toArray(new MethodInterceptor[interceptors.size()]);&#125; 以上就是获取拦截器的过程，代码有点长，不过好在逻辑不是很复杂。这里简单总结一下以上源码的执行过程，如下： 从缓存中获取当前方法的拦截器链 若缓存未命中，则调用 getInterceptorsAndDynamicInterceptionAdvice 获取拦截器链 遍历通知器列表 对于 PointcutAdvisor 类型的通知器，这里要调用通知器所持有的切点（Pointcut）对类和方法进行匹配，匹配成功说明应向当前方法织入通知逻辑 调用 getInterceptors 方法对非 MethodInterceptor 类型的通知进行转换 返回拦截器数组，并在随后存入缓存中 这里需要说明一下，部分通知器是没有实现 MethodInterceptor 接口的，比如 AspectJMethodBeforeAdvice。我们可以看一下前置通知适配器是如何将前置通知转为拦截器的，如下：1234567891011121314class MethodBeforeAdviceAdapter implements AdvisorAdapter, Serializable &#123; @Override public boolean supportsAdvice(Advice advice) &#123; return (advice instanceof MethodBeforeAdvice); &#125; @Override public MethodInterceptor getInterceptor(Advisor advisor) &#123; MethodBeforeAdvice advice = (MethodBeforeAdvice) advisor.getAdvice(); // 创建 MethodBeforeAdviceInterceptor 拦截器 return new MethodBeforeAdviceInterceptor(advice); &#125;&#125; 如上，适配器的逻辑比较简单，这里就不多说了。现在我们已经获得了拦截器链，那接下来要做的事情就是启动拦截器了。所以接下来，我们一起去看看 Sring 是如何让拦截器链运行起来的。 2.3 启动拦截器链2.3.1 执行拦截器链本节的开始，我们先来说说 ReflectiveMethodInvocation。ReflectiveMethodInvocation 贯穿于拦截器链执行的始终，可以说是核心。该类的 proceed 方法用于启动启动拦截器链，下面我们去看看这个方法的逻辑。123456789101112131415161718192021222324252627282930313233public class ReflectiveMethodInvocation implements ProxyMethodInvocation &#123; private int currentInterceptorIndex = -1; public Object proceed() throws Throwable &#123; // 拦截器链中的最后一个拦截器执行完后，即可执行目标方法 if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) &#123; // 执行目标方法 return invokeJoinpoint(); &#125; Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) &#123; InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; /* * 调用具有三个参数（3-args）的 matches 方法动态匹配目标方法， * 两个参数（2-args）的 matches 方法用于静态匹配 */ if (dm.methodMatcher.matches(this.method, this.targetClass, this.arguments)) &#123; // 调用拦截器逻辑 return dm.interceptor.invoke(this); &#125; else &#123; // 如果匹配失败，则忽略当前的拦截器 return proceed(); &#125; &#125; else &#123; // 调用拦截器逻辑，并传递 ReflectiveMethodInvocation 对象 return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); &#125; &#125;&#125; 如上，proceed 根据 currentInterceptorIndex 来确定当前应执行哪个拦截器，并在调用拦截器的 invoke 方法时，将自己作为参数传给该方法。前面的章节中，我们看过了前置拦截器的源码，这里来看一下后置拦截器源码。如下：123456789101112131415161718192021public class AspectJAfterAdvice extends AbstractAspectJAdvice implements MethodInterceptor, AfterAdvice, Serializable &#123; public AspectJAfterAdvice(Method aspectJBeforeAdviceMethod, AspectJExpressionPointcut pointcut, AspectInstanceFactory aif) &#123; super(aspectJBeforeAdviceMethod, pointcut, aif); &#125; @Override public Object invoke(MethodInvocation mi) throws Throwable &#123; try &#123; // 调用 proceed return mi.proceed(); &#125; finally &#123; // 调用后置通知逻辑 invokeAdviceMethod(getJoinPointMatch(), null, null); &#125; &#125; //...&#125; 如上，由于后置通知需要在目标方法返回后执行，所以 AspectJAfterAdvice 先调用 mi.proceed() 执行下一个拦截器逻辑，等下一个拦截器返回后，再执行后置通知逻辑。如果大家不太理解的话，先看个图。这里假设目标方法 method 在执行前，需要执行两个前置通知和一个后置通知。下面我们看一下由三个拦截器组成的拦截器链是如何执行的，如下： 2.3.2 执行目标方法最后是目标方法的执行，执行过程比较简单，如下：123456789101112131415161718protected Object invokeJoinpoint() throws Throwable &#123; return AopUtils.invokeJoinpointUsingReflection(this.target, this.method, this.arguments);&#125;public abstract class AopUtils &#123; public static Object invokeJoinpointUsingReflection(Object target, Method method, Object[] args) throws Throwable &#123; try &#123; ReflectionUtils.makeAccessible(method); // 通过反射执行目标方法 return method.invoke(target, args); &#125; catch (InvocationTargetException ex) &#123;...&#125; catch (IllegalArgumentException ex) &#123;...&#125; catch (IllegalAccessException ex) &#123;...&#125; &#125;&#125; 目标方法时通过反射执行的，比较简单的吧。好了，就不多说了，over。 3、总结AOP相关的文章到这里就结束了，我们最后来回顾一下拦截器链的执行过程。在执行拦截器链的时候，我们已经生成了代理对象，在 JDK 方式下这个代理对象内部包含了一个 JdkDynamicAopProxy 对象，这是一个实现了 InvocationHandler，所以在我们调用代理对象的被增强的方法是就是在直接执行 JdkDynamicAopProxy 的 invoke 方法。JdkDynamicAopProxy 是生成代理对象的时候传送进来的，它里面包含了所有匹配到这个代理对象的 advisor。我们在调用方法的时候，会进入 JdkDynamicAopProxy 的 invoke 方法，这个方法会首先获取匹配当前方法的拦截器，所谓拦截器是由 advisor 生成的，里面包含了那个 advisor 的 advice。生成后的拦截器链会加入缓存中，下次调用该方法时就不用重新生成。拦截器的执行是通过责任链模式执行，责任链是由一个 MethodInterceptor 对象的 process 方法发起，调用各拦截器的 invoke，再由 invoke 回调 process 方法来执行的。不同拦截器的触发时间不一样，这个是由拦截器内部调用回调方法 process 的时机决定的。process 最后会通过反射执行目标方法，这不代表目标方法一定会在通知逻辑之后执行，因为有些拦截器会把通知逻辑放在回调函数之后执行。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring AOP 源码浅析——生成代理对象","slug":"Spring-AOP源码浅析——生成代理对象","date":"2019-03-20T04:13:32.000Z","updated":"2019-03-22T13:26:58.122Z","comments":true,"path":"2019/03/20/Spring-AOP源码浅析——生成代理对象/","link":"","permalink":"http://yoursite.com/2019/03/20/Spring-AOP源码浅析——生成代理对象/","excerpt":"","text":"在上一篇文章中，我分析了 Spring 是如何为目标 bean 筛选合适的通知器的。现在通知器选好了，接下来就要通过代理的方式将通知器（Advisor）所持有的通知（Advice）织入到 bean 的某些方法前后。与筛选合适的通知器相比，创建代理对象的过程则要简单不少。 1、背景知识1.1 proxy-target-class在 Spring AOP 配置中，proxyTargetClass 属性可影响 Spring 生成的代理对象的类型。以注解配置为例，如下：12345@SpringBootApplication@EnableAspectJAutoProxy(proxyTargetClass = true)public class DemoApplication &#123; // ...&#125; 如上，默认情况下 proxy-target-class 属性为 false。当目标 bean 实现了接口时，Spring 会基于 JDK 动态代理为目标 bean 创建代理对象。若未实现任何接口，Spring 则会通过 CGLIB 创建代理。而当 proxy-target-class 属性设为 true 时，则会强制 Spring 通过 CGLIB 的方式创建代理对象，即使目标 bean 实现了接口。 1.2 基于 JDK 的动态代理基于 JDK 的动态代理主要是通过 JDK 提供的代理创建类 Proxy 为目标对象创建代理，下面我们来看一下 Proxy 中创建代理的方法声明。如下：1public static Object newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) 简单说一下上面的参数列表： loader - 类加载器 interfaces - 目标类所实现的接口列表 h - 用于封装代理逻辑 JDK 动态代理对目标类是有一定要求的，即要求目标类必须实现了接口，JDK 动态代理只能为实现了接口的目标类生成代理对象。至于 InvocationHandler，是一个接口类型，定义了一个 invoke 方法。使用者需要实现该方法，并在其中封装代理逻辑。我们来试一下 JDK 动态代理的使用方式，如下：目标类定义：1234567891011121314151617public interface UserService &#123; void save(); void update();&#125;public class UserServiceImpl implements UserService &#123; @Override public void save() &#123; System.out.println(\"save user info\"); &#125; @Override public void update() &#123; System.out.println(\"update user info\"); &#125;&#125; InvocationHandler 定义：123456789101112131415161718public class JdkProxyCreator implements InvocationHandler &#123; Object object; public JdkProxyCreator(Object object) &#123; this.object = object; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(System.currentTimeMillis() + \" - \" + method.getName() + \" method start\"); // 调用目标方法 Object retVal = method.invoke(object, args); System.out.println(System.currentTimeMillis() + \" - \" + method.getName() + \" method over\"); return retVal; &#125;&#125; 如上，invoke 方法中的代理逻辑主要用于记录目标方法的调用时间，和结束时间。下面写点测试代码简单验证一下，如下：123456789public static void main(String[] args) &#123; UserService userService = new UserServiceImpl(); JdkProxyCreator jdkProxyCreator = new JdkProxyCreator(userService); UserService userServiceProxy = (UserService)Proxy.newProxyInstance(userService.getClass().getClassLoader(), userService.getClass().getInterfaces(), jdkProxyCreator); userServiceProxy.save(); userServiceProxy.update();&#125; 结果如下：1234561553059153309 - save method startsave user info1553059153309 - save method over1553059153309 - update method startupdate user info1553059153309 - update method over 如上，可以看到我们的代码正常运行了。 1.3 基于 CGLIB 的动态代理当我们要为未实现接口的类生成代理时，就无法使用 JDK 动态代理了。那么此类的目标对象生成代理时应该怎么办呢？当然是使用 CGLIB 了。在 CGLIB 中，代理逻辑是封装在 MethodInterceptor 实现类中的，代理对象则是通过 Enhancer 类的 create 方法进行创建。下面我来演示一下 CGLIB 创建代理对象的过程，如下：目标类：123456789public class UserService &#123; public void save() &#123; System.out.println(\"save user info\"); &#125; public void update() &#123; System.out.println(\"update user info\"); &#125;&#125; 方法拦截器:12345678910public class UserMethodInterceptor implements MethodInterceptor &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println(System.currentTimeMillis() + \" - \" + method.getName() + \" method start\"); // 调用目标方法 Object retVal = methodProxy.invokeSuper(o, objects); System.out.println(System.currentTimeMillis() + \" - \" + method.getName() + \" method over\"); return retVal; &#125;&#125; 测试类：12345678910111213public static void main(String[] args) &#123; UserService userService = new UserService(); // 代理创建者 Enhancer enhancer = new Enhancer(); // 设置代理类的父类 enhancer.setSuperclass(userService.getClass()); // 设置代理逻辑 enhancer.setCallback(new UserMethodInterceptor()); // 创建代理对象 UserService userServiceProxy = (UserService) enhancer.create(); userServiceProxy.save(); userServiceProxy.update();&#125; 结果如下：1234561553060158279 - save method startsave user info1553060158289 - save method over1553060158289 - update method startupdate user info1553060158289 - update method over 如上，说明我的代理成功了。下面就开始源码的分析。 2、源码分析为目标 bean 创建代理对象前，需要先创建 AopProxy 对象，然后再调用该对象的 getProxy 方法创建实际的代理类。我们先来看看 AopProxy 这个接口的定义，如下：1234567public interface AopProxy &#123; /** 创建代理对象 */ Object getProxy(); Object getProxy(ClassLoader classLoader);&#125; 在 Spring 中，有两个类实现了 AopProxy，如下：Spring 在为目标 bean 创建代理的过程中，要根据 bean 是否实现接口，以及一些其他配置来决定使用 AopProxy 何种实现类为目标 bean 创建代理对象。下面我们就来看一下代理创建的过程，如下：123456789101112131415161718192021222324252627282930313233343536373839protected Object createProxy(Class&lt;?&gt; beanClass, String beanName, Object[] specificInterceptors, TargetSource targetSource) &#123; if (this.beanFactory instanceof ConfigurableListableBeanFactory) &#123; AutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass); &#125; ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.copyFrom(this); // 默认配置下，或用户显式配置 proxyTargetClass = \"false\" 时，这里的 proxyFactory.isProxyTargetClass() 也为 false if (!proxyFactory.isProxyTargetClass()) &#123; if (shouldProxyTargetClass(beanClass, beanName)) &#123; proxyFactory.setProxyTargetClass(true); &#125; else &#123; // 检测 beanClass 是否实现了接口，若未实现，则将 proxyFactory 的成员变量 proxyTargetClass 设为 true evaluateProxyInterfaces(beanClass, proxyFactory); &#125; &#125; // specificInterceptors 中若包含有 Advice，此处将 Advice 转为 Advisor Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); proxyFactory.addAdvisors(advisors); proxyFactory.setTargetSource(targetSource); customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) &#123; proxyFactory.setPreFiltered(true); &#125; // 创建代理 return proxyFactory.getProxy(getProxyClassLoader());&#125;public Object getProxy(ClassLoader classLoader) &#123; // 先创建 AopProxy 实现类对象，然后再调用 getProxy 为目标 bean 创建代理对象 return createAopProxy().getProxy(classLoader);&#125; getProxy 这里有两个方法调用，一个是调用 createAopProxy 创建 AopProxy 实现类对象，然后再调用 AopProxy 实现类对象中的 getProxy 创建代理对象。这里我们先来看一下创建 AopProxy 实现类对象的过程，如下：123456789101112131415161718192021222324252627282930313233343536protected final synchronized AopProxy createAopProxy() &#123; if (!this.active) &#123; activate(); &#125; return getAopProxyFactory().createAopProxy(this);&#125;public class DefaultAopProxyFactory implements AopProxyFactory, Serializable &#123; @Override public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; /* * 下面的三个条件简单分析一下： * * 条件1：config.isOptimize() - 是否需要优化，这个属性不常用 * 条件2：config.isProxyTargetClass() - 检测 proxyTargetClass 的值，前面的代码会设置这个值 * 条件3：hasNoUserSuppliedProxyInterfaces(config) - 目标 bean 是否实现了接口 */ if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(\"TargetSource cannot determine target class: \" + \"Either an interface or a target is required for proxy creation.\"); &#125; if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config); &#125; // 创建 CGLIB 代理，ObjenesisCglibAopProxy 继承自 CglibAopProxy return new ObjenesisCglibAopProxy(config); &#125; else &#123; // 创建 JDK 动态代理 return new JdkDynamicAopProxy(config); &#125; &#125;&#125; 如上，DefaultAopProxyFactory 根据一些条件决定生成什么类型的 AopProxy 实现类对象。生成好 AopProxy 实现类对象后，下面就要为目标 bean 创建代理对象了。这里以 JdkDynamicAopProxy 为例，我们来看一下，该类的 getProxy 方法的逻辑是怎样的。如下：1234567891011121314public Object getProxy() &#123; return getProxy(ClassUtils.getDefaultClassLoader());&#125;public Object getProxy(ClassLoader classLoader) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"Creating JDK dynamic proxy: target source is \" + this.advised.getTargetSource()); &#125; Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); // 调用 newProxyInstance 创建代理对象 return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this);&#125; 如上，请把目光移至最后一行有效代码上，会发现 JdkDynamicAopProxy 最终调用 Proxy.newProxyInstance 方法创建代理对象。到此，创建代理对象的整个过程也就分析完了，不知大家看懂了没。好了，关于创建代理的源码分析，就先说到这里吧。 3、总结本篇文章对 Spring AOP 创建代理对象的过程进行了较为详细的分析，并在分析源码前介绍了相关的背景知识。总的来说，本篇文章涉及的技术点不是很复杂，相信大家都能看懂。限于个人能力，若文中有错误的地方，欢迎大家指出来。好了，本篇文章到此结束，谢谢阅读。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring AOP 源码浅析——筛选合适的通知器","slug":"Spring-AOP源码浅析——筛选合适的通知器","date":"2019-03-19T07:33:18.000Z","updated":"2019-03-28T12:09:06.492Z","comments":true,"path":"2019/03/19/Spring-AOP源码浅析——筛选合适的通知器/","link":"","permalink":"http://yoursite.com/2019/03/19/Spring-AOP源码浅析——筛选合适的通知器/","excerpt":"","text":"上篇我们讲过了 AOP 的概念和使用方式。这篇就开始进行源码分析。 1、AOP 入口分析上篇我们提到过，AOP 在 Spring 中是通过后置处理器的方式织入到对应的 bean 中的。负责这部分逻辑的是后置处理器 AnnotationAwareAspectJAutoProxyCreator ，但由于 AnnotationAwareAspectJAutoProxyCreator 中没有覆写父类的 postProcessAfterInitialization 方法，所以我们找到的入口在它的父类 AbstractAutoProxyCreator 中，如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public abstract class AbstractAutoProxyCreator extends ProxyProcessorSupport implements SmartInstantiationAwareBeanPostProcessor, BeanFactoryAware &#123; @Override /** bean 初始化后置处理方法 */ public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (bean != null) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.contains(cacheKey)) &#123; // 如果需要，为 bean 生成代理对象 return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean; &#125; protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; if (beanName != null &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; // 如果是基础设施类（Pointcut、Advice、Advisor 等接口的实现类），或是应该跳过的类，则不应该生成代理，此时直接返回 bean if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; // 将 &lt;cacheKey, FALSE&gt; 键值对放入缓存中，供上面的 if 分支使用 this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; &#125; // 为目标 bean 查找合适的通知器 Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); // 若 specificInterceptors != null，即 specificInterceptors != DO_NOT_PROXY，则为 bean 生成代理对象，否则直接返回 bean if (specificInterceptors != DO_NOT_PROXY) &#123; this.advisedBeans.put(cacheKey, Boolean.TRUE); // 创建代理 Object proxy = createProxy(bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); // 返回代理对象 return proxy; &#125; this.advisedBeans.put(cacheKey, Boolean.FALSE); // specificInterceptors = null，直接返回 bean return bean; &#125;&#125; 以上就是 Spring AOP 创建代理对象的入口方法分析，过程比较简单，这里简单总结一下： 若 bean 是 AOP 基础设施类型，则直接返回 为 bean 筛选合适的通知器 如果通知器数组不为空，则为 bean 生成代理对象，并返回该对象 若数组为空，则返回原始 bean 上面的流程看起来并不复杂，不过不要被表象所迷糊，以上流程不过是冰山一角。筛选合适的通知器和生成代理对象这两步将是我们重点关注的。 2、查找合适的通知器在向目标 bean 中织入通知之前，我们先要为 bean 筛选出合适的通知器（通知器持有通知）。如何筛选呢？方式有很多，比如我们可以通过正则表达式匹配方法名，当然更多的时候用的是 AspectJ 表达式进行匹配。那下面我们就来看一下使用 AspectJ 表达式筛选通知器的过程，如下：123456789101112131415161718192021protected Object[] getAdvicesAndAdvisorsForBean(Class&lt;?&gt; beanClass, String beanName, TargetSource targetSource) &#123; // 查找合适的通知器 List&lt;Advisor&gt; advisors = findEligibleAdvisors(beanClass, beanName); if (advisors.isEmpty()) &#123; return DO_NOT_PROXY; &#125; return advisors.toArray();&#125;protected List&lt;Advisor&gt; findEligibleAdvisors(Class&lt;?&gt; beanClass, String beanName) &#123; // 查找所有的通知器 List&lt;Advisor&gt; candidateAdvisors = findCandidateAdvisors(); // 筛选可应用在 beanClass 上的 Advisor，通过 ClassFilter 和 MethodMatcher 对目标类和方法进行匹配 List&lt;Advisor&gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); // 拓展操作 extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) &#123; eligibleAdvisors = sortAdvisors(eligibleAdvisors); &#125; return eligibleAdvisors;&#125; 2.1 查找所有的通知器Spring 提供了两种配置 AOP 的方式，一种是通过 XML 进行配置，另一种是注解。对于两种配置方式，Spring 的处理逻辑是不同的。如下面的 xml 方式的配置：1234567public class LogBeforeAdvice implements MethodBeforeAdvice&#123; @Override public void before(Method method, Object[] args, Object target) throws Throwable &#123; System.out.println(\"方法调用之前\"); &#125;&#125; 123456789&lt;bean id=\"logBeforeAdvice\" class=\"com.huzb.demo.LogBeforeAdvice\"/&gt;&lt;bean id=\"logBeforeAdvisor\" class=\"org.springframework.aop.aspectj.AspectJExpressionPointcutAdvisor\"&gt; &lt;property name=\"advice\" ref=\"logBeforeAdvice\" /&gt; &lt;property name=\"expression\" value=\"execution(* com.huzb.demo.CarImpl.run(..))\" /&gt; &lt;/bean&gt;&lt;!--使所有Advisor生效--&gt;&lt;bean class=\"org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator\" /&gt; 它和下面的注解方式是等价的：123456789101112131415161718@Component@Aspectpublic class LogAspect &#123; @Pointcut(\"execution(* com.huzb.demo.CarImpl.run(..))\") public void pointCut() &#123; &#125; @Before(\"pointCut()\") public void logStart(JoinPoint joinPoint) &#123; Object[] args = joinPoint.getArgs(); System.out.println(\"方法调用之前\"); &#125; @AfterReturning(value = \"pointCut()\", returning = \"result\") public void logReturn(JoinPoint joinPoint, Object result) &#123; System.out.println(\"方法返回的结果为：\" + result.toString()); &#125;&#125; 上述两种配置下，Spring 除了生成一个普通的 bean 对象外，还会生成一个类型为 AspectJExpressionPointcut 的对象和两个类型为 AspectJPointcutAdvisor（注意不是 Advice，是 Advisor，这是一种只包含一个 Advice 和一个 Pointcut 的特殊切面，可以叫它通知器） 的对象，分别表示切点和通知器。也就是说，在 Spring 中，切点和通知器是被当成单独的 bean 加入容器的，尽管它们可能并不作为一个类被定义。下面让我们看一下源码：123456789101112131415public class AnnotationAwareAspectJAutoProxyCreator extends AspectJAwareAdvisorAutoProxyCreator &#123; //... @Override protected List&lt;Advisor&gt; findCandidateAdvisors() &#123; // 调用父类方法从容器中查找类型为 Advisor 的通知器 List&lt;Advisor&gt; advisors = super.findCandidateAdvisors(); // 解析 @Aspect 注解，并构建通知器 advisors.addAll(this.aspectJAdvisorsBuilder.buildAspectJAdvisors()); return advisors; &#125; //...&#125; AnnotationAwareAspectJAutoProxyCreator 覆写了父类的方法 findCandidateAdvisors，并增加了一步操作，即解析 @Aspect 注解，并构建成通知器。下面我们先来分析一下父类中的 findCandidateAdvisors 方法的逻辑，然后再来分析 buildAspectJAdvisors 方法的逻辑。 2.1.1 findCandidateAdvisors 方法分析我们先来看一下 AbstractAdvisorAutoProxyCreator 中 findCandidateAdvisors 方法的定义，如下：123456789101112public abstract class AbstractAdvisorAutoProxyCreator extends AbstractAutoProxyCreator &#123; private BeanFactoryAdvisorRetrievalHelper advisorRetrievalHelper; //... protected List&lt;Advisor&gt; findCandidateAdvisors() &#123; return this.advisorRetrievalHelper.findAdvisorBeans(); &#125; //...&#125; 从上面的源码中可以看出，AbstractAdvisorAutoProxyCreator 中的 findCandidateAdvisors 是个空壳方法，所有逻辑封装在了一个 BeanFactoryAdvisorRetrievalHelper 的 findAdvisorBeans 方法中。这里大家可以仔细看一下类名 BeanFactoryAdvisorRetrievalHelper 和方法 findAdvisorBeans，两个名字其实已经描述出他们的职责了。BeanFactoryAdvisorRetrievalHelper 可以理解为从 bean 容器中获取 Advisor 的帮助类，findAdvisorBeans 则可理解为查找 Advisor 类型的 bean。所以即使不看 findAdvisorBeans 方法的源码，我们也可从方法名上推断出它要做什么，即从 bean 容器中将 Advisor 类型的 bean 查找出来。下面我们来分析一下这个方法的源码，如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public List&lt;Advisor&gt; findAdvisorBeans() &#123; String[] advisorNames = null; synchronized (this) &#123; // cachedAdvisorBeanNames 是 advisor 名称的缓存 advisorNames = this.cachedAdvisorBeanNames; // 如果 cachedAdvisorBeanNames 为空，这里到容器中查找，并设置缓存，后续直接使用缓存即可 if (advisorNames == null) &#123; // 从容器中查找所有 Advisor 类型 bean 的名称 advisorNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors(this.beanFactory, Advisor.class, true, false); // 设置缓存 this.cachedAdvisorBeanNames = advisorNames; &#125; &#125; if (advisorNames.length == 0) &#123; return new LinkedList&lt;Advisor&gt;(); &#125; List&lt;Advisor&gt; advisors = new LinkedList&lt;Advisor&gt;(); // 遍历 advisorNames for (String name : advisorNames) &#123; if (isEligibleBean(name)) &#123; // 忽略正在创建中的 advisor bean if (this.beanFactory.isCurrentlyInCreation(name)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"Skipping currently created advisor '\" + name + \"'\"); &#125; &#125; else &#123; try &#123; // 调用 getBean 方法从容器中获取或创建名称为 name 的 bean，并将 bean 添加到 advisors 中 advisors.add(this.beanFactory.getBean(name, Advisor.class)); &#125; catch (BeanCreationException ex) &#123; Throwable rootCause = ex.getMostSpecificCause(); if (rootCause instanceof BeanCurrentlyInCreationException) &#123; BeanCreationException bce = (BeanCreationException) rootCause; if (this.beanFactory.isCurrentlyInCreation(bce.getBeanName())) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"Skipping advisor '\" + name + \"' with dependency on currently created bean: \" + ex.getMessage()); &#125; continue; &#125; &#125; throw ex; &#125; &#125; &#125; &#125; return advisors;&#125; 以上就是从容器中查找 Advisor 类型的 bean 所有的逻辑，代码虽然有点长，但并不复杂。主要做了三件事情： 从缓存中获取所有 Advisor 的名称 如果缓存中没有，就从容器中查找所有类型为 Advisor 的 bean 对应的名称，然后放入缓存 遍历 advisorNames，使用 getBean 方法创建或从容器中获取对应的 bean 看完上面的分析，我们继续来分析一下 @Aspect 注解的解析过程。 2.1.2 buildAspectJAdvisors 方法分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public List&lt;Advisor&gt; buildAspectJAdvisors() &#123; // aspectNames 中存放的是包含 Aspect 注解的类名，这个值不为空说明已经解析过 List&lt;String&gt; aspectNames = this.aspectBeanNames; // 缓存中没有说明还未初始化，则先初始化 if (aspectNames == null) &#123; synchronized (this) &#123; aspectNames = this.aspectBeanNames; if (aspectNames == null) &#123; List&lt;Advisor&gt; advisors = new LinkedList&lt;Advisor&gt;(); aspectNames = new LinkedList&lt;String&gt;(); // 从容器中获取所有 bean 的名称 String[] beanNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors( this.beanFactory, Object.class, true, false); // 遍历 beanNames for (String beanName : beanNames) &#123; if (!isEligibleBean(beanName)) &#123; continue; &#125; // 根据 beanName 获取 bean 的类型 Class&lt;?&gt; beanType = this.beanFactory.getType(beanName); if (beanType == null) &#123; continue; &#125; // 检测 beanType 是否包含 Aspect 注解 if (this.advisorFactory.isAspect(beanType)) &#123; aspectNames.add(beanName); AspectMetadata amd = new AspectMetadata(beanType, beanName); if (amd.getAjType().getPerClause().getKind() == PerClauseKind.SINGLETON) &#123; MetadataAwareAspectInstanceFactory factory = new BeanFactoryAspectInstanceFactory(this.beanFactory, beanName); // 获取通知器 List&lt;Advisor&gt; classAdvisors = this.advisorFactory.getAdvisors(factory); if (this.beanFactory.isSingleton(beanName)) &#123; // 一个包含 Aspect 注解对应多个 Advisor，将这些 Advisor 缓存起来 this.advisorsCache.put(beanName, classAdvisors); &#125; else &#123; this.aspectFactoryCache.put(beanName, factory); &#125; advisors.addAll(classAdvisors); &#125; else &#123; if (this.beanFactory.isSingleton(beanName)) &#123; throw new IllegalArgumentException(\"Bean with name '\" + beanName + \"' is a singleton, but aspect instantiation model is not singleton\"); &#125; MetadataAwareAspectInstanceFactory factory = new PrototypeAspectInstanceFactory(this.beanFactory, beanName); this.aspectFactoryCache.put(beanName, factory); advisors.addAll(this.advisorFactory.getAdvisors(factory)); &#125; &#125; &#125; // 将包含 Aspect 注解的类名保存起来，避免重复解析 this.aspectBeanNames = aspectNames; return advisors; &#125; &#125; &#125; if (aspectNames.isEmpty()) &#123; return Collections.emptyList(); &#125; List&lt;Advisor&gt; advisors = new LinkedList&lt;Advisor&gt;(); for (String aspectName : aspectNames) &#123; // 从缓存中获取 List&lt;Advisor&gt; cachedAdvisors = this.advisorsCache.get(aspectName); if (cachedAdvisors != null) &#123; advisors.addAll(cachedAdvisors); &#125; else &#123; MetadataAwareAspectInstanceFactory factory = this.aspectFactoryCache.get(aspectName); advisors.addAll(this.advisorFactory.getAdvisors(factory)); &#125; &#125; return advisors;&#125; 上面就是 buildAspectJAdvisors 的代码，看起来比较长。代码比较多，我们关注重点的方法调用即可。在进行后续的分析前，这里先对 buildAspectJAdvisors 方法的执行流程做个总结。如下： 检查是否已经解析过，已经解析过的话拿到所有包含 Aspect 注解的类名，跳到步骤6 获取容器中所有 bean 的名称（beanName）和类型 根据 beanType 判断当前 bean 是否是一个包含 Aspect 注解的类，如果是的话调用 advisorFactory.getAdvisors 获取通知器 将通知器保存在缓存中 设置 this.aspectBeanNames 为所有包含 Aspect 注解的类名 按 this.aspectBeanNames 从缓存中获取所有通知器 下面我们来重点分析 advisorFactory.getAdvisors(factory) 这个调用，如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public List&lt;Advisor&gt; getAdvisors(MetadataAwareAspectInstanceFactory aspectInstanceFactory) &#123; // 获取 aspectClass 和 aspectName Class&lt;?&gt; aspectClass = aspectInstanceFactory.getAspectMetadata().getAspectClass(); String aspectName = aspectInstanceFactory.getAspectMetadata().getAspectName(); validate(aspectClass); MetadataAwareAspectInstanceFactory lazySingletonAspectInstanceFactory = new LazySingletonAspectInstanceFactoryDecorator(aspectInstanceFactory); List&lt;Advisor&gt; advisors = new LinkedList&lt;Advisor&gt;(); // getAdvisorMethods 用于返回不包含 @Pointcut 注解的方法 for (Method method : getAdvisorMethods(aspectClass)) &#123; // 为每个方法分别调用 getAdvisor 方法 Advisor advisor = getAdvisor(method, lazySingletonAspectInstanceFactory, advisors.size(), aspectName); if (advisor != null) &#123; advisors.add(advisor); &#125; &#125; // If it's a per target aspect, emit the dummy instantiating aspect. if (!advisors.isEmpty() &amp;&amp; lazySingletonAspectInstanceFactory.getAspectMetadata().isLazilyInstantiated()) &#123; Advisor instantiationAdvisor = new SyntheticInstantiationAdvisor(lazySingletonAspectInstanceFactory); advisors.add(0, instantiationAdvisor); &#125; // Find introduction fields. for (Field field : aspectClass.getDeclaredFields()) &#123; Advisor advisor = getDeclareParentsAdvisor(field); if (advisor != null) &#123; advisors.add(advisor); &#125; &#125; return advisors;&#125;public Advisor getAdvisor(Method candidateAdviceMethod, MetadataAwareAspectInstanceFactory aspectInstanceFactory, int declarationOrderInAspect, String aspectName) &#123; validate(aspectInstanceFactory.getAspectMetadata().getAspectClass()); // 获取切点实现类 AspectJExpressionPointcut expressionPointcut = getPointcut( candidateAdviceMethod, aspectInstanceFactory.getAspectMetadata().getAspectClass()); if (expressionPointcut == null) &#123; return null; &#125; // 创建 Advisor 实现类 return new InstantiationModelAwarePointcutAdvisorImpl(expressionPointcut, candidateAdviceMethod, this, aspectInstanceFactory, declarationOrderInAspect, aspectName);&#125; 如上，getAdvisor 方法包含两个主要步骤，一个是获取 AspectJ 表达式切点，另一个是创建 Advisor 实现类。在第二个步骤中，包含一个隐藏步骤 – 创建 Advice。下面我将按顺序依次分析这两个步骤，先看获取 AspectJ 表达式切点的过程，如下：123456789101112131415161718192021222324252627282930private AspectJExpressionPointcut getPointcut(Method candidateAdviceMethod, Class&lt;?&gt; candidateAspectClass) &#123; // 获取方法上的 AspectJ 相关注解，包括 @Before，@After 等 AspectJAnnotation&lt;?&gt; aspectJAnnotation = AbstractAspectJAdvisorFactory.findAspectJAnnotationOnMethod(candidateAdviceMethod); if (aspectJAnnotation == null) &#123; return null; &#125; // 创建一个 AspectJExpressionPointcut 对象 AspectJExpressionPointcut ajexp = new AspectJExpressionPointcut(candidateAspectClass, new String[0], new Class&lt;?&gt;[0]); // 找到其中的切点表达式，设置到 AspectJExpressionPointcut 对象 ajexp.setExpression(aspectJAnnotation.getPointcutExpression()); ajexp.setBeanFactory(this.beanFactory); return ajexp;&#125;protected static AspectJAnnotation&lt;?&gt; findAspectJAnnotationOnMethod(Method method) &#123; // classesToLookFor 中的元素是大家熟悉的 Class&lt;?&gt;[] classesToLookFor = new Class&lt;?&gt;[] &#123; Before.class, Around.class, After.class, AfterReturning.class, AfterThrowing.class, Pointcut.class&#125;; for (Class&lt;?&gt; c : classesToLookFor) &#123; // 查找注解 AspectJAnnotation&lt;?&gt; foundAnnotation = findAnnotation(method, (Class&lt;Annotation&gt;) c); if (foundAnnotation != null) &#123; return foundAnnotation; &#125; &#125; return null;&#125; 获取切点的过程并不复杂，不过需要注意的是，目前获取到的切点可能还只是个半成品，需要再次处理一下才行。比如下面的代码：12345678910111213@Component@Aspectpublic class LogAspect &#123; @Pointcut(\"execution(* com.huzb.demo.CarImpl.run(..))\") public void pointCut() &#123; &#125; @Before(\"pointCut()\") public void logStart(JoinPoint joinPoint) &#123; Object[] args = joinPoint.getArgs(); System.out.println(\"方法调用之前\"); &#125;&#125; @Before 注解中的表达式是pointcut()，也就是说 ajexp 设置的表达式只是一个中间值，不是最终值。所以后续还需要将 ajexp 中的表达式进行转换，关于这个转换的过程非常复杂，也不是重点，这里就不展开了。说完切点的获取过程，下面再来看看 Advisor 实现类的创建过程。如下：123456789101112131415161718192021222324252627282930public InstantiationModelAwarePointcutAdvisorImpl(AspectJExpressionPointcut declaredPointcut, Method aspectJAdviceMethod, AspectJAdvisorFactory aspectJAdvisorFactory, MetadataAwareAspectInstanceFactory aspectInstanceFactory, int declarationOrder, String aspectName) &#123; this.declaredPointcut = declaredPointcut; this.declaringClass = aspectJAdviceMethod.getDeclaringClass(); this.methodName = aspectJAdviceMethod.getName(); this.parameterTypes = aspectJAdviceMethod.getParameterTypes(); this.aspectJAdviceMethod = aspectJAdviceMethod; this.aspectJAdvisorFactory = aspectJAdvisorFactory; this.aspectInstanceFactory = aspectInstanceFactory; this.declarationOrder = declarationOrder; this.aspectName = aspectName; if (aspectInstanceFactory.getAspectMetadata().isLazilyInstantiated()) &#123; Pointcut preInstantiationPointcut = Pointcuts.union( aspectInstanceFactory.getAspectMetadata().getPerClausePointcut(), this.declaredPointcut); this.pointcut = new PerTargetInstantiationModelPointcut( this.declaredPointcut, preInstantiationPointcut, aspectInstanceFactory); this.lazy = true; &#125; else &#123; this.pointcut = this.declaredPointcut; this.lazy = false; // 按照注解解析 Advice this.instantiatedAdvice = instantiateAdvice(this.declaredPointcut); &#125;&#125; 上面是 InstantiationModelAwarePointcutAdvisorImpl 的构造方法，不过我们无需太关心这个方法中的一些初始化逻辑。我们把目光移到构造方法的最后一行代码中，即 instantiateAdvice(this.declaredPointcut)，这个方法用于创建通知 Advice。我们之前提到过，通知器 Advisor 是通知 Advice 的持有者，所以在 Advisor 实现类的构造方法中创建通知也是合适的。那下面我们就来看看构建通知的过程是怎样的，如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687private Advice instantiateAdvice(AspectJExpressionPointcut pcut) &#123; return this.aspectJAdvisorFactory.getAdvice(this.aspectJAdviceMethod, pcut, this.aspectInstanceFactory, this.declarationOrder, this.aspectName);&#125;public Advice getAdvice(Method candidateAdviceMethod, AspectJExpressionPointcut expressionPointcut, MetadataAwareAspectInstanceFactory aspectInstanceFactory, int declarationOrder, String aspectName) &#123; Class&lt;?&gt; candidateAspectClass = aspectInstanceFactory.getAspectMetadata().getAspectClass(); validate(candidateAspectClass); // 获取 Advice 注解 AspectJAnnotation&lt;?&gt; aspectJAnnotation = AbstractAspectJAdvisorFactory.findAspectJAnnotationOnMethod(candidateAdviceMethod); if (aspectJAnnotation == null) &#123; return null; &#125; if (!isAspect(candidateAspectClass)) &#123; throw new AopConfigException(\"Advice must be declared inside an aspect type: Offending method '\" + candidateAdviceMethod + \"' in class [\" + candidateAspectClass.getName() + \"]\"); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(\"Found AspectJ method: \" + candidateAdviceMethod); &#125; AbstractAspectJAdvice springAdvice; // 按照注解类型生成相应的 Advice 实现类 switch (aspectJAnnotation.getAnnotationType()) &#123; case AtBefore: // @Before -&gt; AspectJMethodBeforeAdvice springAdvice = new AspectJMethodBeforeAdvice(candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); break; case AtAfter: // @After -&gt; AspectJAfterAdvice springAdvice = new AspectJAfterAdvice(candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); break; case AtAfterReturning: // @AfterReturning -&gt; AspectJAfterAdvice springAdvice = new AspectJAfterReturningAdvice(candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); AfterReturning afterReturningAnnotation = (AfterReturning) aspectJAnnotation.getAnnotation(); if (StringUtils.hasText(afterReturningAnnotation.returning())) &#123; springAdvice.setReturningName(afterReturningAnnotation.returning()); &#125; break; case AtAfterThrowing: // @AfterThrowing -&gt; AspectJAfterThrowingAdvice springAdvice = new AspectJAfterThrowingAdvice(candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); AfterThrowing afterThrowingAnnotation = (AfterThrowing) aspectJAnnotation.getAnnotation(); if (StringUtils.hasText(afterThrowingAnnotation.throwing())) &#123; springAdvice.setThrowingName(afterThrowingAnnotation.throwing()); &#125; break; case AtAround: // @Around -&gt; AspectJAroundAdvice springAdvice = new AspectJAroundAdvice(candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); break; /* * 如果注解类型为 AtPointcut 的情况，什么都不做，直接返回 null。 * 从整个方法的调用栈来看，并不会出现注解类型为 AtPointcut 的情况 */ case AtPointcut: if (logger.isDebugEnabled()) &#123; logger.debug(\"Processing pointcut '\" + candidateAdviceMethod.getName() + \"'\"); &#125; return null; default: throw new UnsupportedOperationException( \"Unsupported advice type on method: \" + candidateAdviceMethod); &#125; springAdvice.setAspectName(aspectName); springAdvice.setDeclarationOrder(declarationOrder); /* * 获取方法的参数列表名称，比如方法 int sum(int numX, int numY), * getParameterNames(sum) 得到 argNames = [\"numX\", \"numY\"] */ String[] argNames = this.parameterNameDiscoverer.getParameterNames(candidateAdviceMethod); if (argNames != null) &#123; // 设置参数名 springAdvice.setArgumentNamesFromStringArray(argNames); &#125; springAdvice.calculateArgumentBindings(); return springAdvice;&#125; 上面的代码逻辑不是很复杂，主要的逻辑就是根据注解类型生成与之对应的通知对象。下面来总结一下获取通知器（getAdvisors）整个过程的逻辑，如下： 从目标 bean 中获取不包含 Pointcut 注解的方法列表 遍历上一步获取的方法列表，并调用 getAdvisor 获取当前方法对应的 Advisor 创建 AspectJExpressionPointcut 对象，并从方法中的注解中获取表达式，设置到切点对象中 创建 Advisor 实现类对象 InstantiationModelAwarePointcutAdvisorImpl 调用 instantiateAdvice 方法构建通知 调用 getAdvice 方法，并根据注解类型创建相应的通知 如上所示，上面的步骤做了一定的简化。总的来说，获取通知器的过程还是比较复杂的，并不是很容易看懂。现在，大家知道了通知是怎么创建的。那我们难道不要去看看这些通知的实现源码吗？显然，我们应该看一下。那接下里，我们一起来分析一下 AspectJMethodBeforeAdvice，也就是 @Before 注解对应的通知实现类。看看它的逻辑是什么样的。 2.1.3 AspectJMethodBeforeAdvice 分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class AspectJMethodBeforeAdvice extends AbstractAspectJAdvice implements MethodBeforeAdvice &#123; public AspectJMethodBeforeAdvice( Method aspectJBeforeAdviceMethod, AspectJExpressionPointcut pointcut, AspectInstanceFactory aif) &#123; super(aspectJBeforeAdviceMethod, pointcut, aif); &#125; @Override public void before(Method method, Object[] args, Object target) throws Throwable &#123; // 调用通知方法 invokeAdviceMethod(getJoinPointMatch(), null, null); &#125; @Override public boolean isBeforeAdvice() &#123; return true; &#125; @Override public boolean isAfterAdvice() &#123; return false; &#125;&#125;protected Object invokeAdviceMethod(JoinPointMatch jpMatch, Object returnValue, Throwable ex) throws Throwable &#123; // 调用通知方法，并向其传递参数 return invokeAdviceMethodWithGivenArgs(argBinding(getJoinPoint(), jpMatch, returnValue, ex));&#125;protected Object invokeAdviceMethodWithGivenArgs(Object[] args) throws Throwable &#123; Object[] actualArgs = args; if (this.aspectJAdviceMethod.getParameterTypes().length == 0) &#123; actualArgs = null; &#125; try &#123; ReflectionUtils.makeAccessible(this.aspectJAdviceMethod); // 通过反射调用通知方法 return this.aspectJAdviceMethod.invoke(this.aspectInstanceFactory.getAspectInstance(), actualArgs); &#125; catch (IllegalArgumentException ex) &#123; throw new AopInvocationException(\"Mismatch on arguments to advice method [\" + this.aspectJAdviceMethod + \"]; pointcut expression [\" + this.pointcut.getPointcutExpression() + \"]\", ex); &#125; catch (InvocationTargetException ex) &#123; throw ex.getTargetException(); &#125;&#125; 如上，AspectJMethodBeforeAdvice 的源码比较简单，这里我们仅关注 before 方法。这个方法调用了父类中的 invokeAdviceMethod，然后 invokeAdviceMethod 在调用 invokeAdviceMethodWithGivenArgs，最后在 invokeAdviceMethodWithGivenArgs 通过反射执行通知方法。是不是很简单？ 关于 AspectJMethodBeforeAdvice 就简单介绍到这里吧，至于剩下的几种实现，大家可以自己去看看。好了，关于 AspectJMethodBeforeAdvice 的源码分析，就分析到这里了。我们继续往下看吧。 2.2 筛选通知器查找出所有的通知器，整个流程还没算完，接下来我们还要对这些通知器进行筛选。筛选出适合应用在当前 bean 上的通知器。那下面我们来分析一下通知器筛选的过程，如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596protected List&lt;Advisor&gt; findAdvisorsThatCanApply( List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; beanClass, String beanName) &#123; ProxyCreationContext.setCurrentProxiedBeanName(beanName); try &#123; // 调用重载方法 return AopUtils.findAdvisorsThatCanApply(candidateAdvisors, beanClass); &#125; finally &#123; ProxyCreationContext.setCurrentProxiedBeanName(null); &#125;&#125;public static List&lt;Advisor&gt; findAdvisorsThatCanApply(List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; clazz) &#123; if (candidateAdvisors.isEmpty()) &#123; return candidateAdvisors; &#125; List&lt;Advisor&gt; eligibleAdvisors = new LinkedList&lt;Advisor&gt;(); for (Advisor candidate : candidateAdvisors) &#123; // 筛选 IntroductionAdvisor 类型的通知器 if (candidate instanceof IntroductionAdvisor &amp;&amp; canApply(candidate, clazz)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; boolean hasIntroductions = !eligibleAdvisors.isEmpty(); for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor) &#123; continue; &#125; // 筛选普通类型的通知器 if (canApply(candidate, clazz, hasIntroductions)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; return eligibleAdvisors;&#125;public static boolean canApply(Advisor advisor, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; if (advisor instanceof IntroductionAdvisor) &#123; /* * 从通知器中获取类型过滤器 ClassFilter，并调用 matchers 方法进行匹配。 * ClassFilter 接口的实现类 AspectJExpressionPointcut 为例，该类的 * 匹配工作由 AspectJ 表达式解析器负责，具体匹配细节这个就没法分析了，我 * AspectJ 表达式的工作流程不是很熟 */ return ((IntroductionAdvisor) advisor).getClassFilter().matches(targetClass); &#125; else if (advisor instanceof PointcutAdvisor) &#123; PointcutAdvisor pca = (PointcutAdvisor) advisor; // 对于普通类型的通知器，这里继续调用重载方法进行筛选 return canApply(pca.getPointcut(), targetClass, hasIntroductions); &#125; else &#123; return true; &#125;&#125;public static boolean canApply(Pointcut pc, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; Assert.notNull(pc, \"Pointcut must not be null\"); // 使用 ClassFilter 匹配 class if (!pc.getClassFilter().matches(targetClass)) &#123; return false; &#125; MethodMatcher methodMatcher = pc.getMethodMatcher(); if (methodMatcher == MethodMatcher.TRUE) &#123; return true; &#125; IntroductionAwareMethodMatcher introductionAwareMethodMatcher = null; if (methodMatcher instanceof IntroductionAwareMethodMatcher) &#123; introductionAwareMethodMatcher = (IntroductionAwareMethodMatcher) methodMatcher; &#125; /* * 查找当前类及其父类（以及父类的父类等等）所实现的接口，由于接口中的方法是 public， * 所以当前类可以继承其父类，和父类的父类中所有的接口方法 */ Set&lt;Class&lt;?&gt;&gt; classes = new LinkedHashSet&lt;Class&lt;?&gt;&gt;(ClassUtils.getAllInterfacesForClassAsSet(targetClass)); classes.add(targetClass); for (Class&lt;?&gt; clazz : classes) &#123; // 获取当前类的方法列表，包括从父类中继承的方法 Method[] methods = ReflectionUtils.getAllDeclaredMethods(clazz); for (Method method : methods) &#123; // 使用 methodMatcher 匹配方法，匹配成功即可立即返回 if ((introductionAwareMethodMatcher != null &amp;&amp; introductionAwareMethodMatcher.matches(method, targetClass, hasIntroductions)) || methodMatcher.matches(method, targetClass)) &#123; return true; &#125; &#125; &#125; return false;&#125; 以上是通知器筛选的过程，筛选的工作主要由 ClassFilter 和 MethodMatcher 完成。ClassFilter 和 MethodMatcher 两个接口，都有一个 matches 的抽象方法。以 AspectJExpressionPointcut 类型的切点为例。该类型切点实现了ClassFilter 和 MethodMatcher 接口，匹配的工作则是由 AspectJ 表达式解析器负责。除了使用 AspectJ 表达式进行匹配，Spring 还提供了基于正则表达式的切点类，以及更简单的根据方法名进行匹配的切点类。这块内容很多，这里就不展开了。 在完成通知器的查找和筛选过程后，还需要进行最后一步处理 – 对通知器列表进行拓展。怎么拓展呢？我们一起到下一节中一探究竟吧。 2.3 拓展筛选出通知器列表拓展方法 extendAdvisors 做的事情并不多，逻辑也比较简单。我们一起来看一下，如下：123456789101112131415161718192021222324252627282930313233343536protected void extendAdvisors(List&lt;Advisor&gt; candidateAdvisors) &#123; AspectJProxyUtils.makeAdvisorChainAspectJCapableIfNecessary(candidateAdvisors);&#125;public static boolean makeAdvisorChainAspectJCapableIfNecessary(List&lt;Advisor&gt; advisors) &#123; // 如果通知器列表是一个空列表，则啥都不做 if (!advisors.isEmpty()) &#123; boolean foundAspectJAdvice = false; /* * 下面的 for 循环用于检测 advisors 列表中是否存在 * AspectJ 类型的 Advisor 或 Advice */ for (Advisor advisor : advisors) &#123; if (isAspectJAdvice(advisor)) &#123; foundAspectJAdvice = true; &#125; &#125; /* * 向 advisors 列表的首部添加 DefaultPointcutAdvisor， * 至于为什么这样做，我会在后续的文章中进行说明 */ if (foundAspectJAdvice &amp;&amp; !advisors.contains(ExposeInvocationInterceptor.ADVISOR)) &#123; advisors.add(0, ExposeInvocationInterceptor.ADVISOR); return true; &#125; &#125; return false;&#125;private static boolean isAspectJAdvice(Advisor advisor) &#123; return (advisor instanceof InstantiationModelAwarePointcutAdvisor || advisor.getAdvice() instanceof AbstractAspectJAdvice || (advisor instanceof PointcutAdvisor &amp;&amp; ((PointcutAdvisor) advisor).getPointcut() instanceof AspectJExpressionPointcut));&#125; 如上，上面的代码比较少，也不复杂。由源码可以看出 extendAdvisors 是一个空壳方法，除了调用makeAdvisorChainAspectJCapableIfNecessary，该方法没有其他更多的逻辑了。至于 makeAdvisorChainAspectJCapableIfNecessary 这个方法，该方法主要的目的是向通知器列表首部添加 DefaultPointcutAdvisor 类型的通知器，也就是 ExposeInvocationInterceptor.ADVISOR。这种通知器用到的地方不多，了解一下就好。 3、总结本篇文章我们从 AOP 的入口代码出发，看到 AOP 主要分两步：筛选合适的通知器和生成代理对象。筛选合适的通知器是个复杂的工作，因为要考虑两种情况：xml 和注解。对于 xml 形式的通知器，会声明为 Advisor 类型，所以我们的工作是找出所有 Advisor 类型的 beanName 并通过 getBean 的方式获取或创建它；对于注解形式的通知器，我们要找到所有标记了 @Aspect 的 bean，然后把它的每一个标记了 @Before、@After 等表示通知的方法封装成一个通知器。封装的过程分为两步：解析注解里的切点表达式，将其封装成一个 AspectJExpressionPointcut 对象和按照注解类型生成相应的 Advice 实现类。最后封装好的 Advisor 和原有的 Advisor 类型对象会被一起返回。接下来就是筛选工作，筛选是为了找出能和当前 bean 匹配上的 Advisor，这个工作会由 Advisor 中的 Pointcut 对象完成。以 AspectJExpressionPointcut 为例，它实现了 ClassFilter 和 MethodMatcher 两个接口的 matches 方法，内部有一个 AspectJ 表达式解析器，可以判断当前 bean 和 Advisor 是否匹配。然后我们就得到了一组匹配当前 bean 的 Advisor。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring AOP 源码分析——基本概念介绍","slug":"Spring-AOP源码浅析——基本概念介绍","date":"2019-03-14T12:41:41.000Z","updated":"2019-03-22T13:26:42.490Z","comments":true,"path":"2019/03/14/Spring-AOP源码浅析——基本概念介绍/","link":"","permalink":"http://yoursite.com/2019/03/14/Spring-AOP源码浅析——基本概念介绍/","excerpt":"","text":"AOP 全称是 Aspect Oriented Programming，即面向切面的编程，AOP 是一种开发理念。通过 AOP，我们可以把一些非业务逻辑的代码，比如安全检查，监控等代码从业务方法中抽取出来，以非侵入的方式与原方法进行协同。这样可以使原方法更专注于业务逻辑，代码结构会更加清晰，便于维护。Spring AOP 的原理很简单，就是动态代理，它和 AspectJ 不一样，AspectJ 是直接修改掉你的字节码。但 Spring 中仍然沿用了 AspectJ 的概念，它分为五个部分：连接点、切点、通知、切面和织入。我们通过一次实际 AOP 的使用来说明这五个部分。 1、连接点 - Joinpoint连接点是指程序执行过程中的一些点，比如方法调用，异常处理等。在 Spring AOP 中，仅支持方法级别的连接点。上面是官方的概念，下面举个实际的例子。现在我们有一个 Car 接口，该接口定义如下：1234public interface Car &#123; public void run(); public void stop();&#125; 它有一个实现类，这个实现类的定义如下：123456789101112@Component(\"car\")public class CarImpl implements Car&#123; @Override public void run()&#123; System.out.println(\"car run...\"); &#125; @Override public void stop() &#123; System.out.println(\"car stop\"); &#125;&#125; 现在我们在别的地方调用这个对象：1234567891011@SpringBootApplicationpublic class DemoApplication &#123; public static void main(String[] args) &#123; ApplicationContext applicationContext = SpringApplication.run(DemoApplication.class, args); Car car = (Car)applicationContext.getBean(\"car\"); car.run(); // 连接点1 car.stop(); // 连接点2 &#125;&#125; 如上所示，每次方法调用都是一个连接点。Spring 中把连接点抽象成了一个接口，可以获取当前调用方法的各种信息：123456789101112public interface JoinPoint &#123; String toString(); // 连接点所在位置的相关信息 String toShortString(); // 连接点所在位置的简短相关信息 String toLongString(); // 连接点所在位置的全部相关信息 Object getThis(); // 返回AOP代理对象 Object getTarget(); // 返回目标对象 Object[] getArgs(); // 返回被通知方法参数列表 Signature getSignature(); // 返回当前连接点签名 SourceLocation getSourceLocation();// 返回连接点方法所在类文件中的位置 String getKind(); // 连接点类型 StaticPart getStaticPart(); // 返回连接点静态部分 &#125; 2、切点 - Pointcut切点的作用是选出合适的连接点。我们可以定义一个切点：12@Pointcut(\"execution(* com.huzb.demo.CarImpl.run(..))\")public void pointCut()&#123;&#125; 切点是个空方法，使用@Pointcut 注解表示这是一个切点，注解的 value 是个 execution 表达式，有专门的语法，用于匹配连接点。 3、通知 - Advice通知 Advice 即我们定义的横切逻辑，比如我们可以定义一个用于监控方法性能的通知，也可以定义一个安全检查的通知等。如果说切点解决了通知在哪里调用的问题，那么现在还需要考虑了一个问题，即通知在何时被调用？是在目标方法前被调用，还是在目标方法返回后被调用，还在两者兼备呢？Spring 帮我们解答了这个问题，Spring 中定义了以下几种通知类型： 前置通知（@Before）- 在目标方便调用前执行通知 后置通知（@After）- 在目标方法完成后执行通知 返回通知（@AfterReturning）- 在目标方法执行成功后，调用通知 异常通知（@AfterThrowing）- 在目标方法抛出异常后，执行通知 环绕通知（@Around）- 在目标方法调用前后均可执行自定义逻辑 我们来尝试自定义一个通知：12345@Before(\"com.huzb.demo.LogAspect.pointCut()\")public void logStart(JoinPoint joinPoint) &#123; Object[] args = joinPoint.getArgs(); System.out.println(\"方法调用之前\");&#125; @Before 注释表明了这是一个前置通知，注释中的 value 指定了切点，所以这个通知只会在切点指定的方法调用前执行。参数表的第一个是 JoinPoint 对象，也就是连接点对象，从中我们可以获取到调用方法的参数列表、所属实例等信息。参数列表中也可以没有 JoinPoint 对象，但如果要获取 JoinPoint，就一定要把它放在第一个，不然 Spring 不会自动注入。另外还有其它类型的通知，有些使用方法和前置通知略有不同，如下所示：1234567891011121314151617181920212223242526272829@After(\"com.huzb.demo.LogAspect.pointCut()\")public void logEnd(JoinPoint joinPoint) &#123; System.out.println(\"方法调用之后\");&#125;// 把返回对象注入到参数列表中的 result@AfterReturning(value = \"com.huzb.demo.LogAspect.pointCut()\", returning = \"result\")public void logReturn(JoinPoint joinPoint, Object result) &#123; System.out.println(\"方法返回的结果为：\" + result);&#125;// 把抛出的异常注入到参数列表中的 exception@AfterThrowing(value = \"com.huzb.demo.LogAspect.pointCut()\", throwing = \"exception\")public void logException(JoinPoint joinPoint, Exception exception) &#123; System.out.println(\"方法抛出的异常为：\" + exception.getMessage());&#125;@Around(\"com.huzb.demo.LogAspect.pointCut()\")public void logAround(ProceedingJoinPoint joinPoint) &#123; Object result; System.out.println(\"方法调用之前\"); try &#123; result = joinPoint.proceed(); // 手动执行方法 System.out.println(\"方法返回的结果为：\" + result); &#125; catch (Throwable throwable) &#123; System.out.println(\"方法抛出的异常为：\" + throwable.getMessage()); &#125; System.out.println(\"方法调用之后\");&#125; 4、切面 - Aspect有了切点和通知，我们需要把它们整合起来，这个整合的工具就是切面。我们可以为刚才创建的切点和通知定义一个切面：123456789101112131415161718192021222324252627282930@Component@Aspectpublic class LogAspect &#123; @Pointcut(\"execution(* com.huzb.demo.CarImpl.run(..))\") public void pointCut() &#123; &#125; @Before(\"pointCut()\") public void logStart(JoinPoint joinPoint) &#123; Object[] args = joinPoint.getArgs(); System.out.println(\"方法调用之前\"); &#125; @After(\"pointCut()\") public void logEnd(JoinPoint joinPoint) &#123; System.out.println(\"方法调用之后\"); &#125; // 把返回对象注入到参数列表中的 result @AfterReturning(value = \"pointCut()\", returning = \"result\") public void logReturn(JoinPoint joinPoint, Object result) &#123; System.out.println(\"方法返回的结果为：\" + result); &#125; // 把抛出的异常注入到参数列表中的 exception @AfterThrowing(value = \"pointCut()\", throwing = \"exception\") public void logException(JoinPoint joinPoint, Exception exception) &#123; System.out.println(\"方法抛出的异常为：\" + exception.getMessage()); &#125;&#125; LogAspect 类上的注释@Aspect 表明这是一个切面类，切面类一定要声明为一个 bean 加入 IOC 容器中，否则切面类不会生效。 5、织入 - Weaving现在我们有了连接点、切点、通知，以及切面等，可谓万事俱备，但是还差了一股东风。这股东风是什么呢？没错，就是织入。所谓织入就是在切点的引导下，将通知逻辑插入到方法调用上，使得我们的通知逻辑在方法调用时得以执行。说完织入的概念，现在来说说 Spring 是通过何种方式将通知织入到目标方法上的。先来说说以何种方式进行织入，这个方式就是通过实现后置处理器 BeanPostProcessor 接口。该接口是 Spring 提供的一个拓展接口，通过实现该接口，用户可在 bean 初始化前后做一些自定义操作。那 Spring 是在何时进行织入操作的呢？答案是在 bean 初始化完成后，即 bean 执行完初始化方法（init-method）。Spring 通过切点对 bean 类中的方法进行匹配。若匹配成功，则会为该 bean 生成代理对象，并将代理对象返回给容器。容器向后置处理器输入 bean 对象，得到 bean 对象的代理，这样就完成了织入过程。 运行结果调用 car.run() 方法，我们会得到以下结果： 说明我们的通知已经织入成功了。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring 源码浅析——事件和异步事件","slug":"Spring源码浅析——事件和异步事件","date":"2019-03-13T13:47:24.000Z","updated":"2019-03-22T15:16:28.281Z","comments":true,"path":"2019/03/13/Spring源码浅析——事件和异步事件/","link":"","permalink":"http://yoursite.com/2019/03/13/Spring源码浅析——事件和异步事件/","excerpt":"","text":"1、背景知识1.1 观察者模式Spring事件体系是观察者模式的典型应用。观察者模式简单来说即所有观察者继承一个包含触发方法的父类并重写该方法，然后注册到被观察者的一个列表中。当被观察者发生变化时通过调用列表中所有已注册观察者的触发方法，使观察者得到通知，从而作进一步处理。关于观察者模式的具体描述网上有很多，这里不再赘述。 1.2 Spring 事件体系Spring事件体系包含三个部件： 事件：ApplicationEvent 事件监听器：ApplicationListener，对监听到的事件进行处理。 事件广播器：ApplicationEventMulticaster，将 publish 的事件广播给所有的监听器。通常情况下无需自行实现，Spring 默认提供了 SimpleApplicationEventMulticaster。 1.3 Spring 事件实例这里我们模拟一个交通信号灯状态改变的事件，而相应的监听者应当根据交通信号灯的状态做出不同的反应：红灯停，黄灯等，绿灯行。首先定义一个 LightEvent 继承 ApplicationEvent 来描述交通信号灯状态改变的事件。其中 lightColor 描述信号灯状态，1：红灯 2：黄灯 3：绿灯。123456789101112public class LightEvent extends ApplicationEvent &#123; private int lightColor; public LightEvent(int lightColor) &#123; this.lightColor = lightColor; &#125; public int getLightColor() &#123; return lightColor; &#125;&#125; 事件监听器实现 ApplicationListener 接口的 onApplicationEvent 方法，表明该监听器只关心 LightEvent 事件，不关心其他类型的事件。123456789101112131415161718@Componentpublic class LightEventListener implements ApplicationListener&lt;LightEvent&gt;&#123; @Override public void onApplicationEvent(LightEvent lightEvent) &#123; switch(lightEvent.getLightColor())&#123; case 1: System.out.println(\"红灯停\"); break; case 2: System.out.println(\"黄灯等\"); break; case 3: System.out.println(\"绿灯行\"); break; &#125; &#125;&#125; 最后是测试函数，调用了 ApplicationContext 的 publishEvent 方法：12345678910@SpringBootApplicationpublic class DemoApplication &#123; public static void main(String[] args) &#123; ApplicationContext applicationContext = SpringApplication.run(DemoApplication.class, args); LightEvent lightEvent = new LightEvent(1); applicationContext.publishEvent(lightEvent); &#125;&#125; 输出结果为：1红灯停 2、Spring 事件原理Spring 事件体系基于 ApplicationContext 实现，其构建过程主要包含在 ApplicationContext 接口的抽象实现类 AbstractApplicationContext 中。AbstractApplicationContext 中持有了 applicationEventMulticaster 成员变量，其初始化（initApplicationEventMulticaster）以及事件监听器的注册（registerListeners）过程可以在 refresh 函数中看到：123456789101112@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // ... // 初始化当前 ApplicationContext 的事件广播器 initApplicationEventMulticaster(); // ... // 注册事件监听器，监听器需要实现 ApplicationListener 接口 registerListeners(); // ... &#125;&#125; 2.1 初始化事件广播1234567891011121314151617181920212223//...public static final String APPLICATION_EVENT_MULTICASTER_BEAN_NAME = \"applicationEventMulticaster\";//... protected void initApplicationEventMulticaster() &#123; ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (beanFactory.containsLocalBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME)) &#123; this.applicationEventMulticaster = beanFactory.getBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, ApplicationEventMulticaster.class); if (logger.isDebugEnabled()) &#123; logger.debug(\"Using ApplicationEventMulticaster [\" + this.applicationEventMulticaster + \"]\"); &#125; &#125; else &#123; this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(beanFactory); beanFactory.registerSingleton(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, this.applicationEventMulticaster); if (logger.isDebugEnabled()) &#123; logger.debug(\"Unable to locate ApplicationEventMulticaster with name '\" + APPLICATION_EVENT_MULTICASTER_BEAN_NAME + \"': using default [\" + this.applicationEventMulticaster + \"]\"); &#125; &#125;&#125; 在初始化事件广播器的过程中，Spring 会首先检查是否存在用户自定义的名称为“applicationEventMulticaster”且实现了 ApplicationEventMulticaster 接口的 Bean。若存在则使用用户自定义的 ApplicationEventMulticaster 作为事件广播器，否则使用默认的 SimpleApplicationEventMulticaster。 2.2 注册事件监听器12345678protected void registerListeners() &#123; //... String[] listenerBeanNames = getBeanNamesForType(ApplicationListener.class, true, false); for (String listenerBeanName : listenerBeanNames) &#123; getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName); &#125; //...&#125; 最主要的步骤为通过反射获取所有实现了 ApplicationListener 接口的 Bean 的BeanName，并调用 addApplicationListenerBean 方法将其注册到 applicationEventMulticaster 中。addApplicationListenerBean 方法的实现在 AbstractApplicationEventMulticaster 中：123456public void addApplicationListenerBean(String listenerBeanName) &#123; synchronized (this.retrievalMutex) &#123; this.defaultRetriever.applicationListenerBeans.add(listenerBeanName); this.retrieverCache.clear(); &#125;&#125; 其中 ListenerRetriever 的数据结构如下，可以看到 applicationListenerBeans 中存放的是监听器的名字：12345private class ListenerRetriever &#123; public final Set&lt;ApplicationListener&lt;?&gt;&gt; applicationListeners; public final Set&lt;String&gt; applicationListenerBeans; //..&#125; 这些注册在 applicationListenerBeans 的监听器的名字会在广播器获取监听器的时候通过 getBean 的方式加入到 applicationListeners。 2.3 发布事件123456789101112131415161718192021public void publishEvent(Object event) &#123; publishEvent(event, null);&#125;protected void publishEvent(Object event, ResolvableType eventType) &#123; Assert.notNull(event, \"Event must not be null\"); if (logger.isTraceEnabled()) &#123; logger.trace(\"Publishing event in \" + getDisplayName() + \": \" + event); &#125; //... if (this.earlyApplicationEvents != null) &#123; this.earlyApplicationEvents.add(applicationEvent); &#125; else &#123; getApplicationEventMulticaster().multicastEvent(applicationEvent, eventType); &#125; //...&#125; publishEvent(Object event) 最终调用了 publishEvent(Object event, ResolvableType eventType)，其中最关键的部分即调用了 applicationEventMulticaster的multicastEvent 方法，我们可以看一下 SimpleApplicationEventMulticaster 中 multicastEvent 方法的实现：123456789101112131415161718192021222324252627282930313233public void multicastEvent(final ApplicationEvent event, ResolvableType eventType) &#123; ResolvableType type = (eventType != null ? eventType : resolveDefaultEventType(event)); for (final ApplicationListener&lt;?&gt; listener : getApplicationListeners(event, type)) &#123; Executor executor = getTaskExecutor(); if (executor != null) &#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; invokeListener(listener, event); &#125; &#125;); &#125; else &#123; invokeListener(listener, event); &#125; &#125;&#125;//...@SuppressWarnings(&#123;\"unchecked\", \"rawtypes\"&#125;)protected void invokeListener(ApplicationListener listener, ApplicationEvent event) &#123; ErrorHandler errorHandler = getErrorHandler(); if (errorHandler != null) &#123; try &#123; listener.onApplicationEvent(event); &#125; catch (Throwable err) &#123; errorHandler.handleError(err); &#125; &#125; else &#123; listener.onApplicationEvent(event); &#125;&#125; 该方法获取所有注册了给定事件的监听器，默认情况下 executor 为null，因此会直接调用 invokeListener 方法，在该方法中调用了 listener 的 onApplicationEvent 方法，实现对事件监听器的通知，完成事件发布。这样默认的调用方式是同步的，主流程需要等待所有 listener 的 onApplictionEvent 方法执行完毕才能继续执行。 2.4 异步事件通过源码我们可以知道事件广播器在广播前会检查是否传入了 executor，所以我们只需要在广播器中设置 Executor 属性就可以实现异步事件：12SimpleApplicationEventMulticaster applicationEventMulticaster = applicationContext.getBean(SimpleApplicationEventMulticaster.class);applicationEventMulticaster.setTaskExecutor(Executors.newCachedThreadPool()); 3、基于注解的事件与异步事件Spring 4.2 开始提供了@EventListener 注解，使得监听器不再需要实现 ApplicationListener 接口，只需要在监听方法上加上该注解即可，方法不一定叫 onApplicationEvent，但有且只能有一个参数，指定监听的事件类型。如：1234@EventListenerpublic void handler(LightEvent lightEvent) &#123; //...&#125; 对于异步事件，Spring 3.0 开始提供了@Async 注解，可以方便快速地实现异步事件：12345@EventListener@Asyncpublic void handler(LightEvent lightEvent) &#123; //..&#125; 3.1 @EventListener 原理对于@EventListener 注解，实现的逻辑在这里：123456789101112131415161718192021public void preInstantiateSingletons() throws BeansException &#123; // ... // 到这里说明所有的非懒加载的 singleton beans 已经完成了初始化 // 如果我们定义的 bean 是实现了 SmartInitializingSingleton 接口的，那么在这里回调它的 afterSingletonsInstantiated 方法 // 通过名字可以知道它表示单例对象初始化后需要做的操作 for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125;&#125; 这里有个实现了 SmartInitializingSingleton 接口的 EventListenerMethodProcessor 会被调用，它的 afterSingletonsInstantiated 方法如下：1234567891011121314151617public void afterSingletonsInstantiated() &#123; // 从 BeanFactory 中获取 EventListenerFactory 的 bean，默认是 DefaultEventListenerFactory List&lt;EventListenerFactory&gt; factories = getEventListenerFactories(); ConfigurableApplicationContext context = getApplicationContext(); String[] beanNames = context.getBeanNamesForType(Object.class); for (String beanName : beanNames) &#123; // ... try &#123; // 对所有 bean 进行处理 processBean(factories, beanName, type); &#125; catch (Throwable ex) &#123; throw new BeanInitializationException(\"Failed to process @EventListener \" + \"annotation on bean with name '\" + beanName + \"'\", ex); &#125; &#125;&#125; 两个逻辑： 从 BeanFactory 中获取 EventListenerFactory 的 bean，默认两种情况： DefaultEventListenerFactory — applicationContext 自己注入的 TransactionalEventListenerFactory – 使用配置进去的 调用 processBean 对所有的 bean 进行处理 processBean 的逻辑如下：12345678910111213141516171819202122232425262728protected void processBean(final List&lt;EventListenerFactory&gt; factories, final String beanName, final Class&lt;?&gt; targetType) &#123; // ... Map&lt;Method, EventListener&gt; annotatedMethods = null; // 查找当前 bean 标注了@EventListener 的方法 annotatedMethods = MethodIntrospector.selectMethods(targetType, (MethodIntrospector.MetadataLookup&lt;EventListener&gt;) method -&gt; AnnotatedElementUtils.findMergedAnnotation(method, EventListener.class)); &#125; // ... for (Method method : annotatedMethods.keySet()) &#123; for (EventListenerFactory factory : factories) &#123; if (factory.supportsMethod(method)) &#123; Method methodToUse = AopUtils.selectInvocableMethod(method, context.getType(beanName)); // 使用监听器工厂创建事件监听器 ApplicationListener&lt;?&gt; applicationListener = factory.createApplicationListener(beanName, targetType, methodToUse); if (applicationListener instanceof ApplicationListenerMethodAdapter) &#123; ((ApplicationListenerMethodAdapter) applicationListener).init(context, this.evaluator); &#125; // 注册事件到容器中 context.addApplicationListener(applicationListener); break; &#125; &#125; &#125; // ... &#125; &#125;&#125; 三个逻辑： 查找当前 bean 标注了@EventListener 的方法 使用监听器工厂创建事件监听器 注册事件到容器中 监听器工厂创建事件监听器的逻辑如下：123public ApplicationListener&lt;?&gt; createApplicationListener(String beanName, Class&lt;?&gt; type, Method method) &#123; return new ApplicationListenerMethodAdapter(beanName, type, method);&#125; 返回的是一个 ApplicationListenerMethodAdapter 对象，这个 ApplicationListenerMethodAdapter 对象内部持有了 Method 且实现了 ApplicationListener 接口，它的 onApplicationEvent 方法就是调用相应 Method：123456789101112131415161718192021222324@Overridepublic void onApplicationEvent(ApplicationEvent event) &#123; processEvent(event);&#125;public void processEvent(ApplicationEvent event) &#123; Object[] args = resolveArguments(event); if (shouldHandle(event, args)) &#123; Object result = doInvoke(args); if (result != null) &#123; handleResult(result); &#125; // ... &#125;&#125;protected Object doInvoke(Object... args) &#123; //... try&#123; // 调用 method 的方法 return this.method.invoke(bean, args); &#125; // ...&#125; 最终，我们注释了@EventListener 的方法会被封装成 ApplicationListenerMethodAdapter 对象，它也是一个 ApplicationListener 对象，跟通过实现 ApplicationListener 接口的监听器一样被注册进容器，被广播器调用。调用的时候执行的就是原方法。 4、总结Spring 中的事件机制是观察者模式的一个典型应用。Spring 会首先初始化事件广播器 ApplicationEventMulticaster，这个组件内部维护了一个事件监听器 ApplicationListener 的集合，事件监听器需要实现 onApplicationEvent 方法，这个方法有且只有一个参数就是 ApplicationEvent 的子类。当我们调用 applicationContext.publishEvent 发布事件的时候，本质上就是调用容器内部的事件广播器把它内部维护的监听器列表轮询一遍，依次调用各个监听器的 onApplicationEvent 方法。我们也可以给广播器传入一个线程池对象，这样广播器在调用各监听器的 onApplicationEvent 方法的时候会把它放在线程池里进行。注解@EventListener 可以指定一个方法为监听器方法，它的原理是在单例 bean 初始化之后，调用一个处理器来检查所有 bean 的方法是否包含@EventListener 注解（这里有一个容易搞错的地方就是处理器检查的时机是在单例 bean 初始化后没错，但检查的是所有的 bean，因为这是通过 beanName 来获取的，只要定义过的 bean 都会被检查到，不管是单例、多例还是懒加载），然后把这个方法包装成一个 ApplicationListener 注册到广播器中（这里又有一个容易搞错的地方就是这个 ApplicationListener 对象和持有方法的原对象是两个对象，原对象是单例还是多例、是创建出来了还是没创建出来，都不会影响生成 ApplicationListener，生成后就是单独的一个单例 bean）。@EventListener 定义的方法必须满足两点要求：一是只能有一个参数，二是参数必须继承 ApplicationEvent，否则会报错。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring 源码浅析——解决循环依赖","slug":"Spring源码浅析——解决循环依赖","date":"2019-03-11T04:40:01.000Z","updated":"2019-03-22T15:16:06.460Z","comments":true,"path":"2019/03/11/Spring源码浅析——解决循环依赖/","link":"","permalink":"http://yoursite.com/2019/03/11/Spring源码浅析——解决循环依赖/","excerpt":"","text":"本文来看一下 Spring 是如何解决循环依赖的。前几篇我们讲完了 IOC 容器创建的问题，但还有一些点没讲。比如在 doCreateBean 方法中的 earlySingletonExposure 是什么意思。这里我们要提到一个循环依赖的问题。 1、背景知识1、什么是循环依赖循环依赖其实就是循环引用，也就是两个或则两个以上的 bean 互相持有对方，最终形成闭环。比如 A 依赖于 B，B 又依赖于A。在 Spring 中这样的场景有很多，比如构造器参数中的循环依赖，属性注入时的循环依赖。而其中，只有单例对象的属性注入循环依赖是可以被解决的。 IOC 容器在实例化上左图两个 bean 时，会按照顺序，先去实例化 beanA，然后发现 beanA 依赖于 beanB，又去实例化 beanB；实例化 beanB 时，发现 beanB 又依赖于 beanA…如果容器不处理循环依赖的话，容器会无限执行上面的流程，直到内存溢出，程序崩溃。当然，Spring 是不会让这种情况发生的。在容器发现 beanB 依赖于 beanA 时，容器会获取 beanA 对象的一个早期的引用（early reference），并把这个早期引用注入到 beanB 中，让 beanB先完成实例化，beanA 就可以获取到 beanB 的引用，然后 beanA 完成实例化。 2、三级缓存Spring 为了解决循环依赖问题使用了三级缓存的方式，三级缓存的定义如下：12345678/** Cache of singleton objects: bean name --&gt; bean instance */private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;String, Object&gt;(256);/** Cache of early singleton objects: bean name --&gt; bean instance */private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;String, Object&gt;(16);/** Cache of singleton factories: bean name --&gt; ObjectFactory */private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;String, ObjectFactory&lt;?&gt;&gt;(16); 缓存 用途 singletonObjects 用于存放完全初始化好的 bean，从该缓存中取出的 bean 可以直接使用 earlySingletonObjects 存放早期 bean 对象（尚未填充属性），用于解决循环依赖 singletonFactories 存放 bean 工厂对象，用于解决循环依赖 3、回顾获取 bean 的过程这里我们来了解从 Spring IOC 容器中获取 bean 实例的流程（简化版），这对我们后续的源码分析会有比较大的帮助。先看图：这张图是一个简化后的流程图。开始流程图中只有一条执行路径，在条件 sharedInstance != null 这里出现了岔路，形成了绿色和红色两条路径。在上图中，读取/添加缓存的方法我用蓝色的框和☆标注了出来。 我来按照上面的图，分析一下整个流程的执行顺序。这个流程从 getBean 方法开始，getBean 是个空壳方法，所有逻辑都在 doGetBean 方法中。doGetBean 首先会调用 getSingleton(beanName) 方法获取 sharedInstance，sharedInstance 可能是完全实例化好的 bean，也可能是一个早期 bean 对象，当然也有可能是 null。如果不为 null，则走绿色的那条路径。再经 getObjectForBeanInstance 这一步处理后，绿色的这条执行路径就结束了。 我们再来看一下红色的那条执行路径，也就是 sharedInstance = null 的情况。在第一次获取某个 bean 的时候，缓存中是没有记录的，所以这个时候要走创建逻辑。上图中的 getSingleton(beanName,new ObjectFactory() {…}) 方法会创建一个 bean 实例，上图虚线路径指的是 getSingleton 方法内部调用的两个方法，其逻辑如下：123456public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; // 省略部分代码 singletonObject = singletonFactory.getObject(); // ... addSingleton(beanName, singletonObject);&#125; 如上所示，getSingleton 会在内部先调用 getObject 方法创建 singletonObject，然后再调用 addSingleton 将 singletonObject 放入缓存中。getObject 在内部代用了 createBean 方法，createBean 方法基本上也属于空壳方法，更多的逻辑是写在 doCreateBean 方法中的。doCreateBean 方法中的逻辑很多，其首先调用了 createBeanInstance 方法创建了一个原始的 bean 对象，随后调用 addSingletonFactory 方法向缓存中添加单例 bean 工厂，从该工厂可以获取早期 bean 对象。再之后，继续调用 populateBean 方法向原始 bean 对象中填充属性，并解析依赖。getObject 执行完成后，会返回完全实例化好的 bean。紧接着再调用 addSingleton 把完全实例化好的 bean 对象放入缓存中。到这里，红色执行路径差不多也就结束了。 这是对我们之前 bean 创建的回顾，下面来看一下源码。 2、源码分析源码部分我们从 beanA、beanB 循环依赖的情况出发，来看 beanA 和 beanB 的创建流程。首先是 beanA： 1、beanA：doGetBean12345678910protected &lt;T&gt; T doGetBean(final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; // ...... // 从缓存中获取 bean 实例 Object sharedInstance = getSingleton(beanName); // ......&#125; 在 doGetBean 方法中 Spring 会先调用 getSingleton 查看是否能直接从缓存中获取 beanA 实例。 2、beanA：getSingleton 1234567891011121314public Object getSingleton(String beanName) &#123; return getSingleton(beanName, true);&#125;protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; // 从 singletonObjects 获取实例，此时 beanA 还未创建，自然获取不到 Object singletonObject = this.singletonObjects.get(beanName); // 判断 beanName 对应的 bean 是否正在创建中，此时创建还未开始，所以不会进入 if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; // ...... &#125; // 返回的是 null return (singletonObject != NULL_OBJECT ? singletonObject : null);&#125; 第一次访问 getSingleton 返回的是个 null，然后回到 doGetBean 继续执行。 3、beanA：doGetBean12345678910111213141516171819202122232425262728293031323334protected &lt;T&gt; T doGetBean(final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; // ...... // 从缓存中获取的是 null Object sharedInstance = getSingleton(beanName); // 走 sharedInstance == null 的路线 if (sharedInstance != null &amp;&amp; args == null) &#123; // ...... &#125; else &#123; // ...... // mbd.isSingleton() 用于判断 bean 是否是单例模式 if (mbd.isSingleton()) &#123; // 调用 getSingleton 获取 bean 实例 sharedInstance = getSingleton(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; try &#123; // 内部调用的是 createBean 方法 return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; // ...... &#125; &#125; &#125;); // ...... &#125; // ...... &#125; // ......&#125; 我们走 sharedInstance == null 的路线，会调用 getSingleton 的重载方法，这次会传两个参数进去，一个 beanName 大家都知道，另一个 ObjectFactory 是一个匿名内部类，它持有外部方法的 beanName, mbd, args 变量（匿名内部类的性质），并把它们作为参数传递给 getObject 中调用的 createBean 方法。我们来看一下这个 getSingleton 的重载方法： 4、beanA：重载的 getSingleton12345678910111213141516public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; synchronized (this.singletonObjects) &#123; // ...... // 标记为正在创建 beforeSingletonCreation(beanName); // ...... // 调用 getObject 方法创建 bean 实例 singletonObject = singletonFactory.getObject(); // ...... &#125;&#125; getSingleton 的重载方法里面主要调用了传入的 ObjectFactory 的 getObject 方法。这个方法大家在之前看到过，这是匿名内部类实现的方法，里面返回的是 createBean 的结果，我们直接来看 createBean 方法： 5、beanA：createBean1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Overrideprotected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException &#123; // ...... // 空壳方法，主要逻辑在这里 Object beanInstance = doCreateBean(beanName, mbdToUse, args); // ......&#125;protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException &#123; // ...... // 实例化 beanA instanceWrapper = createBeanInstance(beanName, mbd, args); // ...... // 此处的 bean 可以认为是一个原始的 beanA 实例，暂未填充属性 final Object bean = instanceWrapper.getWrappedInstance(); // earlySingletonExposure 表示是否提前暴露，beanA 满足条件 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(\"Eagerly caching bean '\" + beanName + \"' to allow for resolving potential circular references\"); &#125; // 添加 beanA 的工厂对象到 singletonFactories 缓存中 addSingletonFactory(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; // 获取 beanA 的早期引用，如果 beanA 中的方法被 AOP 切点所匹配到，此时 AOP 相关逻辑会介入 return getEarlyBeanReference(beanName, mbd, bean); &#125; &#125;); &#125; // ...... // 属性装配，把属性填充进去，这一步 Spring 会发现 beanA 依赖 beanB，然后调用 getBean 获取 beanB populateBean(beanName, mbd, instanceWrapper); // ......&#125; createBean 是个空壳方法，里面调用了 doCreateBean 来实现主要逻辑。doCreateBean 中会实例化 beanA 对象，此时还没有填充属性。然后把工厂对象添加到 singletonFactories 缓存中。这个工厂对象是也是一个匿名内部类，里面持有了外部方法的 beanName, mbd, bean 三个变量，其中 bean 就是 beanA 的早期引用，是还没填充属性的 beanA 实例。ObjectFactory 实现了 getObject 方法，实际上就是调用 getEarlyBeanReference，它会获取 beanName, mbd, bean 三个参数，对于没有被 AOP 匹配到的对象 beanA，它会直接返回传入的参数 bean，也就是 beanA 的早期引用。 接下来 beanA 会填充属性，这一步里 Spring 会发现 beanA 依赖于 beanB，于是会调用 getBean 获取 beanA。 6、beanB：getBeanbeanB 的 getBean 流程跟目前为止的 beanA 流程一模一样，所以我们就快速前进了。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public Object getBean(String name) throws BeansException &#123; return doGetBean(name, null, null, false);&#125;protected &lt;T&gt; T doGetBean(final String name, @Nullable final Class&lt;T&gt; requiredType, @Nullable final Object[] args, boolean typeCheckOnly) throws BeansException &#123; // ...... sharedInstance = getSingleton(beanName, () -&gt; &#123; try &#123; // getSingleton 内部会调用这个 return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; // ...... &#125; &#125;); // ......&#125;protected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException &#123; // ...... // 空壳方法，主要逻辑在这里 Object beanInstance = doCreateBean(beanName, mbdToUse, args); // ......&#125;protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException &#123; // ...... // 实例化 beanB instanceWrapper = createBeanInstance(beanName, mbd, args); // ...... // 获取 beanB 的早期对象 final Object bean = instanceWrapper.getWrappedInstance(); // ...... // 添加 beanB 的工厂对象到 singletonFactories 缓存中 addSingletonFactory(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; // 获取 beanB 的早期引用，如果 beanB 中的方法被 AOP 切点所匹配到，此时 AOP 相关逻辑会介入 return getEarlyBeanReference(beanName, mbd, bean); &#125; &#125;); // ...... // 属性装配，把属性填充进去，这一步会发现 beanB 依赖 beanA，然后又调用 getBean 获取 beanA populateBean(beanName, mbd, instanceWrapper); // ......&#125; beanB 的创建过程和 beanA 到目前为止的一模一样，最终都是来到了 populateBean 填充属性。然后 Spring 在这里又会发现 beanB 依赖 beanA，然后又调用 getBean 获取 beanA。 7、beanA：getBean1234567891011121314public Object getBean(String name) throws BeansException &#123; return doGetBean(name, null, null, false);&#125;protected &lt;T&gt; T doGetBean(final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; // ...... // 从缓存中获取 bean 实例 Object sharedInstance = getSingleton(beanName); // ......&#125; Spring 会再次调用 getSingleton 方法获取 beanA 实例。 8、beanA：getSingleton12345678910111213141516171819202122232425262728293031public Object getSingleton(String beanName) &#123; return getSingleton(beanName, true);&#125;protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; // 从 singletonObjects 获取 beanA 的实例，因为还没完全创建成功，所以获取不到 Object singletonObject = this.singletonObjects.get(beanName); // 判断 beanA 是否正在创建中，在第 4 步已经把 beanA 标记为正在创建 if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; // 从 earlySingletonObjects 中获取提前曝光的 beanA，这里依旧没有 singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; // 从 singletonFactories 获取 beanA 的对象工厂，在第 5 步已经把 beanA 的对象工厂添加进去 ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; // 通过对象工厂获取 beanA 的早期引用 singletonObject = singletonFactory.getObject(); // 将 beanA 的早期引用放入缓存 earlySingletonObjects 中 this.earlySingletonObjects.put(beanName, singletonObject); // 将 beanA 的对象工厂从缓存 singletonFactories 中移除 this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; // 返回 beanA 的早期引用 return (singletonObject != NULL_OBJECT ? singletonObject : null);&#125; 这里就和第 2 步中第一次访问 getSingleton 不一样了，因为标记了 beanA 正在创建，所以这里 Spring 会依次查找三级缓存，最终在第三级缓存 singletonFactories 中找到了 beanA 对应的对象工厂，然后通过工厂对象获取到 beanA 的早期引用，并这个早期引用放在了第二级缓存 earlySingletonObjects 中。有了这个早期对象，beanB 的初始化过程就可以走完了。 9、beanB：doCreateBean12345678910111213141516protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException &#123; // ...... // 属性填充完成 populateBean(beanName, mbd, instanceWrapper); // 进行余下的初始化工作 exposedObject = initializeBean(beanName, exposedObject, mbd); // ...... // 返回完成初始化的 beanB return exposedObject;&#125; beanB 初始化完成。 10、beanB：重载的getSingleton1234567891011121314151617181920212223242526272829303132333435public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; synchronized (this.singletonObjects) &#123; // ...... // 得到初始化后的 beanB singletonObject = singletonFactory.getObject(); newSingleton = true; // 将 beanB 的正在创建状态移除 afterSingletonCreation(beanName); if (newSingleton) &#123; // 添加 beanB 到 singletonObjects 缓存中，并从其他集合中将 beanB 相关记录移除 addSingleton(beanName, singletonObject); &#125; // ...... // 返回 beanB return (singletonObject != NULL_OBJECT ? singletonObject : null); &#125;&#125;protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized (this.singletonObjects) &#123; // 将 beanB 存入 singletonObjects 中 this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT)); // 从其他缓存中移除 beanB 相关映射 this.singletonFactories.remove(beanName); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125;&#125; 在完成初始化后，Spring 会把 beanB 存入 singletonObjects 中，然后把 singletonFactories 和 earlySingletonObjects 中的相应记录删除。 11、beanA：doCreateBean同样地，BeanA 的初始化流程也可以走完了12345678910111213141516protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException &#123; // ...... // 属性填充完成 populateBean(beanName, mbd, instanceWrapper); // 进行余下的初始化工作 exposedObject = initializeBean(beanName, exposedObject, mbd); // ...... // 返回完成初始化的 beanA return exposedObject;&#125; beanA 初始化完成。 12、beanA：重载的getSingleton1234567891011121314151617181920212223public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; synchronized (this.singletonObjects) &#123; // ...... // 得到初始化后的 beanA singletonObject = singletonFactory.getObject(); newSingleton = true; // 将 beanA 的正在创建状态移除 afterSingletonCreation(beanName); if (newSingleton) &#123; // 添加 beanA 到 singletonObjects 缓存中，并从其他集合中将 beanA 相关记录移除 addSingleton(beanName, singletonObject); &#125; // ...... // 返回 beanA return (singletonObject != NULL_OBJECT ? singletonObject : null); &#125;&#125; 到这里，beanA 和 beanB 就都初始化完成了。 3、为什么是三级缓存看到这里，可能有同学心中有疑问，为什么要使用三级缓存呢？好像使用 earlySingletonObjects 和 singletonObjects 两级缓存，一个存放早期对象，一个存放初始化完成后的对象，也能实现同样的功能，singletonFactories 好像显得有些多此一举。其实不是的，对于普通对象，确实只要返回刚创建完的早期对象就好了，但对于内部有被 AOP 增强的方法的对象，需要返回的是代理对象。我们可以看一下 ObjectFactory 匿名内部类里面调用的 getEarlyBeanReference 方法：12345678910111213protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) &#123; Object exposedObject = bean; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; // SmartInstantiationAwareBeanPostProcessor 这个后置处理器会在返回早期对象时被调用，如果返回的对象需要加强，那这里就会生成代理对象 if (bp instanceof SmartInstantiationAwareBeanPostProcessor) &#123; SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); &#125; &#125; &#125; return exposedObject;&#125; 我们可以看到对于一般的对象，返回的就是传入的早期对象，但是对于内部有被 AOP 增强的方法的对象，会使用后置处理器返回一个代理对象。 4、总结本篇文章主要分析了 Spring 如何解决循环依赖的问题。是通过三级缓存的方式，在对象实例化之后但还没有填充属性之前，提前暴露出早期的对象，之后有对象需要的话，可以从缓存中获取到这个早期对象，避免了无止境的循环依赖。同时我们也分析了为什么要使用三级缓存而不是二级缓存，是因为对于内部有被 AOP 增强的方法的对象，需要返回的不是实例化后的对象，而是在此基础上的代理对象。这就需要有一级缓存来存这些代理对象。 5、参考Spring IOC 源码分析Spring-IOC-容器源码分析-循环依赖的解决办法","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring 源码浅析——bean 创建流程","slug":"Spring源码浅析——bean 创建流程","date":"2019-03-04T09:52:47.000Z","updated":"2019-03-28T06:39:31.914Z","comments":true,"path":"2019/03/04/Spring源码浅析——bean 创建流程/","link":"","permalink":"http://yoursite.com/2019/03/04/Spring源码浅析——bean 创建流程/","excerpt":"","text":"上一篇中概览了容器刷新的过程，其中着重跟进了容器准备的代码。那么现在容器准备好了，就要创建和往里面注入 bean 了。 bean 生命周期在跟进这部分代码之前，我们首先需要对 Spring 中 bean 的生命周期有个宏观的认识。下面是 bean 生命周期的大致流程： 接下来对照上图，一步一步对 singleton 类型 bean 的生命周期进行解析： 扫描得到 beanDefinition，里面包含了 bean 创建的全部信息。 调用 BeanFactoryPostProcessor 后置处理方法，可以最后对 beanDefinition 作一些修改。 实例化 bean 对象，类似于 new XXObject()。 将配置文件或者注解中设置的属性填充到刚刚创建的 bean 对象中。 检查 bean 对象是否实现了 Aware 一类的接口，如果实现了则把相应的依赖设置到 bean 对象中。比如如果 bean 实现了 BeanFactoryAware 接口，Spring 容器在实例化bean的过程中，会将 BeanFactory 容器注入到 bean 中。 调用 BeanPostProcessor 前置处理方法，即 postProcessBeforeInitialization(Object bean, String beanName)。 检查 bean 对象是否实现了 InitializingBean 接口，如果实现，则调用 afterPropertiesSet 方法。或者检查配置文件中是否配置了 init-method 属性，如果配置了，则去调用 init-method 属性配置的方法。 调用 BeanPostProcessor 后置处理方法，即 postProcessAfterInitialization(Object bean, String beanName)。我们所熟知的 AOP 就是在这里将 Advice 逻辑织入到 bean 中的。 注册 Destruction 相关回调方法。 bean 对象处于就绪状态，可以使用了。 应用上下文被销毁，调用注册的 Destruction 相关方法。如果 bean 实现了 DispostbleBean 接口，Spring 容器会调用 destroy 方法。如果在配置文件中配置了 destroy 属性，Spring 容器则会调用 destroy 属性对应的方法。 beanDefinition 介绍我们走完了容器准备流程，其实就已经走到了第三步。此时 beanDefinition 已经保存在我们的容器中了。beanDefinition 对象顾名思义保存的就是 bean 的定义信息。它用 Java 类的方式保存了我们在 xml 或者注解中对 bean 对象的配置。然后 Spring 就根据它里面保存的信息初始化 bean 对象。BeanDefinition 的接口定义如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement &#123; // 我们可以看到，默认只提供 sington 和 prototype 两种， // 大家可能知道还有 request, session, globalSession, application, websocket 这几种， // 不过，它们属于基于 web 的扩展。 String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; // 比较不重要，直接跳过吧 int ROLE_APPLICATION = 0; int ROLE_SUPPORT = 1; int ROLE_INFRASTRUCTURE = 2; // 设置父 bean，这里涉及到 bean 继承，不是 java 继承。一句话就是：继承父 bean 的配置信息 void setParentName(String parentName); String getParentName(); // 设置 bean 的类名称，将来是要通过反射来生成实例的 void setBeanClassName(String beanClassName); String getBeanClassName(); // 设置 bean 的 scope void setScope(String scope); String getScope(); // 设置是否懒加载 void setLazyInit(boolean lazyInit); boolean isLazyInit(); // 设置该 bean 依赖的所有的 bean，注意，这里的依赖不是指属性依赖(如 @Autowire 标记的)， // 是 depends-on=\"\" 属性设置的值。一句话就是：不直接依赖于其它 bean 但希望其它 bean 先初始化 void setDependsOn(String... dependsOn); String[] getDependsOn(); // 设置该 bean 是否可以注入到其他 bean 中，只对根据类型注入有效， // 如果根据名称注入，即使这边设置了 false，也是可以的 void setAutowireCandidate(boolean autowireCandidate); boolean isAutowireCandidate(); // 设置是否 primary。同一接口的如果有多个实现，如果不指定名字的话，Spring 会优先选择设置 primary 为 true 的 bean void setPrimary(boolean primary); boolean isPrimary(); // 如果该 Bean 采用工厂方法生成，指定工厂名称。 // 一句话就是：有些实例不是用反射生成的，而是用工厂模式生成的 void setFactoryBeanName(String factoryBeanName); // 获取工厂名称 String getFactoryBeanName(); // 指定工厂类中的 工厂方法名称 void setFactoryMethodName(String factoryMethodName); // 获取工厂类中的 工厂方法名称 String getFactoryMethodName(); // 构造器参数 ConstructorArgumentValues getConstructorArgumentValues(); // Bean 中的属性值，后面给 bean 注入属性值的时候会说到 MutablePropertyValues getPropertyValues(); // 是否 singleton boolean isSingleton(); // 是否 prototype boolean isPrototype(); // 如果这个 Bean 是被设置为 abstract，那么不能实例化， // 常用于作为父 bean 用于继承，其实也很少用...... boolean isAbstract(); int getRole(); String getDescription(); String getResourceDescription(); BeanDefinition getOriginatingBeanDefinition();&#125; 了解了 beanDefinition 之后，接下来我们就要看 Spring 是如何把这个定义信息转化成具体的对象的。 源码分析我们来总结一下，到目前为止，应该说 BeanFactory 已经创建完成，并且所有的实现了 BeanFactoryPostProcessor 接口的 bean 都已经初始化并且其中的 postProcessBeanFactory(factory) 方法已经得到回调执行了。而且 Spring 已经“手动”注册了一些特殊的 bean，如 environment、systemProperties 等。 剩下的就是初始化 singleton beans 了，我们知道它们是单例的，如果没有设置懒加载，那么 Spring 会在接下来初始化所有的 singleton beans。进入上篇略过的 finishBeanFactoryInitialization 方法。 1、finishBeanFactoryInitialization// AbstractApplicationContext 83512345678910111213141516171819202122232425262728293031323334// 初始化剩余的 singleton beansprotected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 首先，初始化名字为 conversionService 的 Bean。 // conversionService 用的很多。它是容器级的全局类型转换器，常见的有 String 转 Date 的类型转换器等 // 什么，看代码这里没有初始化 Bean 啊！ // 注意了，初始化的动作包装在 beanFactory.getBean(...) 中，这里先不说细节，先往下看吧 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // 检查 beanFactory 是否没有内嵌值解析器，默认是有的 if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(new StringValueResolver() &#123; @Override public String resolveStringValue(String strVal) &#123; return getEnvironment().resolvePlaceholders(strVal); &#125; &#125;); &#125; // 加载第三方模块，如 AspectJ；这个不重要，跳过 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // 停用临时类加载器 beanFactory.setTempClassLoader(null); // 禁止对 bean 的定义再修改。 // 没什么别的目的，因为到这一步的时候，Spring 已经开始预初始化 singleton beans 了， // 肯定不希望这个时候还出现 bean 定义解析、加载、注册。 beanFactory.freezeConfiguration(); // 开始初始化 beanFactory.preInstantiateSingletons();&#125; 从上面最后一行往里看，默认 beanFactory 的实现类就是 DefaultListableBeanFactory 这个类了，我们进入它的 preInstantiateSingletons 方法来看一看。 2、preInstantiateSingletons// DefaultListableBeanFactory 7281234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public void preInstantiateSingletons() throws BeansException &#123; // this.beanDefinitionNames 保存了所有的 beanNames List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); // 触发所有的非懒加载的 singleton beans 的初始化操作 for (String beanName : beanNames) &#123; // 合并父 beanDefinition 与子 beanDefinition，涉及到 bean 继承的关系，前面提到过：子 bean 继承父 bean 的配置信息 // 详情可见附录 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // 非抽象、非懒加载的 singletons。如果配置了 'abstract = true'，那是不需要初始化的 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; // 处理 FactoryBean（如果不熟悉，附录中有介绍） if (isFactoryBean(beanName)) &#123; // FactoryBean 的话，在 beanName 前面加上 ‘&amp;’ 符号。再调用 getBean Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); if (bean instanceof FactoryBean) &#123; final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean; boolean isEagerInit; // 这里需要判断是不是 SmartFactoryBean， // 因为 SmartFactoryBean 会定义一个 isEagerInit() 方法来决定 getObject() 的实例对象是否懒加载 if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; isEagerInit = AccessController.doPrivileged((PrivilegedAction&lt;Boolean&gt;) ((SmartFactoryBean&lt;?&gt;) factory)::isEagerInit, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; // 对非懒加载的 bean 实例化 if (isEagerInit) &#123; getBean(beanName); &#125; &#125; &#125; // 对于普通的 Bean，只要调用 getBean(beanName) 这个方法就可以进行初始化了 else &#123; getBean(beanName); &#125; &#125; &#125; // 到这里说明所有的非懒加载的 singleton beans 已经完成了初始化 // 如果我们定义的 bean 是实现了 SmartInitializingSingleton 接口的，那么在这里回调它的 afterSingletonsInstantiated 方法 // 通过名字可以知道它表示单例对象初始化后需要做的操作 for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125;&#125; 接下来，我们就进入到 getBean(beanName) 方法了，这个方法我们经常用来从 BeanFactory 中获取一个 Bean，而初始化的过程也封装到了这个方法里。 3、getBean// AbstractBeanFactory 198123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188@Overridepublic Object getBean(String name) throws BeansException &#123; return doGetBean(name, null, null, false);&#125;// getBean 方法是我们经常用来获取 bean 的，但它也同时封装了初始化的过程，已经初始化过了就从容器中直接返回，否则就先初始化再返回protected &lt;T&gt; T doGetBean(final String name, @Nullable final Class&lt;T&gt; requiredType, @Nullable final Object[] args, boolean typeCheckOnly) throws BeansException &#123; // 获取一个 “正统的” beanName，处理两种情况，一个是前面说的 FactoryBean(前面带 ‘&amp;’)， // 一个是别名问题，因为这个方法是 getBean，获取 Bean 用的，你要是传一个别名进来，是完全可以的 final String beanName = transformedBeanName(name); // 这个是返回值 Object bean; // 检查下是不是已经创建过了 Object sharedInstance = getSingleton(beanName); // if 内部是获取 bean 的逻辑。 // 这里说下 args，前面我们一路进来的时候都是 getBean(beanName)，所以 args 传参其实是 null 的， // 但是如果 args 不为空的时候，那么意味着调用方不是希望获取 Bean，而是创建 Bean if (sharedInstance != null &amp;&amp; args == null) &#123; if (logger.isTraceEnabled()) &#123; if (isSingletonCurrentlyInCreation(beanName)) &#123; logger.trace(\"Returning eagerly cached instance of singleton bean '\" + beanName + \"' that is not fully initialized yet - a consequence of a circular reference\"); &#125; else &#123; logger.trace(\"Returning cached instance of singleton bean '\" + beanName + \"'\"); &#125; &#125; // 下面这个方法，如果是普通 Bean 的话，直接返回 sharedInstance，如果是 FactoryBean 的话，返回它创建的那个实例对象。 // 如果对 FactoryBean 不熟悉，附录中有介绍。 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; // else 内部是初始化 bean 的逻辑 else &#123; // 当前 beanName 的 prototype 类型的 bean 正在被创建则抛异常 // 往往是因为陷入了循环引用。prototype 类型的 bean 的循环引用是没法被解决的。这跟 Java 里面的一样，会导致栈溢出。 if (isPrototypeCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException(beanName); &#125; BeanFactory parentBeanFactory = getParentBeanFactory(); // 检查一下这个 BeanDefinition 在容器中是否存在 if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // 如果当前容器不存在这个 BeanDefinition，试试父容器中有没有 String nameToLookup = originalBeanName(name); // 返回父容器的查询结果 if (parentBeanFactory instanceof AbstractBeanFactory) &#123; // Delegation to parent with explicit args. return ((AbstractBeanFactory) parentBeanFactory).doGetBean( nameToLookup, requiredType, args, typeCheckOnly); &#125; else if (args != null) &#123; // Delegation to parent with explicit args. return (T) parentBeanFactory.getBean(nameToLookup, args); &#125; else if (requiredType != null) &#123; // No args -&gt; delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); &#125; else &#123; return (T) parentBeanFactory.getBean(nameToLookup); &#125; &#125; // typeCheckOnly 为 false，将当前 beanName 放入一个 alreadyCreated 的 Set 集合中 if (!typeCheckOnly) &#123; markBeanAsCreated(beanName); &#125; /** * 稍稍总结一下： * 到这里的话，要准备创建 Bean 了，对于 singleton 的 Bean 来说，容器中还没创建过此 Bean； * 对于 prototype 的 Bean 来说，本来就是要创建一个新的 Bean。 */ try &#123; final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // 先初始化依赖的所有 Bean，注意，这里的依赖指的是 depends-on 中定义的依赖 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) &#123; for (String dep : dependsOn) &#123; // 检查是不是有循环依赖 // 这里的依赖还是 depends-on 中定义的依赖 if (isDependent(beanName, dep)) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Circular depends-on relationship between '\" + beanName + \"' and '\" + dep + \"'\"); &#125; // 注册一下依赖关系 registerDependentBean(dep, beanName); try &#123; // 先初始化被依赖项 getBean(dep); &#125; catch (NoSuchBeanDefinitionException ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"'\" + beanName + \"' depends on missing bean '\" + dep + \"'\", ex); &#125; &#125; &#125; // 如果是 singleton scope 的，创建 singleton 的实例 if (mbd.isSingleton()) &#123; // 这里并没有直接调用 createBean 方法创建 bean 实例，而是通过 getSingleton(String, ObjectFactory) 方法获取 bean 实例。 // getSingleton(String, ObjectFactory) 方法会在内部调用 ObjectFactory 的 getObject() 方法创建 bean，并会在创建完成后， // 将 bean 放入缓存中。 sharedInstance = getSingleton(beanName, () -&gt; &#123; try &#123; // 执行创建 Bean，详情后面再说 return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; destroySingleton(beanName); throw ex; &#125; &#125;); // 跟上面的一样，如果是普通 Bean 的话，直接返回 sharedInstance，如果是 FactoryBean 的话，返回它创建的那个实例对象。 bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; // 如果是 prototype scope 的，创建 prototype 的实例 else if (mbd.isPrototype()) &#123; // prototype 对象每次获取都会创建新的实例 Object prototypeInstance = null; try &#123; beforePrototypeCreation(beanName); // 执行创建 Bean prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); &#125; // 如果不是 singleton 和 prototype 的话，需要委托给相应的实现类来处理 else &#123; String scopeName = mbd.getScope(); final Scope scope = this.scopes.get(scopeName); if (scope == null) &#123; throw new IllegalStateException(\"No Scope registered for scope name '\" + scopeName + \"'\"); &#125; try &#123; Object scopedInstance = scope.get(beanName, () -&gt; &#123; beforePrototypeCreation(beanName); try &#123; // 执行创建 Bean return createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; &#125;); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); &#125; catch (IllegalStateException ex) &#123; throw new BeanCreationException(beanName, \"Scope '\" + scopeName + \"' is not active for the current thread; consider \" + \"defining a scoped proxy for this bean if you intend to refer to it from a singleton\", ex); &#125; &#125; &#125; catch (BeansException ex) &#123; cleanupAfterBeanCreationFailure(beanName); throw ex; &#125; &#125; // 最后，检查一下类型对不对，不对的话就抛异常，对的话就返回了 if (requiredType != null &amp;&amp; !requiredType.isInstance(bean)) &#123; try &#123; T convertedBean = getTypeConverter().convertIfNecessary(bean, requiredType); if (convertedBean == null) &#123; throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); &#125; return convertedBean; &#125; catch (TypeMismatchException ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(\"Failed to convert bean '\" + name + \"' to required type '\" + ClassUtils.getQualifiedName(requiredType) + \"'\", ex); &#125; throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); &#125; &#125; return (T) bean;&#125; 到了这儿，我们会发现当 Spring 获取不到 bean 的时候，会调用 createBean 方法进行创建。singleton 和 prototype 对象的区别在于是否通过 getSingleton 这个方法调用，我们来看看 getSingleton 方法是如何运作的。 4、getSingleton// DefaultSingletonBeanRegistry 202123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(beanName, \"Bean name must not be null\"); synchronized (this.singletonObjects) &#123; // 从 singletonObjects 获取实例，singletonObjects 中缓存的实例都是完全实例化好的 bean，可以直接使用 Object singletonObject = this.singletonObjects.get(beanName); // 如果 singletonObject = null，表明还没创建，或者还没完全创建好。 if (singletonObject == null) &#123; if (this.singletonsCurrentlyInDestruction) &#123; throw new BeanCreationNotAllowedException(beanName, \"Singleton bean creation not allowed while singletons of this factory are in destruction \" + \"(Do not request a bean from a BeanFactory in a destroy method implementation!)\"); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(\"Creating shared instance of singleton bean '\" + beanName + \"'\"); &#125; beforeSingletonCreation(beanName); boolean newSingleton = false; boolean recordSuppressedExceptions = (this.suppressedExceptions == null); if (recordSuppressedExceptions) &#123; this.suppressedExceptions = new LinkedHashSet&lt;&gt;(); &#125; try &#123; // 调用 singletonFactory 的 getObject 方法，就是传入的匿名类，最终也是调用 createBean singletonObject = singletonFactory.getObject(); newSingleton = true; &#125; catch (IllegalStateException ex) &#123; singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; throw ex; &#125; &#125; catch (BeanCreationException ex) &#123; if (recordSuppressedExceptions) &#123; for (Exception suppressedException : this.suppressedExceptions) &#123; ex.addRelatedCause(suppressedException); &#125; &#125; throw ex; &#125; finally &#123; if (recordSuppressedExceptions) &#123; this.suppressedExceptions = null; &#125; afterSingletonCreation(beanName); &#125; if (newSingleton) &#123; // 将创建好的 bean 存入缓存 addSingleton(beanName, singletonObject); &#125; &#125; return singletonObject; &#125;&#125; 我们可以看到 getSingleton 内部也是调用 createBean 方法，只是在调用之前会先去缓存中找，调用之后会把创建好的 bean 存入缓存中。下面就到了重头戏 createBean 方法了。 5、createBean// AbstractAutowireCapableBeanFactory 4471234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Overrideprotected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException &#123; if (logger.isTraceEnabled()) &#123; logger.trace(\"Creating instance of bean '\" + beanName + \"'\"); &#125; RootBeanDefinition mbdToUse = mbd; // 确保 BeanDefinition 中的 Class 被加载 Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) &#123; mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); &#125; // 涉及到一个概念：方法覆写。具体涉及到 lookup-method 和 replace-method 两个标签，附录中会有介绍 try &#123; mbdToUse.prepareMethodOverrides(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, \"Validation of method overrides failed\", ex); &#125; try &#123; // 让 InstantiationAwareBeanPostProcessor 在这一步有机会返回代理，在 AOP 相关文章会有解释，这里先跳过 Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) &#123; return bean; &#125; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, \"BeanPostProcessor before instantiation of bean failed\", ex); &#125; try &#123; // 重头戏，创建 bean Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isTraceEnabled()) &#123; logger.trace(\"Finished creating instance of bean '\" + beanName + \"'\"); &#125; return beanInstance; &#125; catch (BeanCreationException | ImplicitlyAppearedSingletonException ex) &#123; throw ex; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( mbdToUse.getResourceDescription(), beanName, \"Unexpected exception during bean creation\", ex); &#125;&#125; createBean 并不是真正初始化 bean 的方法，而是对 doCreateBean 的预处理。它主要做了两件事情：确保 BeanDefinition 中的 Class 被加载和准备方法覆写。真正创建 bean 对象的逻辑在 doCreateBean 方法里面。 这里我们稍微总结一下。目前为止，我们已经到了真正创建 bean 对象的 doCreateBean 这个方法。而在这之前，从容器创建到开始执行真正的创建，我们也经历了相当多的步骤。主要有： 触发初始化操作 合并父 beanDefinition 与子 beanDefinition 过滤掉抽象、懒加载的、非单例的 bean 预处理 FactoryBean（加’&amp;’，懒加载判断） 别名转化 尝试从缓存中获取 检查 BeanDefinition 是否存在，不存在就去父容器创建 先初始化 depends-on 中定义的依赖 准备方法覆写 创建并缓存 bean 对 FactoryBean 额外处理 返回 bean 这些步骤确保了 bean 可以被创建且能按用户定义的方式创建。下面我们将详细分析真正创建 bean 对象的 doCreateBean 方法内部的逻辑。 6、doCreateBean// AbstractAutowireCapableBeanFactory 529123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException &#123; // BeanWrapper 是一个基础接口，由接口名可看出这个接口的实现类用于包裹 bean 实例。 // 通过 BeanWrapper 的实现类可以方便的设置/获取 bean 实例的属性 BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) &#123; // 从缓存中获取 BeanWrapper，并清理相关记录 instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); &#125; if (instanceWrapper == null) &#123; // 创建 bean 实例，并将实例包裹在 BeanWrapper 实现类对象中返回，之后会细谈 instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; // 此处的 bean 可以认为是一个原始的 bean 实例，暂未填充属性 final Object bean = instanceWrapper.getWrappedInstance(); Class&lt;?&gt; beanType = instanceWrapper.getWrappedClass(); if (beanType != NullBean.class) &#123; mbd.resolvedTargetType = beanType; &#125; // 涉及接口：MergedBeanDefinitionPostProcessor，用于处理已“合并的 BeanDefinition”，这块细节就不展开了 synchronized (mbd.postProcessingLock) &#123; if (!mbd.postProcessed) &#123; try &#123; applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Post-processing of merged bean definition failed\", ex); &#125; mbd.postProcessed = true; &#125; &#125; // earlySingletonExposure 是一个重要的变量，用于解决循环依赖，该变量表示是否提前暴露， // earlySingletonExposure = 单例 &amp;&amp; 是否允许循环依赖 &amp;&amp; 是否存于创建状态中 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(\"Eagerly caching bean '\" + beanName + \"' to allow for resolving potential circular references\"); &#125; // 添加工厂对象到 singletonFactories 缓存中 addSingletonFactory(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; // 获取早期 bean 的引用，如果 bean 中的方法被 AOP 切点所匹配到，此时 AOP 相关逻辑会介入 return getEarlyBeanReference(beanName, mbd, bean); &#125; &#125;); &#125; Object exposedObject = bean; try &#123; // 这一步也是非常关键的，这一步负责属性装配，因为前面的实例只是实例化了，并没有设值，这里就是设值 populateBean(beanName, mbd, instanceWrapper); // 进行余下的初始化工作，之后会细谈 exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; catch (Throwable ex) &#123; if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; throw (BeanCreationException) ex; &#125; else &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Initialization of bean failed\", ex); &#125; &#125; if (earlySingletonExposure) &#123; Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) &#123; if (exposedObject == bean) &#123; exposedObject = earlySingletonReference; &#125; else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123; String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;&gt;(dependentBeans.length); for (String dependentBean : dependentBeans) &#123; if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123; actualDependentBeans.add(dependentBean); &#125; &#125; if (!actualDependentBeans.isEmpty()) &#123; throw new BeanCurrentlyInCreationException(beanName, \"Bean with name '\" + beanName + \"' has been injected into other beans [\" + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + \"] in its raw version as part of a circular reference, but has eventually been \" + \"wrapped. This means that said other beans do not use the final version of the \" + \"bean. This is often the result of over-eager type matching - consider using \" + \"'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example.\"); &#125; &#125; &#125; &#125; // 注册销毁逻辑 try &#123; registerDisposableBeanIfNecessary(beanName, bean, mbd); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Invalid destruction signature\", ex); &#125; return exposedObject;&#125; 我们看到 doCreateBean 内部的逻辑非常多，我们先来总结一下 doCreateBean 方法的执行流程吧，如下： 从缓存中获取 BeanWrapper 实现类对象，并清理相关记录 若未命中缓存，则创建 bean 实例，并将实例包裹在 BeanWrapper 实现类对象中返回 应用 MergedBeanDefinitionPostProcessor 后置处理器相关逻辑 根据条件决定是否提前暴露 bean 的早期引用（early reference），用于处理循环依赖问题 调用 populateBean 方法向 bean 实例中填充属性 调用 initializeBean 方法完成余下的初始化工作 注册销毁逻辑 接下来我们挑 doCreateBean 中的三个细节出来说说。一个是创建 Bean 实例的 createBeanInstance 方法，一个是依赖注入的 populateBean 方法，还有就是回调方法 initializeBean。 6.1、创建实例// AbstractAutowireCapableBeanFactory 111212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) &#123; // 确保已经加载了此 class Class&lt;?&gt; beanClass = resolveBeanClass(mbd, beanName); // 校验一下这个类的访问权限 if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Bean class isn't public, and non-public access not allowed: \" + beanClass.getName()); &#125; Supplier&lt;?&gt; instanceSupplier = mbd.getInstanceSupplier(); if (instanceSupplier != null) &#123; return obtainFromSupplier(instanceSupplier, beanName); &#125; // 如果工厂方法不为空，则通过工厂方法构建 bean 对象。工厂方式可以见附录 if (mbd.getFactoryMethodName() != null) &#123; return instantiateUsingFactoryMethod(beanName, mbd, args); &#125; // 如果不是第一次创建，比如第二次创建 prototype bean。这种情况下，我们可以从第一次创建知道， // 采用无参构造函数，还是构造函数依赖注入来完成实例化。 // 这里的 resolved 和 mbd.constructorArgumentsResolved 将会在 bean 第一次实例化的过程中被设置，在后面的源码中会分析到，先继续往下看。 boolean resolved = false; boolean autowireNecessary = false; if (args == null) &#123; synchronized (mbd.constructorArgumentLock) &#123; if (mbd.resolvedConstructorOrFactoryMethod != null) &#123; resolved = true; autowireNecessary = mbd.constructorArgumentsResolved; &#125; &#125; &#125; if (resolved) &#123; if (autowireNecessary) &#123; // 通过有参构造器构造 bean 对象 return autowireConstructor(beanName, mbd, null, null); &#125; else &#123; // 通过无参构造器构造 bean 对象 return instantiateBean(beanName, mbd); &#125; &#125; // 判断是否采用有参构造函数 Constructor&lt;?&gt;[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); if (ctors != null || mbd.getResolvedAutowireMode() == AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123; // 通过有参构造器构造 bean 对象 return autowireConstructor(beanName, mbd, ctors, args); &#125; // 通过无参构造器构造 bean 对象 return instantiateBean(beanName, mbd);&#125; 以上就是 createBeanInstance 方法的源码，不是很长。配合着注释，应该不是很难懂。下面我们来总结一下这个方法的执行流程，如下： 检测类的访问权限，若禁止访问，则抛出异常 若工厂方法不为空，则通过工厂方法构建 bean 对象，并返回结果 若构造方式已解析过，则走快捷路径构建 bean 对象，并返回结果 如第三步不满足，则通过组合条件决定使用哪种方式构建 bean 对象 工厂方法就不深入了，下面我们来看看使用有参构造器方式和无参构造器方式创建 bean 的过程。 6.1.1、有参构造器方式// AbstractAutowireCapableBeanFactory 1305123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284protected BeanWrapper autowireConstructor( String beanName, RootBeanDefinition mbd, Constructor&lt;?&gt;[] ctors, Object[] explicitArgs) &#123; // 创建 ConstructorResolver 对象，并调用其 autowireConstructor 方法 return new ConstructorResolver(this).autowireConstructor(beanName, mbd, ctors, explicitArgs);&#125;public BeanWrapper autowireConstructor(String beanName, RootBeanDefinition mbd, @Nullable Constructor&lt;?&gt;[] chosenCtors, @Nullable Object[] explicitArgs) &#123; // 创建 BeanWrapperImpl 对象 BeanWrapperImpl bw = new BeanWrapperImpl(); this.beanFactory.initBeanWrapper(bw); Constructor&lt;?&gt; constructorToUse = null; ArgumentsHolder argsHolderToUse = null; Object[] argsToUse = null; // 确定参数值列表（argsToUse） if (explicitArgs != null) &#123; argsToUse = explicitArgs; &#125; else &#123; Object[] argsToResolve = null; synchronized (mbd.constructorArgumentLock) &#123; // 获取已解析的构造方法 constructorToUse = (Constructor&lt;?&gt;) mbd.resolvedConstructorOrFactoryMethod; if (constructorToUse != null &amp;&amp; mbd.constructorArgumentsResolved) &#123; // 获取已解析的构造方法参数列表 argsToUse = mbd.resolvedConstructorArguments; if (argsToUse == null) &#123; // 若 argsToUse 为空，则获取未解析的构造方法参数列表 argsToResolve = mbd.preparedConstructorArguments; &#125; &#125; &#125; if (argsToResolve != null) &#123; // 解析参数列表 argsToUse = resolvePreparedArguments(beanName, mbd, bw, constructorToUse, argsToResolve); &#125; &#125; if (constructorToUse == null) &#123; boolean autowiring = (chosenCtors != null || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR); ConstructorArgumentValues resolvedValues = null; int minNrOfArgs; if (explicitArgs != null) &#123; minNrOfArgs = explicitArgs.length; &#125; else &#123; ConstructorArgumentValues cargs = mbd.getConstructorArgumentValues(); resolvedValues = new ConstructorArgumentValues(); /* * 确定构造方法参数数量，比如下面的配置： * &lt;bean id=\"persion\" class=\"com.huzb.demo.Person\"&gt; * &lt;constructor-arg index=\"0\" value=\"xiaoming\"/&gt; * &lt;constructor-arg index=\"1\" value=\"1\"/&gt; * &lt;constructor-arg index=\"2\" value=\"man\"/&gt; * &lt;/bean&gt; * * 此时 minNrOfArgs = maxIndex + 1 = 2 + 1 = 3，除了计算 minNrOfArgs， * 下面的方法还会将 cargs 中的参数数据转存到 resolvedValues 中 */ minNrOfArgs = resolveConstructorArguments(beanName, mbd, bw, cargs, resolvedValues); &#125; // 获取构造方法列表 Constructor&lt;?&gt;[] candidates = chosenCtors; if (candidates == null) &#123; Class&lt;?&gt; beanClass = mbd.getBeanClass(); try &#123; candidates = (mbd.isNonPublicAccessAllowed() ? beanClass.getDeclaredConstructors() : beanClass.getConstructors()); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Resolution of declared constructors on bean Class [\" + beanClass.getName() + \"] from ClassLoader [\" + beanClass.getClassLoader() + \"] failed\", ex); &#125; &#125; // 按照构造方法的访问权限级别和参数数量进行排序 AutowireUtils.sortConstructors(candidates); int minTypeDiffWeight = Integer.MAX_VALUE; Set&lt;Constructor&lt;?&gt;&gt; ambiguousConstructors = null; LinkedList&lt;UnsatisfiedDependencyException&gt; causes = null; for (Constructor&lt;?&gt; candidate : candidates) &#123; Class&lt;?&gt;[] paramTypes = candidate.getParameterTypes(); /* * 下面的 if 分支的用途是：若匹配到到合适的构造方法了，提前结束 for 循环 * constructorToUse != null 这个条件比较好理解，下面分析一下条件 argsToUse.length &gt; paramTypes.length： * 前面说到 AutowireUtils.sortConstructors(candidates) 用于对构造方法进行排序，排序规则如下： * 1. 具有 public 访问权限的构造方法排在非 public 构造方法前 * 2. 参数数量多的构造方法排在前面 * * 假设现在有一组构造方法按照上面的排序规则进行排序，排序结果如下（省略参数名称）： * * 1. public Hello(Object, Object, Object) * 2. public Hello(Object, Object) * 3. public Hello(Object) * 4. protected Hello(Integer, Object, Object, Object) * 5. protected Hello(Integer, Object, Object) * 6. protected Hello(Integer, Object) * * argsToUse = [num1, obj2]，可以匹配上的构造方法2和构造方法6。由于构造方法2有更高的访问权限， * 所以没理由不选他（尽管后者在参数类型上更加匹配）。 由于构造方法3参数数量 &lt; argsToUse.length， * 参数数量上不匹配，也不应该选。所以 argsToUse.length &gt; paramTypes.length 这个条件用途是： * 在当前找到候选构造器的情况下，如果接下来的构造器参数个数小于传入的参数个数，就没必要继续找下去了。 */ if (constructorToUse != null &amp;&amp; argsToUse.length &gt; paramTypes.length) &#123; break; &#125; // 构造方法参数数量低于配置的参数数量，则忽略当前构造方法。 if (paramTypes.length &lt; minNrOfArgs) &#123; continue; &#125; ArgumentsHolder argsHolder; if (resolvedValues != null) &#123; try &#123; /* * 判断否则方法是否有 ConstructorProperties 注解，若有，则取注解中的值。比如下面的代码： * * public class Persion &#123; * private String name; * private Integer age; * * @ConstructorProperties(value = &#123;\"huzb\", \"20\"&#125;) * public Persion(String name, Integer age) &#123; * this.name = name; * this.age = age; * &#125; * &#125; */ String[] paramNames = ConstructorPropertiesChecker.evaluate(candidate, paramTypes.length); if (paramNames == null) &#123; ParameterNameDiscoverer pnd = this.beanFactory.getParameterNameDiscoverer(); if (pnd != null) &#123; /* * 获取构造方法参数名称列表，比如有这样一个构造方法: * public Person(String name, int age, String sex) * * 调用 getParameterNames 方法返回 paramNames = [name, age, sex] * 这里要注意 Java 的反射是无法获取参数名的，获取到的是 arg0,arg1... * Spring 使用 ASM 字节码操作框架来获取方法参数的名称。 */ paramNames = pnd.getParameterNames(candidate); &#125; &#125; /* * 创建参数值列表，返回 argsHolder 会包含进行类型转换后的参数值，比如下面的配置: * * &lt;bean id=\"persion\" class=\"com.huzb.demo.Person\"&gt; * &lt;constructor-arg name=\"name\" value=\"xiaoming\"/&gt; * &lt;constructor-arg name=\"age\" value=\"1\"/&gt; * &lt;constructor-arg name=\"sex\" value=\"man\"/&gt; * &lt;/bean&gt; * * Person 的成员变量 age 是 Integer 类型的，但由于在 Spring 配置中只能配成 String 类型，所以这里要进行类型转换。 * 注解方式下，这里会自动注入按类型匹配的参数 */ argsHolder = createArgumentArray(beanName, mbd, resolvedValues, bw, paramTypes, paramNames, getUserDeclaredConstructor(candidate), autowiring); &#125; catch (UnsatisfiedDependencyException ex) &#123; if (this.beanFactory.logger.isTraceEnabled()) &#123; this.beanFactory.logger.trace( \"Ignoring constructor [\" + candidate + \"] of bean '\" + beanName + \"': \" + ex); &#125; if (causes == null) &#123; causes = new LinkedList&lt;UnsatisfiedDependencyException&gt;(); &#125; causes.add(ex); continue; &#125; &#125; else &#123; if (paramTypes.length != explicitArgs.length) &#123; continue; &#125; argsHolder = new ArgumentsHolder(explicitArgs); &#125; /* * 计算参数值（argsHolder.arguments）每个参数类型与构造方法参数列表（paramTypes）中参数的类型差异量， * 差异量越大表明参数类型差异越大。参数类型差异越大，表明当前构造方法并不是一个最合适的候选项。 * 引入差异量（typeDiffWeight）变量目的：是将候选构造方法的参数列表类型与参数值列表类型的差异进行量化，通过量化 * 后的数值筛选出最合适的构造方法。 */ int typeDiffWeight = (mbd.isLenientConstructorResolution() ? argsHolder.getTypeDifferenceWeight(paramTypes) : argsHolder.getAssignabilityWeight(paramTypes)); if (typeDiffWeight &lt; minTypeDiffWeight) &#123; constructorToUse = candidate; argsHolderToUse = argsHolder; argsToUse = argsHolder.arguments; minTypeDiffWeight = typeDiffWeight; ambiguousConstructors = null; &#125; /* * 如果两个构造方法与参数值类型列表之间的差异量一致，那么这两个方法都可以作为候选项，这个时候就出现歧义了， * 这里先把有歧义的构造方法放入 ambiguousConstructors 集合中 */ else if (constructorToUse != null &amp;&amp; typeDiffWeight == minTypeDiffWeight) &#123; if (ambiguousConstructors == null) &#123; ambiguousConstructors = new LinkedHashSet&lt;Constructor&lt;?&gt;&gt;(); ambiguousConstructors.add(constructorToUse); &#125; ambiguousConstructors.add(candidate); &#125; &#125; // 若上面未能筛选出合适的构造方法，这里将抛出 BeanCreationException 异常 if (constructorToUse == null) &#123; if (causes != null) &#123; UnsatisfiedDependencyException ex = causes.removeLast(); for (Exception cause : causes) &#123; this.beanFactory.onSuppressedException(cause); &#125; throw ex; &#125; throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Could not resolve matching constructor \" + \"(hint: specify index/type/name arguments for simple parameters to avoid type ambiguities)\"); &#125; /* * 如果 constructorToUse != null，且 ambiguousConstructors 也不为空，表明解析出了多个的合适的构造方法， * 此时就出现歧义了。Spring 不会擅自决定使用哪个构造方法，所以抛出异常。 */ else if (ambiguousConstructors != null &amp;&amp; !mbd.isLenientConstructorResolution()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Ambiguous constructor matches found in bean '\" + beanName + \"' \" + \"(hint: specify index/type/name arguments for simple parameters to avoid type ambiguities): \" + ambiguousConstructors); &#125; if (explicitArgs == null) &#123; /* * 缓存相关信息，比如： * 1. 已解析出的构造方法对象 resolvedConstructorOrFactoryMethod * 2. 构造方法参数列表是否已解析标志 constructorArgumentsResolved * 3. 参数值列表 resolvedConstructorArguments 或 preparedConstructorArguments * * 这些信息可用在其他地方，用于进行快捷判断 */ argsHolderToUse.storeCache(mbd, constructorToUse); &#125; &#125; try &#123; Object beanInstance; if (System.getSecurityManager() != null) &#123; final Constructor&lt;?&gt; ctorToUse = constructorToUse; final Object[] argumentsToUse = argsToUse; beanInstance = AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; return beanFactory.getInstantiationStrategy().instantiate( mbd, beanName, beanFactory, ctorToUse, argumentsToUse); &#125; &#125;, beanFactory.getAccessControlContext()); &#125; else &#123; // 调用实例化策略创建实例，默认情况下使用反射创建实例。 // 如果 bean 的配置信息中，包含 lookup-method 和 replace-method，则通过 CGLIB 增强 bean 实例 beanInstance = this.beanFactory.getInstantiationStrategy().instantiate( mbd, beanName, this.beanFactory, constructorToUse, argsToUse); &#125; // 设置 beanInstance 到 BeanWrapperImpl 对象中 bw.setBeanInstance(beanInstance); return bw; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Bean instantiation via constructor failed\", ex); &#125;&#125; 上面的方法逻辑比较复杂，做了不少事情，该方法的核心逻辑是根据参数值类型筛选合适的构造方法。解析出合适的构造方法后，剩下的工作就是构建 bean 对象了，这个工作交给了实例化策略去做。下面罗列一下这个方法的工作流程吧： 创建 BeanWrapperImpl 对象 解析构造方法参数，并算出 minNrOfArgs 获取构造方法列表，并排序 遍历排序好的构造方法列表，筛选合适的构造方法 获取构造方法参数列表中每个参数的名称 再次解析参数，此次解析会将value 属性值进行类型转换，由 String 转为合适的类型；注解方式下会自动注入对应类型的参数。 计算构造方法参数列表与参数值列表之间的类型差异量，以筛选出更为合适的构造方法 缓存已筛选出的构造方法以及参数值列表，若再次创建 bean 实例时，可直接使用，无需再次进行筛选 使用初始化策略创建 bean 对象 将 bean 对象放入 BeanWrapperImpl 对象中，并返回该对象 6.1.2、无参构造器方式// AbstractAutowireCapableBeanFactory 12521234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) &#123; try &#123; Object beanInstance; final BeanFactory parent = this; // if 条件分支里的一大坨是 Java 安全相关的代码，可以忽略，直接看 else 分支 if (System.getSecurityManager() != null) &#123; beanInstance = AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; return getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; &#125;, getAccessControlContext()); &#125; else &#123; /** * 调用实例化策略创建实例，默认情况下使用反射创建对象。如果 bean 的配置信息中 * 包含 lookup-method 和 replace-method，则通过 CGLIB 创建 bean 对象 */ beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; // 创建 BeanWrapperImpl 对象 BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Instantiation of bean failed\", ex); &#125;&#125;public Object instantiate(RootBeanDefinition bd, String beanName, BeanFactory owner) &#123; // 检测 bean 配置中是否配置了 lookup-method 或 replace-method，若配置了，则需使用 CGLIB 构建 bean 对象 if (bd.getMethodOverrides().isEmpty()) &#123; Constructor&lt;?&gt; constructorToUse; synchronized (bd.constructorArgumentLock) &#123; constructorToUse = (Constructor&lt;?&gt;) bd.resolvedConstructorOrFactoryMethod; if (constructorToUse == null) &#123; final Class&lt;?&gt; clazz = bd.getBeanClass(); if (clazz.isInterface()) &#123; throw new BeanInstantiationException(clazz, \"Specified class is an interface\"); &#125; try &#123; if (System.getSecurityManager() != null) &#123; constructorToUse = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Constructor&lt;?&gt;&gt;() &#123; @Override public Constructor&lt;?&gt; run() throws Exception &#123; return clazz.getDeclaredConstructor((Class[]) null); &#125; &#125;); &#125; else &#123; // 获取默认构造方法 constructorToUse = clazz.getDeclaredConstructor((Class[]) null); &#125; // 设置 resolvedConstructorOrFactoryMethod bd.resolvedConstructorOrFactoryMethod = constructorToUse; &#125; catch (Throwable ex) &#123; throw new BeanInstantiationException(clazz, \"No default constructor found\", ex); &#125; &#125; &#125; // 通过无参构造方法创建 bean 对象 return BeanUtils.instantiateClass(constructorToUse); &#125; else &#123; // 使用 CGLIB 创建 bean 对象 return instantiateWithMethodInjection(bd, beanName, owner); &#125;&#125; 到这里，我们就算实例化完成了。我们开始说怎么进行属性注入。 6.2、属性注入// AbstractAutowireCapableBeanFactory 131912345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576protected void populateBean(String beanName, RootBeanDefinition mbd, BeanWrapper bw) &#123; // bean 实例的所有属性都在这里了 PropertyValues pvs = mbd.getPropertyValues(); if (bw == null) &#123; if (!pvs.isEmpty()) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Cannot apply property values to null instance\"); &#125; else &#123; return; &#125; &#125; // 到这步的时候，bean 实例化完成（通过工厂方法或构造方法），但是还没开始属性设值， // InstantiationAwareBeanPostProcessor 的实现类可以在这里对 bean 进行状态设置，比如忽略属性值的设置 boolean continueWithPropertyPopulation = true; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 如果返回 false，代表不需要进行后续的属性设值，也不需要再经过其他的 BeanPostProcessor 的处理 if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; continueWithPropertyPopulation = false; break; &#125; &#125; &#125; &#125; if (!continueWithPropertyPopulation) &#123; return; &#125; if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // 通过名字找到所有属性值，如果是 bean 依赖，先初始化依赖的 bean。记录依赖关系 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; // 通过类型装配。复杂一些 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; pvs = newPvs; &#125; boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE); if (hasInstAwareBpps || needsDepCheck) &#123; PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); if (hasInstAwareBpps) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 这里有个非常有用的 BeanPostProcessor 进到这里: AutowiredAnnotationBeanPostProcessor // 对采用 @Autowired、@Value 注解的依赖进行设值 pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvs == null) &#123; return; &#125; &#125; &#125; &#125; if (needsDepCheck) &#123; checkDependencies(beanName, mbd, filteredPds, pvs); &#125; &#125; // 设置 bean 实例的属性值 applyPropertyValues(beanName, mbd, bw, pvs);&#125; 上面的源码注释的比较详细了，下面我们来总结一下这个方法的执行流程。如下： 获取属性列表 pvs 在属性未注入之前，使用后置处理器进行状态设置 根据名称或类型解析相关依赖 再次应用后置处理，用于实现基于注解的属性注入 将属性应用到 bean 对象中 注意第3步，也就是根据名称或类型解析相关依赖。该逻辑只会解析依赖，并不会将解析出的依赖立即注入到 bean 对象中。这里解析出的属性值是在 applyPropertyValues 方法中统一被注入到 bean 对象中的。我们常用到的注解式属性注入比较多，所以这里就看一下基于注解的属性注入。 6.2.1、基于注解的属性注入注解形式的属性注入是通过 AutowiredAnnotationBeanPostProcessor 的 postProcessProperties 方法实现的。源码如下：// AutowiredAnnotationBeanPostProcessor 371123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213@Overridepublic PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName) &#123; // 找到要以注解形式注入的属性信息 InjectionMetadata metadata = findAutowiringMetadata(beanName, bean.getClass(), pvs); try &#123; // 开始注入 metadata.inject(bean, beanName, pvs); &#125; catch (BeanCreationException ex) &#123; throw ex; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, \"Injection of autowired dependencies failed\", ex); &#125; return pvs;&#125;public void inject(Object target, @Nullable String beanName, @Nullable PropertyValues pvs) throws Throwable &#123; Collection&lt;InjectedElement&gt; checkedElements = this.checkedElements; Collection&lt;InjectedElement&gt; elementsToIterate = (checkedElements != null ? checkedElements : this.injectedElements); if (!elementsToIterate.isEmpty()) &#123; for (InjectedElement element : elementsToIterate) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(\"Processing injected element of bean '\" + beanName + \"': \" + element); &#125; // 对每个属性依次注入 element.inject(target, beanName, pvs); &#125; &#125;&#125;@Overrideprotected void inject(Object bean, @Nullable String beanName, @Nullable PropertyValues pvs) throws Throwable &#123; // 要注入的字段 Field field = (Field) this.member; Object value; // 会把注入过的属性缓存起来 if (this.cached) &#123; value = resolvedCachedArgument(beanName, this.cachedFieldValue); &#125; else &#123; // 字段的描述，包括字段名、是否必需、所属类、注解信息等 DependencyDescriptor desc = new DependencyDescriptor(field, this.required); desc.setContainingClass(bean.getClass()); Set&lt;String&gt; autowiredBeanNames = new LinkedHashSet&lt;&gt;(1); Assert.state(beanFactory != null, \"No BeanFactory available\"); // 类型转换器，用于将 String 类型转换成其它类型 TypeConverter typeConverter = beanFactory.getTypeConverter(); try &#123; // 核心，实际解析属性的地方，返回的是依赖的实例 value = beanFactory.resolveDependency(desc, beanName, autowiredBeanNames, typeConverter); &#125; catch (BeansException ex) &#123; throw new UnsatisfiedDependencyException(null, beanName, new InjectionPoint(field), ex); &#125; synchronized (this) &#123; if (!this.cached) &#123; if (value != null || this.required) &#123; this.cachedFieldValue = desc; registerDependentBeans(beanName, autowiredBeanNames); if (autowiredBeanNames.size() == 1) &#123; String autowiredBeanName = autowiredBeanNames.iterator().next(); if (beanFactory.containsBean(autowiredBeanName) &amp;&amp; beanFactory.isTypeMatch(autowiredBeanName, field.getType())) &#123; this.cachedFieldValue = new ShortcutDependencyDescriptor( desc, autowiredBeanName, field.getType()); &#125; &#125; &#125; else &#123; this.cachedFieldValue = null; &#125; this.cached = true; &#125; &#125; &#125; if (value != null) &#123; ReflectionUtils.makeAccessible(field); // 通过反射注入 field.set(bean, value); &#125;&#125;public Object resolveDependency(DependencyDescriptor descriptor, String requestingBeanName, Set&lt;String&gt; autowiredBeanNames, TypeConverter typeConverter) throws BeansException &#123; descriptor.initParameterNameDiscovery(getParameterNameDiscoverer()); if (javaUtilOptionalClass == descriptor.getDependencyType()) &#123; return new OptionalDependencyFactory().createOptionalDependency(descriptor, requestingBeanName); &#125; else if (ObjectFactory.class == descriptor.getDependencyType() || ObjectProvider.class == descriptor.getDependencyType()) &#123; return new DependencyObjectProvider(descriptor, requestingBeanName); &#125; else if (javaxInjectProviderClass == descriptor.getDependencyType()) &#123; return new Jsr330ProviderFactory().createDependencyProvider(descriptor, requestingBeanName); &#125; else &#123; Object result = getAutowireCandidateResolver().getLazyResolutionProxyIfNecessary( descriptor, requestingBeanName); if (result == null) &#123; // 核心，解析依赖 result = doResolveDependency(descriptor, requestingBeanName, autowiredBeanNames, typeConverter); &#125; return result; &#125;&#125;public Object doResolveDependency(DependencyDescriptor descriptor, String beanName, Set&lt;String&gt; autowiredBeanNames, TypeConverter typeConverter) throws BeansException &#123; InjectionPoint previousInjectionPoint = ConstructorResolver.setCurrentInjectionPoint(descriptor); try &#123; // 该方法最终调用了 beanFactory.getBean(String, Class)，从容器中获取依赖 Object shortcut = descriptor.resolveShortcut(this); // 如果容器中存在所需依赖，这里进行断路操作，提前结束依赖解析逻辑 if (shortcut != null) &#123; return shortcut; &#125; Class&lt;?&gt; type = descriptor.getDependencyType(); // 处理 @value 注解 Object value = getAutowireCandidateResolver().getSuggestedValue(descriptor); if (value != null) &#123; if (value instanceof String) &#123; String strVal = resolveEmbeddedValue((String) value); BeanDefinition bd = (beanName != null &amp;&amp; containsBean(beanName) ? getMergedBeanDefinition(beanName) : null); value = evaluateBeanDefinitionString(strVal, bd); &#125; TypeConverter converter = (typeConverter != null ? typeConverter : getTypeConverter()); return (descriptor.getField() != null ? converter.convertIfNecessary(value, type, descriptor.getField()) : converter.convertIfNecessary(value, type, descriptor.getMethodParameter())); &#125; // 解析数组、list、map 等类型的依赖 Object multipleBeans = resolveMultipleBeans(descriptor, beanName, autowiredBeanNames, typeConverter); if (multipleBeans != null) &#123; return multipleBeans; &#125; /* * findAutowireCandidates 这个方法逻辑比较复杂，它返回的是一个&lt;名称，类型/实例&gt;的候选列表。比如下面的配置： * * &lt;bean name=\"mongoDao\" class=\"com.huzb.demo.MongoDao\" primary=\"true\"/&gt; * &lt;bean name=\"service\" class=\"com.huzb.demo.Service\" autowire=\"byType\"/&gt; * &lt;bean name=\"mysqlDao\" class=\"com.huzb.demo.MySqlDao\"/&gt; * * 我们假设这个属性的类型是 Dao，而 mongoDao 和 mysqlDao 都继承了 Dao 接口，mongoDao 已被实例化，mysqlDao * 尚未实例化，那么返回的候选列表就是： * * matchingBeans = [ &lt;\"mongoDao\", Object@MongoDao&gt;, &lt;\"mysqlDao\", Class@MySqlDao&gt; ] * * 方法内部的工作流程如下： * 1. 方法开始时有一个 type 记录需要的属性的类型信息。 * 2. 类型如果是容器对象（我们在容器准备时放进 resolvableDependencies 的），那直接从容器中拿到，加入候选列表。 * 3. 根据类型信息从 BeanFactory 中获取某种类型 bean 的名称列表，比如按上面配置拿到的就是[\"mongoDao\",\"mysqlDao\"] * 4. 遍历上一步得到的名称列表，并判断 bean 名称对应的 bean 是否是合适的候选项，若是合适，则把实例对象（已实例化） * 或类型（未实例化）加入候选列表 * 5. 返回候选列表 */ Map&lt;String, Object&gt; matchingBeans = findAutowireCandidates(beanName, type, descriptor); if (matchingBeans.isEmpty()) &#123; if (isRequired(descriptor)) &#123; // 抛出 NoSuchBeanDefinitionException 异常 raiseNoMatchingBeanFound(type, descriptor.getResolvableType(), descriptor); &#125; return null; &#125; String autowiredBeanName; Object instanceCandidate; if (matchingBeans.size() &gt; 1) &#123; /* * matchingBeans.size() &gt; 1，则表明存在多个可注入的候选项，这里判断使用哪一个候选项。 * 候选项的判定规则是： * 1）声明了 primary 的优先级最高 * 2）实现了排序接口，如 Ordered 的优先级比没实现排序接口的高；同样实现了排序接口的会通过比较器比较 * 3）还没有得到结果的话，则按字段名进行匹配 */ autowiredBeanName = determineAutowireCandidate(matchingBeans, descriptor); if (autowiredBeanName == null) &#123; if (isRequired(descriptor) || !indicatesMultipleBeans(type)) &#123; // 抛出 NoUniqueBeanDefinitionException 异常 return descriptor.resolveNotUnique(type, matchingBeans); &#125; else &#123; return null; &#125; &#125; // 根据解析出的 autowiredBeanName，获取相应的候选项 instanceCandidate = matchingBeans.get(autowiredBeanName); &#125; else &#123; // 只有一个候选项，直接取出来即可 Map.Entry&lt;String, Object&gt; entry = matchingBeans.entrySet().iterator().next(); autowiredBeanName = entry.getKey(); instanceCandidate = entry.getValue(); &#125; if (autowiredBeanNames != null) &#123; autowiredBeanNames.add(autowiredBeanName); &#125; // 返回候选项实例，如果实例是 Class 类型，则调用 beanFactory.getBean(String, Class) 获取相应的 bean。否则直接返回即可 return (instanceCandidate instanceof Class ? descriptor.resolveCandidate(autowiredBeanName, type, this) : instanceCandidate); &#125; finally &#123; ConstructorResolver.setCurrentInjectionPoint(previousInjectionPoint); &#125;&#125; 代码调用很长，实际的逻辑总结起来有： 找到要以注解形式注入的属性信息 依次对每个字段操作 构造字段的描述，包括字段名、是否必需、所属类、注解信息等 获取属性值 @Value 设置的值，将 String 转成对应类型后返回 依赖的类型是个数组或集合，会将符合条件的 bean 全部放在数组或集合中返回 普通类型返回一个候选列表，然后根据判定规则选出优先级最高的一个 如果得到的属性是个 Class 对象，则调用 getBean 生成实例 通过反射注入 6.3、处理各种回调属性注入完成后，这一步其实就是处理各种回调了，这块代码比较简单。// AbstractAutowireCapableBeanFactory 17251234567891011121314151617181920212223242526272829303132333435363738protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) &#123; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; invokeAwareMethods(beanName, bean); return null; &#125; &#125;, getAccessControlContext()); &#125; else &#123; // 如果 bean 实现了 BeanNameAware、BeanClassLoaderAware 或 BeanFactoryAware 接口，回调 invokeAwareMethods(beanName, bean); &#125; Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; // BeanPostProcessor 的 postProcessBeforeInitialization 回调 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; // 处理 bean 中定义的 init-method， // 或者如果 bean 实现了 InitializingBean 接口，调用 afterPropertiesSet() 方法 invokeInitMethods(beanName, wrappedBean, mbd); &#125; catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, \"Invocation of init method failed\", ex); &#125; if (mbd == null || !mbd.isSynthetic()) &#123; // BeanPostProcessor 的 postProcessAfterInitialization 回调 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; 大家发现没有，BeanPostProcessor 的两个回调都发生在这边，只不过中间处理了 init-method，是不是和原来的认知有点不一样了？ 总结目前为止，我们已经走完了创建一个 bean 对象的流程。这个流程分为两步：创建对象的前置处理和真正创建对象的过程。创建对象的前置处理确保了对象能按用户定义的方式创建，这些步骤包括： 触发初始化操作 合并父 beanDefinition 与子 beanDefinition 过滤掉抽象、懒加载的、非单例的 bean 预处理 FactoryBean（加’&amp;’，懒加载判断） 别名转化 尝试从缓存中获取 检查 BeanDefinition 是否存在，不存在就去父容器创建 先初始化 depends-on 中定义的依赖 准备方法覆写 准备创建对象 实例化对象 属性注入 处理回调 对 FactoryBean 额外处理 缓存对象 返回 bean 创建对象这个过程是比较重要的，它一共有三步：实例化对象——属性注入——处理回调。Spring 实例化对象分为工厂方式和构造器方式，构造器方式有分为有参构造器和无参构造器。有参构造器会根据传入的参数自动选择合适的构造器实例化对象。实例化之后 Spring 又会为对象注入属性。这里又分为配置文件的方式和注解的方式，注解方式有一系列规则选择最合适的实例注入。最后 Spring 调用 BeanPostProcessor 的前置回调和后置回调，我们发现这两个回调是在同一个方法中实现的，只不过中间处理了 init-method 方法。然后 Spring 会把对象放入缓存中，以后可以直接取出。 附录1、合并父 BeanDefinition 与子 BeanDefinitionSpring 支持配置继承，在标签中可以使用 parent 属性配置父类 bean。这样子类 bean 可以继承父类 bean 的配置信息，同时也可覆盖父类中的配置。比如下面的配置：1234567&lt;bean id=\"hello\" class=\"com.huzb.demo.Hello\"&gt; &lt;property name=\"name\" value=\"hello\"/&gt; &lt;property name=\"content\" value=\"I`m hello-parent\"/&gt;&lt;/bean&gt;&lt;bean id=\"hello-child\" parent=\"hello\"&gt; &lt;property name=\"content\" value=\"I`m hello-child\"/&gt;&lt;/bean&gt; hello-child 继承了 hello 的两个属性，但是对 content 属性进行了覆写。子 bean 还会继承父 bean 的 scope、构造器参数值、属性值、init-method、destroy-method 等等。 2、FactoryBeanFactoryBean 适用于 Bean 的创建过程比较复杂的场景，比如数据库连接池的创建等。这里我们用一个简单的例子演示：1234567891011121314151617public class CarFactoryBean implements FactoryBean&lt;Car&gt; &#123; @Override public Car getObject() throws Exception &#123; Car car = new Car(); return car; &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Car.class; &#125; @Override public boolean isSingleton()&#123; return true; &#125;&#125; 我们可以通过这两种方式获取生成的 bean：123// 注意这里的 Bean Name 是 FactoryBean 的类名开头小写而不是 Bean 的类名开头小写Car car1 = (Car) applicationContext.getBean(\"carFactoryBean\");Car car2 = (Car) applicationContext.getBean(Car.class); FactoryBean 本身也是 bean 想要获取的话，也有两种方式：123// 注意加了'&amp;'CarFactoryBean carFactoryBean1 = (CarFactoryBean) applicationContext.getBean(\"&amp;carFactoryBean\");CarFactoryBean carFactoryBean2 = (CarFactoryBean) applicationContext.getBean(CarFactoryBean.class); 3、方法覆写3.1、lookup-method我们通过 BeanFactory getBean 方法获取 bean 实例时，对于 singleton 类型的 bean，BeanFactory 每次返回的都是同一个 bean。对于 prototype 类型的 bean，BeanFactory 则会返回一个新的 bean。现在考虑这样一种情况，一个 singleton 类型的 bean 中有一个 prototype 类型的成员变量。BeanFactory 在实例化 singleton 类型的 bean 时，会向其注入一个 prototype 类型的实例。但是 singleton 类型的 bean 只会实例化一次，那么它内部的 prototype 类型的成员变量也就不会再被改变。但如果我们每次从 singleton bean 中获取这个 prototype 成员变量时，都想获取一个新的对象。这个时候怎么办？123456789@Componentpublic class NewsProvider &#123; @Autowired News news; // prototype bean public News getNews() &#123; return news; &#125;&#125; 这种情况下每次调用 getNews 获得的都是同一个对象。我们可以使用@Lookup 解决这个问题：12345678910@Componentpublic class NewsProvider &#123; @Autowired News news; // prototype bean @Lookup public News getNews() &#123; return news; &#125;&#125; 标注了@Lookup 后 Spring 会采用 CGLIB 生成字节码的方式来生成一个子类，这个子类的 getNews 方法每次返回的都是一个新的 News 对象。 3.2、replaced-methodreplaced-method 的作用是替换掉 bean 中的一些方法，同样是基于 CGLIB 实现的。首先定义一个 bean：123456public class Car &#123; public void run() &#123; System.out.print(\"run...\"); &#125;&#125; 然后定义一个方法覆写类，需要继承 MethodReplacer 接口：1234567public class CarReplacer implements MethodReplacer &#123; @Override public Object reimplement(Object obj, Method method, Object[] args) throws Throwable &#123; System.out.print(\"replacer...\"); return null; &#125;&#125; 配置文件;1234&lt;bean id=\"car\" class=\"com.huzb.demo.Car\" &gt; &lt;replaced-method name=\"run\" replacer=\"carReplacer\"/&gt;&lt;/bean&gt;&lt;bean id=\"carReplacer\" class=\"com.huzb.demo.CarReplacer\" /&gt; 这样调用 Car 的实例的 run 方法的时候会打印“replacer…”而不是“run…”。 4、工厂模式创建 Bean这个工厂模式和 FactoryBean 不一样，前者是实例化对象的方式，后者是 Spring 一类特殊的接口，代表一类 bean。Spring 里面的工厂模式分为静态工厂和实例工厂。 4.1、静态工厂1234567891011121314public class CarFactory &#123; // 静态方法 public static Car createCar(String color) &#123; if(color == \"red\")&#123; return new Car(\"red\"); &#125; if(color == \"white\")&#123; return new Car(\"white\"); &#125; else&#123; return new Car(\"black\"); &#125; &#125;&#125; 配置文件：123&lt;bean id=\"myCar\" class=\"com.huzb.demo.CarFactory\" factory-method=\"createCar\"&gt; &lt;constructor-arg value=\"red\" /&gt;&lt;/bean&gt; 4.2、实例工厂1234567891011121314public class CarFactory &#123; // 实例方法 public Car createCar(String color) &#123; if(color == \"red\")&#123; return new Car(\"red\"); &#125; if(color == \"white\")&#123; return new Car(\"white\"); &#125; else&#123; return new Car(\"black\"); &#125; &#125;&#125; 配置文件：12345&lt;bean id=\"carFactory\" class=\"com.huzb.demo.CarFactory\"/&gt;&lt;bean id=\"myCar\" factory-bean=\"carFactory\" factory-method=\"createCar\"&gt; &lt;constructor-arg value=\"red\" /&gt;&lt;/bean&gt; 另外注解方式在配置类中声明对象，也是通过实例工厂的方式实现的：1234567@Configurationpublic class DemoApplication &#123; @Bean public Car car()&#123; return new Car(); &#125;&#125; 参考资料Spring IOC 容器源码分析Spring IOC 容器源码分析 - 获取单例 beanSpring IOC 容器源码分析 - 获取单例 beanlookup-method和replaced-method使用","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring 源码浅析——容器刷新流程概览","slug":"Spring源码浅析——容器刷新流程概览","date":"2019-03-03T09:08:49.000Z","updated":"2019-03-22T13:24:16.755Z","comments":true,"path":"2019/03/03/Spring源码浅析——容器刷新流程概览/","link":"","permalink":"http://yoursite.com/2019/03/03/Spring源码浅析——容器刷新流程概览/","excerpt":"","text":"本文是 Spring 源码浅析系列的第一篇。Spring 版本是 Spring Boot 2.1.2.RELEASE （即 Spring 5.1.4），以默认配置启动，分析框架工作的原理。 众所周知，Spring 以容器管理所有的 bean 对象，容器的实体是一个 BeanFactory 对象。但我们常用的容器是另一个 ApplicationContext ，它在内部持有了 BeanFactory，所有和 BeanFactory 相关的操作都会委托给内部的 BeanFactory 来完成。 ApplicationContext 的继承关系如下图所示： ApplicationContext 是一个接口，ClassPathXmlApplicationContext和AnnotationConfigApplicationContext是两个比较常用的实现类，前者基于 xml 使用，后者基于注解使用。SpringBoot 中默认后面一种。 ApplicationContext 也继承了 BeanFactory 接口，BeanFactory 的继承关系如下图所示： 从继承关系我们可以获得以下信息： ApplicationContext 继承了 ListableBeanFactory，这个 Listable 的意思就是，通过这个接口，我们可以获取多个 Bean，最顶层 BeanFactory 接口的方法都是获取单个 Bean 的。 ApplicationContext 继承了 HierarchicalBeanFactory，Hierarchical 的意思是说我们可以在应用中起多个 BeanFactory，然后可以将各个 BeanFactory 设置为父子关系。 ApplicationContext 非常重要，所以我们第一篇就看一下 ApplicationContext 初始化的过程。默认配置下，SpringBoot 中的 ApplicationContext 初始化在 refresh() 方法中，为什么叫 refresh() 而不是 init() 呢？因为 ApplicationContext 建立起来以后，其实我们是可以通过调用 refresh() 这个方法重建的，这样会将原来的 ApplicationContext 销毁，然后再重新执行一次初始化操作。 1、容器刷新概览refresh 是个总览全局的方法，我们可以通过这个方法概览容器刷新的过程：// AbstractApplicationContext 511123456789101112131415161718192021222324252627282930313233343536373839404142434445@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、检验配置文件格式 prepareRefresh(); // ClassPathXmlApplicationContext 会在这里解析 xml 配置；AnnotationConfigApplicationContext 的解析发在初始化，这里只是简单的获取 // 这里的解析是指把配置信息都提取出来了，保存在了一个 Map&lt;String,BeanDefinition&gt; 中 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean 等 prepareBeanFactory(beanFactory); try &#123; // BeanFactory 准备工作完成后进行的后置处理工作，子类可以自定义实现，Spring Boot 中是个空方法 postProcessBeanFactory(beanFactory); //=======以上是 BeanFactory 的预准备工作======= // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类（注意和之前的 BeanFactoryPostProcessor 的区别） registerBeanPostProcessors(beanFactory); // 初始化 MessageSource 组件（做国际化功能；消息绑定，消息解析） initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器 initApplicationEventMulticaster(); // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前），Spring Boot 中默认没有定义 onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口 registerListeners(); // 初始化所有的 singleton beans（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 容器刷新完成操作 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); &#125; destroyBeans(); cancelRefresh(ex); throw ex; &#125; finally &#123; resetCommonCaches(); &#125; &#125;&#125; 通过上面的代码和注释我们总览了容器刷新的整个流程，下面我们来一步步探索每个环节都做了什么。 2、刷新前的准备工作这步比较简单，直接看代码中的注释即可。// AbstractApplicationContext 58012345678910111213141516protected void prepareRefresh() &#123; // 记录启动时间， // 将 active 属性设置为 true，closed 属性设置为 false，它们都是 AtomicBoolean 类型 this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); // Spring Boot 中是个空方法 initPropertySources(); // 校验配置属性的合法性 getEnvironment().validateRequiredProperties(); // 记录早期的事件 this.earlyApplicationEvents = new LinkedHashSet&lt;&gt;();&#125; 3、获取 Bean 容器前面说过 ApplicationContext 内部持有了一个 BeanFactory，这步就是获取 ApplicationContext 中的 BeanFactory。在 ClassPathXmlApplicationContext 中会做很多工作，因为一开始 ClassPathXmlApplicationContext 中的 BeanFactory 并没有创建，但在 AnnotationConfigApplicationContext 比较简单，直接返回即可。// AbstractApplicationContext 6211234567891011protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; // 通过 cas 设置刷新状态 if (!this.refreshed.compareAndSet(false, true)) &#123; throw new IllegalStateException( \"GenericApplicationContext does not support multiple refresh attempts: just call 'refresh' once\"); &#125; // 设置序列号 this.beanFactory.setSerializationId(getId()); // 返回已创建的 BeanFactory return this.beanFactory;&#125; 4、准备 Bean 容器BeanFactory 获取之后并不能马上使用，还要在 BeanFactory 中做一些准备工作，包括类加载器、表达式解析器的设置，几个特殊的 BeanPostProcessor 的添加等。// AbstractApplicationContext 63112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; // 设置 BeanFactory 的类加载器，这里设置为当前 ApplicationContext 的类加载器 beanFactory.setBeanClassLoader(getClassLoader()); // 设置表达式解析器 beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // 添加Aware后置处理器，实现了 Aware 接口的 beans 在初始化的时候，这个 processor 负责回调 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); /** * 下面几行的意思是，如果某个 bean 依赖于以下几个接口的实现类，在自动装配的时候忽略它们，Spring 会通过其他方式来处理这些依赖 */ beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); /** * 下面几行是为了解决特殊的依赖，如果有 bean 依赖了以下几个（可以发现都是跟容器相关的接口），会注入这边相应的值， * 这是因为 Spring 容器里面不保存容器本身，所以容器相关的依赖要到 resolvableDependencies 里面找。上文有提到过， * ApplicationContext 继承了 ResourceLoader、ApplicationEventPublisher、MessageSource，所以对于这几个依赖， * 可以赋值为 this，注意 this 是一个 ApplicationContext。 * 那这里怎么没看到为 MessageSource 赋值呢？那是因为 MessageSource 被注册成为了一个普通的 bean */ beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); /** * 这也是个 BeanPostProcessor ，在 bean 实例化后，如果是 ApplicationListener 的子类，那么将其添加到 listener 列表中， * 可以理解成：注册监听器 */ beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123; beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); &#125; /** * 从下面几行代码我们可以知道，Spring 往往很 \"智能\" 就是因为它会帮我们默认注册一些有用的 bean，我们也可以选择覆盖 */ if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); &#125; if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); &#125; if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); &#125;&#125; 5、调用 BeanFactory 后置处理器调用 BeanFactoryPostProcessor 的 postProcessBeanFactory(beanFactory) 方法，它允许在 beanFactory 准备完成之后对 beanFactory 进行一些修改，比如在 bean 初始化之前对 beanFactory 中的 beanDefinition 进行修改。篇幅有限就不展开了。 6、注册各类 Bean 后置处理器也是一个名字就体现功能的方法，把各种 BeanPostProcessor 注册到 BeanFactory 中，BeanPostProcessor 允许在 bean 初始化前后插手对 bean 的初始化过程，不展开了。 7、初始化事件分派器Event 会有单独的篇幅详解，这里就不展开了。 8、初始化所有非懒加载 singleton beans这是容器刷新中最重要的方法。Spring 需要在这个阶段完成所有的 singleton beans 的实例化。这一步骤非常重要而且过程非常长，下一篇中我们来专门分析这个方法。 9、容器刷新完成// AbstractApplicationContext 8711234567891011121314protected void finishRefresh() &#123; clearResourceCaches(); // 创建Lifecycle处理器 initLifecycleProcessor(); // 调用LifecycleProcessor的onRefresh()方法，默认是调用所有Lifecycle的start()方法 getLifecycleProcessor().onRefresh(); // 发布容器刷新完成事件 publishEvent(new ContextRefreshedEvent(this)); LiveBeansView.registerApplicationContext(this);&#125; 总结本篇文章简单梳理了一遍 Spring IOC 的核心代码 refresh() 的部分，着重跟进了容器准备的部分，比如 ApplicationContext 的预准备、BeanFactory 的获取、BeanFactory 的预准备等等。容器准备好了，接下来就要往容器中注入 bean 了，下一篇将分析 Spring 是如何创建并往容器中注入 bean。 参考资料Spring中bean后置处理器BeanPostProcessorSpring IOC 容器源码分析Spring IOC 容器源码分析系列文章导读","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Redis 复制、哨兵和集群","slug":"Redis高可用和分布式","date":"2019-02-23T08:20:24.000Z","updated":"2019-03-11T13:28:18.259Z","comments":true,"path":"2019/02/23/Redis高可用和分布式/","link":"","permalink":"http://yoursite.com/2019/02/23/Redis高可用和分布式/","excerpt":"","text":"Redis 可以单机部署，但会带来单点问题和性能瓶颈。为此 Redis 提供了主从复制、哨兵和集群的方式来解决这些问题。 一、主从复制主从复制可以将两台或者多台服务器之间的数据同步，这样在主服务器下线后，从服务器可以继续对外提供服务，保证了系统的高可用；另外主从复制也可以进行读写分离，主服务器只提供写操作或少量的读，把多余读请求通过负载均衡算法分流到单个或多个从服务器上。 Redis 的复制功能分为同步和命令传播两个操作，同步操作用于将从服务器的状态更新至主服务器当前的状态；命令传播操作用于在主服务状态被修改时，让主从状态重新保持一致。 同步可以通过使用SLAVEOF &lt;host&gt; &lt;port&gt;命令来让一个服务器成为另一个服务器的从服务器。此时从服务器会自动向主服务器发送 SYNC 的命令进行同步操作，步骤如下： 1）主服务器收到 SYNC 命令后执行 BGSAVE 命令，在后台生成一个 RDB 快照文件，并用一个缓冲区记录从现在开始执行的所有写命令。 2）从服务器从主服务器接收到 RDB 文件后，丢弃所有旧数据，载入主服务器发来的快照文件。 3）主服务器把记录在缓冲区的命令发送给从服务器，从服务器执行这些命令，同步完成。 但是这种同步方式会带来一个断线后重复值效率低的问题：如果从服务器短暂掉线后重连主服务器，主从之间不得不重新同步所有数据，这是没有必要的，因为从服务器仍然保留了大部分数据。Redis 2.8 推出了部分重同步的方式解决了这个问题。 部分重同步部分重同步用于处理断线后复制的情况。Redis 部分重同步的实现由以下三个部分构成： 主服务器和从服务器的复制偏移量：复制偏移量表示当前从服务器从主服务器中接收了多少字节的数据。 主服务器的复制积压缓冲区：复制积压缓冲区以 FIFO 的形式保存了最近的写命令。 服务器运行 id：用于标识服务器 当从服务器断线重连之后，会向主服务器发送一个PSYNC &lt;runid&gt; &lt;offset&gt;的命令，offset 即当前从服务器的复制偏移量，runid 是从服务器记录的掉线之前的主服务器 id。主服务器在收到 PSYNC 后，会首先对比自己的 runid 和传过来的 runid 是否一致，如果一致，说明这台从服务器之前和自己同步过数据，然后根据 offset 的差值把复制积压缓冲区的数据同步给从服务器。这样就实现了增量更新。 命令传播在同步完成之后，主从服务器就进入了命令传播阶段。在这个阶段，主服务器发生写操作后，会把相应的命令发送给从服务器执行，这样主从之间就保持了数据一致性。 从服务器也会向主服务器发送命令REPLCONF ACK &lt;replication_offset&gt;，这是一个心跳检测命令，每隔 1 秒就会发一次，它有以下两个作用： 检测主从服务器的网络连接状态：主服务器会记录从服务器上次心跳检测的时间，如果超过 1 秒，就说明连接出了故障。如果主服务器和大量从服务器之间的连接出了故障，比如有 3 台以上的从服务器超过 10 秒没有心跳检测，则会拒绝执行写命令。 检测命令丢失：和部分重同步类似，心跳检测也会发送从服务器的当前复制偏移量（replication_offset），主服务器在接收到心跳检测后会检查这个偏移量是否和自己的一致，如果不一致会补发缺失的数据。 二、Sentinel（哨兵）Sentinel（哨兵）是 Redis 高可用的解决方案：由一个或多个 Sentinel 实例组成的 Sentinel 系统可以监视任意多个主服务器以及这些服务器下的所有从服务器。在主服务器进入下线状态时，自动将下线服务器下的某个从服务器升级成主服务器。 Sentinel 系统是由 Sentinel 实例组成的，彼此间通过命令连接相互通信： Sentinel 实例会以每秒一次的频率向自己监控的主服务器发送 PING 命令，主服务器会回复该命令，以此来监控主服务器的在线状态。当 Sentinel 实例发送 PING 命令之后等待超过配置指定时间之后，Sentinel 实例会判定该主服务器处于主观下线状态。 当一个 Sentinel 实例判定自己监控的某个主服务器为主观下线之后，它会询问其它监控该服务器的 Sentinel 实例，当超过配置指定数量的 Sentinel 实例也认为该服务器已下线时，Sentinel 实例会判定该服务器为客观下线。 当有超过配置指定数量的 Sentinel 实例认为该服务器客观下线之后，监视该服务器的各个 Sentinel 会进行协商，通过 Raft 算法选举出一个领头 Sentinel 对下线主服务器执行故障转移操作。 故障转移操作包括三个部分： 1）在已下线主服务器属下的所有从服务器中挑选一个从服务器，将其转换为主服务器。 2）让其它从服务器改为复制新的主服务器（发送 SLAVEOF 命令）。 3）将原来的主服务器设置为从服务器。 三、集群Redis 集群是 Redis 提供的分布式数据库方案，集群通过分片的方式进行数据共享，并提供复制和转移功能。 集群中的节点可以通过向其它节点发送CLUSTER MEET &lt;ip&gt; &lt;port&gt;命令邀请其它节点加入集群。 集群的整个数据库被分为 16384 个槽，数据库中的每一个键都属于这 16384 个槽中的一个，只有当集群中的每个槽都有节点在处理时，集群才处于上线状态。 我们可以通过CLUSTER ADDSLOTS [slot...]命令将一个或多个槽指派给节点。每个节点会记录自己和集群中其它节点被指派的槽。 节点在接到一个命令请求时，会先检查这个命令请求要处理的键所在的槽是否由自己负责，如果不是的话，节点将向客户端返回一个 MOVED 错误，MOVED 错误携带的信息可以指引客户端转向正确的节点。 除了可以指派槽以外，我们还可以通过 redis-trib 这个软件将槽重新分片。重新分片的关键是将属于某个槽的所有键值对从一个节点转移至另一个节点。 在重新分片期间，如果客户端向原来的节点请求键 k，而 k 已经被转移到另一个节点时，节点会返回一个 ASK 错误，指引客户端到新的节点。 MOVED 错误表示槽的负责权已经永远转移了，而 ASK 错误只是两个节点在槽迁移时的临时措施。 故障转移为了集群的高可用，集群中的节点也分主节点和从节点，从节点复制主节点的数据。 主节点之间通过 PING 消息来互相确认对方的在线状态，如果没有在规定时间内收到对方的 PONG，则会把对方标记为疑似下线状态，并将这个消息告知给集群中的其它主节点。当某个主节点发现集群中有超过半数主节点认为某个主节点疑似下线，它就会把这个主节点标记为下线，并广播给集群中其它的节点（主节点+从节点）。 当从节点发现自己正在复制的主节点已下线时，会通过 raft 算法选举出新的主节点。选举的候选节点是已下线主节点的所有从节点，投票节点是其它所有主节点。最后一个从节点会被推选为新的主节点，接管由已下线节点负责处理的槽。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"CMS,G1 和 ZGC","slug":"CMS-G1和ZGC","date":"2019-02-21T11:23:54.000Z","updated":"2019-03-08T14:03:47.398Z","comments":true,"path":"2019/02/21/CMS-G1和ZGC/","link":"","permalink":"http://yoursite.com/2019/02/21/CMS-G1和ZGC/","excerpt":"","text":"本文主要介绍比较常用的垃圾收集器：CMS，G1 和 ZGC。CMS 是服务器使用比较多的收集器，侧重点在低停顿；JDK 9 以后，G1 成为了默认收集器，它的设计目标是停顿可控；最新的 JDK 11 中，加入了实验性质的 ZGC，这个收集器可以将停顿时间降至 10ms 以下。 1、JDK1.8 之前默认的垃圾收集器JDK1.8 之前（包括 1.8），默认的垃圾收集器是 Parallel Scavenge（新生代）+Parallel Old（老年代）。 Parallel Scavenge 是一款基于复制算法的新生代垃圾处理器，它的设计目标是吞吐量优先。所谓吞吐量就是指：用户运行时间/(用户运行时间+垃圾回收时间)。很显然吞吐量越大越好。用户需要给 Parallel Scavenge 设置一个吞吐量的目标，然后 Parallel Scavenge 会自动控制每一次垃圾回收的时间。另外 Parallel Scavenge 还有自适应调节策略，只要打开-XX:+UseAdaptiveSizePolicy，它就会动态调整新生代大小、Eden 与 Survivor 的比例、晋升老年代对象大小等参数，以达到最大的吞吐量。 和 Parallel Scavenge 配套使用的老年代收集器是 Parallel Old，这是一款基于标记-整理算法的垃圾处理器。 为什么不用 CMS 作为老年代收集器？ 这是因为 Parallel Scavenge 的作者没有使用 HotSpot VM 给定的代码框架，而是自己独立实现了一个。这就导致 Parallel Scavenge 和当时大部分收集器都不兼容，其中就包括 CMS。所以在 1.8 时代，比较流行的有两套收集器，一套是 Parallel Scavenge（新生代）+Parallel Old（老年代），另一套是 ParNew（使用复制算法，新生代）+ CMS（老年代） 2、低停顿的 CMSCMS，Concurrent Mark Sweep，是一款老年代的收集器，它关注的是垃圾回收最短的停顿时间。命名中 Concurrent 说明这个收集器是有与工作执行并发的能力的，Mark Sweep 则代表算法用的是标记-清除算法。 CMS 的工作原理分为四步： 初始标记：单线程执行，仅仅把 GC Roots 的直接关联可达的对象标记一下，速度很快，需要停顿。 并发标记：对于初始标记过程所标记的初始对象，进行并发追踪标记，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：清除之前标记的垃圾，不需要停顿。 由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 由于 CMS 以上特性，缺点也是比较明显的： 标记-清除算法会导致内存碎片比较多。 CMS 的并发能力依赖于 CPU 资源，所以在 CPU 数少和 CPU 资源紧张的情况下，性能较差。 无法处理浮动垃圾。浮动垃圾是指在并发清除阶段用户线程继续运行而产生的垃圾，这部分垃圾只能等下次 GC 时处理。由于浮动垃圾的存在，CMS 不能等待内存耗尽的时候才进行 GC，而要预留一部分内存空间给用户线程。这里会浪费一些空间。 为什么不用标记-整理？ 因为在并发清除阶段，其它用户线程还在工作，要保证它们运行的资源不受影响。而标记-整理算法会移动对象，所以不能使用标记-整理。 3、停顿可控的 G1G1 是一款可以掌管所有堆内存空间的收集器。G1 把堆划分成多个大小相等的独立区域（Region），新生代和老年代不再物理隔离。 通过引入 Region 的概念，将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法避免了空间碎片化，也提高了回收的灵活性——G1 会根据平均每个 Region 回收需要的时间（经验预测）和各个 Region 的回收收益，制定回收计划。 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 三种 GC 模式： Young GC，发生于新生代空间不足时，回收全部新生代，可以通过控制新生代 Region 的个数来控制 Young GC 的时间开销。 Mixed GC，当堆中内存使用超过整个堆大小的 InitiatingHeapOccupancyPercent（默认 45）时启动。 回收全部新生代，并根据预期停顿时间回收部分收益较高的老年代。 Full GC（JDK 9 引入），发生于老年代空间不足时，相当于执行一次 STW 的 full gc。 整体的执行流程： 初始标记：标记了从 GC Root 开始直接关联可达的对象，速度很快，单线程，需停顿。 并发标记：对于初始标记过程所标记的初始对象，进行并发追踪标记，并记录下每个 Region 中的存活对象信息用于计算收益，不需要停顿。 最终标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 筛选回收：根据 GC 模式、回收时间和回收收益确定回收计划，回收后的空 Region 会加入到空闲列表，需要停顿。 由于 G1 会把存活的对象集中起来放到 Survivor Region 中，并通过空闲列表整理所有的空 Region，所以整体来看是基于“标记 - 整理”算法实现的收集器；但从局部（两个 Region 之间）上来看又是基于“复制”算法实现的。但不论如何，这两种算法都需要移动对象，所以 G1 的回收阶段是需要停顿的。 4、几乎无停顿的 ZGC在 JDK 11 当中，加入了实验性质的 ZGC。它的回收耗时平均不到 2 毫秒。它是一款低停顿高并发的收集器。ZGC 几乎在所有地方并发执行的，除了初始标记的是 STW 的。所以停顿时间几乎就耗费在初始标记上，这部分的实际是非常少的。那么其他阶段是怎么做到可以并发执行的呢？ZGC 主要新增了两项技术，一个是着色指针 Colored Pointer，另一个是读屏障 Load Barrier。 着色指针 Colored Pointer ZGC 利用指针的 64 位中的几位表示 Finalizable、Remapped、Marked1、Marked0（ZGC 仅支持 64 位平台），以标记该指向内存的存储状态。相当于在对象的指针上标注了对象的信息。注意，这里的指针相当于 Java 术语当中的引用。 在这个被指向的内存发生变化的时候（内存在整理时被移动），颜色就会发生变化。 读屏障 Load Barrier 由于着色指针的存在，在程序运行时访问对象的时候，可以轻易知道对象在内存的存储状态（通过指针访问对象），若请求读的内存在被着色了。那么则会触发读屏障。读屏障会更新指针再返回结果，此过程有一定的耗费，从而达到与用户线程并发的效果。 把这两项技术联合下理解，引用 R 大（RednaxelaFX）的话 与标记对象的传统算法相比，ZGC 在指针上做标记，在访问指针时加入 Load Barrier（读屏障），比如当对象正被 GC 移动，指针上的颜色就会不对，这个屏障就会先把指针更新为有效地址再返回，也就是，永远只有单个对象读取时有概率被减速，而不存在为了保持应用与 GC 一致而粗暴整体的 Stop The World。 ZGC 和 G1 一样将堆划分为 Region 来清理、移动，稍有不同的是 ZGC 中 Region 的大小是会动态变化的。 ZGC 的回收流程如下： 1、初始停顿标记 停顿 JVM 地标记 Root 对象，1，2，4 三个被标为 live。 2、并发标记 并发地递归标记其他对象，5 和 8 也被标记为 live。 3、移动对象 对比发现 3、6、7 是过期对象，也就是中间的两个灰色 region 需要被压缩清理，所以陆续将 4、5、8 对象移动到最右边的新 Region。移动过程中，有个 forward table 纪录这种转向。 活的对象都移走之后，这个 region 可以立即释放掉，并且用来当作下一个要扫描的 region 的 to region。所以理论上要收集整个堆，只需要有一个空 region 就 OK 了。 4、修正指针 最后将指针都妥帖地更新指向新地址。 ZGC 虽然目前还在 JDK 11 还在实验阶段，但由于算法与思想是一个非常大的提升，相信在未来不久会成为主流的 GC 收集器使用。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"Redis 事件、事务和 pipeline","slug":"Redis事件、事务和pipeline","date":"2019-02-12T11:34:57.000Z","updated":"2019-03-08T14:14:14.002Z","comments":true,"path":"2019/02/12/Redis事件、事务和pipeline/","link":"","permalink":"http://yoursite.com/2019/02/12/Redis事件、事务和pipeline/","excerpt":"","text":"本文介绍了 Redis 中事件的类型和事件的调度与执行，以及对批量事件处理的两种方式：事务和 pipeline。 一、事件Redis 服务器是一个事件驱动程序。Redis 的事件有两类： 文件事件：服务器通过套接字与客户端连接，文件事件就是服务器对套接字操作的抽象。 时间事件：服务器对定时操作的抽象。 文件事件Redis 包装了底层的 select、epoll 等来实现自己的网络事件处理器。它使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。 时间事件服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。 时间事件又分为： 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 时间事件中的属性 when 会记录下次执行的时间，周期性事件在执行后会更新 when 的值，而定时事件会被删除。 Redis 将所有时间事件都放在一个无序链表中，由时间事件执行器通过遍历整个链表查找出已到达的时间事件，并调用相应的事件处理器。 事件的调度与执行服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。 事件调度与执行由 aeProcessEvents 函数负责，伪代码如下：12345678910111213141516def aeProcessEvents(): # 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() # 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms &lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) # 处理所有已产生的文件事件 procesFileEvents() # 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下： 12345678def main(): # 初始化服务器 init_server() # 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() # 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下： 二、事务Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 一个事务包括三个步骤： 事务开始：事务以 MULTI 开始，返回 OK 命令。 命令入队：每个事务命令成功进入队列后，返回 QUEUED。 事务执行：EXEC 执行事务。 Redis 不支持事务回滚功能，事务中的一个 Redis 命令执行失败以后，会继续执行后续的命令。 三、pipeline多个命令被一次性发送给服务器，而不是一条一条发送，这种方式被称为流水线，它可以减少客户端与服务器之间的网络通信次数从而提升性能。 可以通过redis-cli --pipe的方式批量发送命令。如cat commands.txt | redis-cli --pipe，commands.txt 中的命令会被以 RESP 协议（这是一个 Redis 自行规定的协议，用于命令的批量执行）的格式发给服务器，服务器也会返回一个 RESP 格式的结果。 当然我们不用自己去实现这个协议，Jedis 为我们实现好了，我们可以很方便地调用：1234567891011Jedis jedis = new Jedis(\"localhost\", 6379);//使用 pipelinePipeline pipeline = jedis.pipelined();//删除 listspipeline.del(\"lists\");//循环添加 10000 个元素for(int i = 0; i &lt; 10000; i++)&#123; pipeline.rpush(\"lists\", i + \"\");&#125;//执行pipeline.sync();","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"Redis 持久化和过期机制","slug":"Redis持久化和过期机制","date":"2019-02-08T06:12:30.000Z","updated":"2019-04-11T13:10:16.247Z","comments":true,"path":"2019/02/08/Redis持久化和过期机制/","link":"","permalink":"http://yoursite.com/2019/02/08/Redis持久化和过期机制/","excerpt":"","text":"本文主要介绍了 Redis 持久化的两种机制：RDB 和 AOF，以及键过期的策略：惰性删除和定期删除，还有 RDB、AOF 和复制功能对过期键的处理。 RDBRDB 是 Redis 持久化的第一种方式。有两个 Redis 命令可以用于生成 RDB 文件，一个是 SAVE，另一个是 BGSAVE。SAVE 会阻塞 Redis 服务器进程，执行时 Redis 服务器会阻塞所有客户端发送的命令。12redis&gt; SAVEOK BGSAVE 会派生出一个子进程执行，执行时仍可继续处理客户端的命令，但会拒绝客户端 SAVE 和 BGSAVE 的命令，延迟 BGREWRITEAOF 命令。12redis&gt; BGSAVEBackground saving started 执行条件SAVE 命令会阻塞服务器，所以只能手动执行。BGSAVE 可以在不阻塞的情况下执行，所以可以配置 save 选项让服务器每隔一段时间自动执行一次。 比如我们可以向服务器提供以下配置：123save 900 1save 300 10save 60 10000 那么只要满足以下三个条件中的任意一个即可被执行： 服务器在 900 秒之内对数据库进行了至少 1 次修改。 服务器在 300 秒之内对数据库进行了至少 10 次修改。 服务器在 60 秒之内对数据库进行了至少 10000 次修改。 为了实现这一功能，服务器会维持一个记录距离上次保存之后修改的次数的 dirty 计数器和一个记录上次保存时间的 lastsave 属性。 周期操作函数 serverCron 默认每个 100 毫秒就会执行一次，它的其中一项工作就是检查 save 选项设置的条件是否满足，如果满足的话就会执行 BGSAVE 命令。 文件内容RDB 文件有多个部分，包括握手字段 ‘REDIS’ 字符串，版本号，数据库，’EOF’ 和校验字段。 核心部分是数据库字段，数据库字段包括了握手字段 ‘SELECTDB’，数据库编号和键值对，数据库编号指示了这是第几个数据库，而键值对则保存了各项数据。 键值对中除了类型和数据，还可能会有过期时间。对于不同类型的键值对，RDB 文件会用不同的方式来保存它们。 RDB 文件本身是一个经过压缩的二进制文件，每次 SAVE 或者 BGSAVE 都会创建一个新的 RDB 文件，不支持追加操作。 AOFAOF 是 Redis 持久化的第二种方式，在 AOF 和 RDB 同时开启时，服务器会优先考虑从 AOF 恢复数据，因为 AOF 每次记录间隔的时间更短。 和 RDB 直接记录键值对不同，AOF 记录的是命令。服务器在执行完一个写命令以后，会把这条命令追加到服务器 aof_buf 缓冲区的末尾，并在一个适当的时候写入文件。重建时服务器会创建一个伪客户端，依次执行文件中的命令即可完成数据的载入。 文件的写入与同步AOF 的持久化发生在每次事件循环结束之前，会阻塞服务器。在持久化时会调用操作系统的 write 函数，但通常该函数会把数据保存在一个内存缓冲区里面而不是立刻刷入磁盘。这就带来一个安全问题。为了避免这个问题操作系统又提供了 fsync 和 fdatasync 两个强制刷盘的同步函数。我们把 write 称为写入，把 fsync 和 fdatasync 称为同步。 服务器会在每次事件循环结束之前根据 appendfsync 选项写入和同步 aof_buf 中的数据： always：写入并同步 everysec：写入，如果距离上次同步超过 1 秒，则同步 no：只写入，何时同步由操作系统决定 AOF 重写随着服务器运行时间的流逝，AOF 文件中的内容会越来越多，文件的体积也会越来越大，不仅会对宿主计算机造成影响，也拖慢了数据恢复所需要的时间。 AOF 重写是指重新生成一个 AOF 文件替换原来的 AOF 文件。但这里的重写不会对原有的文件进行读取、分析或者写入，而是把数据库中的键值对折算成命令，重新写入新的文件。 重写是一个耗时的操作，因此 Redis 把它放到后台去操作，对应的指令是 BGREWRITEAOF。在重写过程中服务器还可能接收新的指令，因此 Redis 会维护一个 AOF 重写缓冲区，记录重写期间的写命令，在重写完成后追加到 AOF 文件末尾。 RDB 和 AOF 对比RDB 的优点： RDB 是一个非常紧凑的文件，它的体积更小，且可以选择持久化的时间，适合做备份的文件。比如每天的备份，每月的备份。 RDB 对主进程更友好，父进程只需要 fork 出一个子进程，无须执行任何磁盘 I/O 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 RDB 的缺点： 因为 RDB 文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少 5 分钟才保存一次 RDB 文件，间隔时间比较长。 RDB 虽然会把持久化的操作交给子进程，但每次都会从头开始，在数据集比较庞大时， fork() 可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 AOF 的优点： AOF 使用追加的方式，每次写入时间很短，因此可以允许更短间隔的持久化操作，比如 1 秒。 AOF 文件的可读性比较好，如果你不小心执行了一条命令，只要 AOF 文件未被重写，那么只要停止服务器，移除 AOF 文件里的该条命令然后重启 Redis 即可。 AOF 的缺点： 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 使用 fsync 会降低 Redis 的性能，导致 AOF 的速度可能会慢于 RDB 。 RDB 和 AOF 各有所长，RDB 体积小，恢复速度快，而且可以生成快照；AOF 频率更高，可以保存更新的数据。一般来说，推荐同时使用。 Redis 过期机制Redis 采取的是惰性删除和定期删除配合使用的方式。 惰性删除是指 Redis 会在访问某个键的时候检查该键是否过期，如果过期，就会将输入键从数据库中删除。但惰性删除不能及时清理内存，因此 Redis 还有定期删除的机制。 定期删除是另一种过期键删除方式。Redis 会维护一个过期字典（如下图所示），所有声明了过期时间的键都会被添加进这个字典中。周期操作函数 serverCron 执行时，会在规定时间内随机检查一部分键的过期时间，并删除其中的过期键。 RDB、AOF 和复制功能对过期键的处理RDB 对过期键的处理RDB 文件在生成时会检查每个键的过期时间，过期键不会被添加进 RDB 文件里。 载入 RDB 文件时，如果该服务器是主服务器，则不会载入文件中过期的键；如果该服务器是从服务器，则不论过期与否都会被载入。不过，因为主从服务器在同步的时候，从服务器的数据库会被清空，所以一般来讲，过期键对载入 RDB 文件的从服务器不会造成影响。 AOF 对过期键的处理AOF 文件写入时，如果数据库中的某个键已过期，但它还没被删除，那么 AOF 文件不会因为这个键产生任何影响。当它被惰性删除或者定期删除之后，程序会向 AOF 文件追加一条 DEL 命令显示记录该键已被删除。 AOF 重写时，和生成 RDB 文件一样，会过滤掉已经过期的键。 复制功能对过期键的处理主服务器在删除一个过期键后，会显式地向所有从服务器发送一个 DEL 命令，告知从服务器删除这个过期键。 Redis 3.2 前，为了保持主从一致性，从服务器在执行客户端发送的读命令时，即使碰到过期键也不会将过期键删除，而是继续像处理未过期键一样处理过期键。从服务器只有在接到主服务器发来的 DEL 命令之后，才会删除过期键。Redis 3.2 后，从节点在读取数据时，增加了对数据是否过期的判断：如果该数据已过期，则不返回给客户端。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"Redis 数据结构和对象","slug":"Redis数据结构和对象","date":"2019-02-05T05:45:44.000Z","updated":"2019-03-08T14:14:49.420Z","comments":true,"path":"2019/02/05/Redis数据结构和对象/","link":"","permalink":"http://yoursite.com/2019/02/05/Redis数据结构和对象/","excerpt":"","text":"本文主要介绍了 Redis 的 6 种数据结构：SDS、链表、字典、跳跃表、整数集合、压缩列表，和 5 种对象：字符串、列表、哈希、集合、有序集合。 一、数据结构SDSSDS 是对 C 字符串的封装，用于表示字符串 SDS 使用预分配的方式为字符串分配空间，free 字段表示当前未使用的空间，当字符串增长时，会优先使用未使用的空间，如果未使用的空间不足，Redis 会为 SDS 分配额外的空间。分配算法具体为：1）如果增长后的字符串长度小于 1MB，Redis 将额外分配等同于增长后的字符串长度的空间，此时 free 和 len 的大小相等；2）如果增长后的字符串长度大于 1MB，Redis 将额外分配 1MB 大小的空间。 分配后的空间不会被回收，如果字符串缩短，缩短的空间会被加入到 free 空间中。 链表 Redis 的链表是一个双向链表，它的特性如下： 双端：链表节点带有 prev 和 next 指针，获取某个节点的前置和后置节点的复杂度都是 O(1)。 无环：表头节点的 prev 和表尾节点的 next 指针都指向 NULL。 带表头指针和表尾指针：通过 head 和 tail 指针获取表头和表尾节点的复杂度为 O(1)。 计数器：获取节点个数复杂度为 O(1) 多态：利用 dup（复制节点保存的值）、free(释放节点保存的值) 和 match（比较节点的值和输入的值是否相等），来实现保存不同的值。 字典字典是一个散列表结构，使用 MurmurHash2 算法计算哈希值，使用拉链法保存哈希冲突。 Redis 的字典 dict 中包含两个哈希表 dictht，这是为了方便进行 rehash 操作。在扩容时，将其中一个 dictht 上的键值对 rehash 到另一个 dictht 上面，完成之后释放空间并交换两个 dictht 的角色。 渐进式 rehash 通过记录 dict 的 rehashidx 完成，它从 0 开始，然后每执行一次 rehash 都会递增。例如在一次 rehash 中，要把 dict[0] rehash 到 dict[1]，这一次会把 dict[0] 上 table[rehashidx] 的键值对 rehash 到 dict[1] 上，dict[0] 的 table[rehashidx] 指向 null，并令 rehashidx++。 在 rehash 期间，每次对字典执行添加、删除、查找或者更新操作时，都会执行一次渐进式 rehash。 采用渐进式 rehash 会导致字典中的数据分散在两个 dictht 上，因此对字典的查找操作也需要到对应的 dictht 去执行。 跳跃表跳跃表是有序集合的底层实现之一。 跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。每个跳跃表节点的层高都是 1-32 之间的随机数。 跳跃表中的节点按照分数大小排列，当分值相同时，节点按照成员对象的大小进行排序。 在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找。下图演示了查找 22 的过程。 与红黑树等平衡树相比，跳跃表具有以下优点： 插入速度非常快速，因为不需要进行旋转等操作来维护平衡性； 并发插入时只需锁住少数节点 支持范围查找 更容易实现 跳跃表的缺点： 重复存储分层节点，消耗内存 整数集合整数集合是集合键的底层实现之一，它的底层是一个数组，这个数组以有序、无重复的方式保存集合元素。元素的类型可以为 int16_t,int32_t 或者 int64_t。程序会根据新添加元素的类型，改变数组中元素的类型。 encoding 表示元素的类型，数组中所有元素的类型是一样的。当有更大的数加入进来的时候，数组会进行升级操作，比如从 int16_t 升级到 int32_t。 整数集合只能升级，不能降级。 压缩列表压缩列表被用作列表和哈希的底层实现之一，是一种为节约内存而开发的，由任意多个节点组成的顺序数据结构。每个节点可以保存一个字节数组或一个整数值。 entry1、entry2…是实际存储数据的节点，除此之外还有些字段记录列表的信息：zlbytes 记录整个压缩列表占用的内存字节数，zltail 指向压缩列表表尾节点，zllen 记录包含的节点数量，zlend 是个特殊值字段（0xFF）用于标记末端。 每个节点由 previous_entry_length、encoding、content 三部分组成。previous_entry_length 保存了前一个节点的长度，由此可以实现从表尾向表头的遍历。encoding 是个复用字段，记录了 content 的编码和长度。下图的 encoding 最高两位 00 表示节点保存的是一个字节数组，后六位 001011 记录了字节数组长度 11。content 保存着节点的值”hello world”。 二、对象Redis 并没有使用以上的数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统。 对象类型 底层实现 可以存储的值 操作 STRING int，sds 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 LIST 链表，压缩列表 列表 从两端压入或者弹出元素 对单个或者多个元素 进行修剪，只保留一个范围内的元素 HASH 字典，压缩列表 包含键值对的无序散列表 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 SET 整数集合，字典 无序集合 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 ZSET （字典+跳跃表），压缩列表 有序集合 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 SET 对象使用整数集合保存只包含整数的集合，使用字典保存含有字符串的集合。使用字典保存集合时，字典的键是一个元素的成员，字典的值为 NULL。 ZSET 对象同时使用字典和跳跃表保存有序集合，使用字典保存有序集合时，字典的键保存了元素的成员，字典的值保存了元素的分值。ZSET 集合元素会同时共享在字典和跳跃表中（保存的是指针，不会造成数据的重复） 为什么有序集合需要同时使用字典和跳跃表来实现？ 在理论上，有序集合可以单独使用字典或者跳跃表来实现。但两者都有不可替代的地方：字典可以在 O(1) 时间复杂度查找成员的分值，跳跃表可以执行范围型操作。因此为了同时获得这两个特性，Redis 使用了字典和跳跃表来实现有序集合。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"HTTP2.0 和 QUIC","slug":"HTTP2-0和QUIC","date":"2019-02-01T05:23:15.000Z","updated":"2019-03-09T07:40:24.255Z","comments":true,"path":"2019/02/01/HTTP2-0和QUIC/","link":"","permalink":"http://yoursite.com/2019/02/01/HTTP2-0和QUIC/","excerpt":"","text":"HTTP1.1 在应用层以纯文本的形式进行通信。每次通信都要带完整的 HTTP 的头，而且不考虑 pipeline 模式的话，每次都是一去一回。这样在实时性、并发性上都存在问题。http2.0 通过首部压缩、多路复用、二进制分帧、服务端推送等方式获得了更高的并发和更低的延迟。 首部压缩HTTP 2.0 将原来每次都要携带的大量请求头中的 key value 保存在服务器和客户端两端，对相同的头只发送索引表中的索引。 如果首部发生变化了，那么只需要发送变化了数据在 Headers 帧里面，新增或修改的首部帧会被追加到“首部表”。首部表在 HTTP 2.0 的连接存续期内始终存在,由客户端和服务器共同渐进地更新 。 多路复用原先的 http 会为每一个请求建立一个 tcp 连接。但由于客户端对单个域名的允许的最大连接数有限，以及三次握手和慢启动等问题，导致效率很低。pipeline 模式是一个比较好的解决办法，但同样会带来队头阻塞问题：同时发出的请求必须按顺序接收，如果第一个请求被阻塞了，则后面的请求即使处理完毕了，也需要等待。 http2.0 的多路复用完美解决了这个问题。一个 request 对应一个 stream 并分配一个 id，这样一个连接上可以有多个 stream，每个 stream 的 frame 可以随机的混杂在一起，接收方可以根据 stream id 将 frame 再归属到各自不同的 request 里面。 http2.0 还可以为每个 stream 设置优先级（Priority）和依赖（Dependency）。优先级高的 stream 会被 server 优先处理和返回给客户端，stream 还可以依赖其它的 sub streams。优先级和依赖都是可以动态调整的。动态调整在有些场景下很有用，假想用户在用你的 app 浏览商品的时候，快速的滑动到了商品列表的底部，但前面的请求先发出，如果不把后面的请求优先级设高，用户当前浏览的图片要到最后才能下载完成，而如果设置了优先级，则可以先加载后面的商品，体验会好很多。 二进制分帧HTTP 1.x 在应用层以纯文本的形式进行通信，而 HTTP 2.0 将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。二进制格式的好处在于解析更快，而且文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认 0 和 1 的组合，因此健壮性更好。 服务端推送服务端可以在发送页面 HTML 时主动推送其它资源，而不用等到浏览器解析到相应位置，发起请求再响应。例如服务端可以主动把 JS 和 CSS 文件推送给客户端，而不需要客户端解析 HTML 时再发送这些请求。 服务端可以主动推送，客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送 RST_STREAM 帧来拒收。主动推送也遵守同源策略，服务器不会随便推送第三方资源给客户端。 QUICHTTP2.0 虽然大大增加了并发性，但还是有问题的。因为 HTTP2.0 也是基于 TCP 协议的，TCP 协议在处理包时是有严格顺序的。 当其中一个数据包遇到问题，TCP 连接需要等待这个包完成重传之后才能继续进行。虽然 HTTP2.0 通过多个 stream，使得逻辑上一个 TCP 连接上的并行内容，进行多路数据的传输，然而这中间没有关联的数据。一前一后，前面 stream2 的帧没有收到，后面 stream1 的帧也会因此阻塞。 于是，就有了从 TCP 切换到 UDP 的时候。这就是 Google 的 QUIC 协议。 机制一：自定义连接机制我们都知道，一条 TCP 连接是由四元组标识的，分别是源 IP、源端口、目的 IP、目的端口。一旦一个元素发生变化时，就需要断开重连，重新连接。在移动互联的情况下，当手机信号不稳定或者在 WIFI 和移动网络切换时，都会导致重连，从而进行再次的三次握手，导致一定的时延。 QUIC 使用一个 64 位的随机数作为 ID 来标识，由于 UDP 是无连接的，所以当 IP 或者端口发生变化时，只要 ID 不变，就不需要重新建立连接。 机制二：自定义重传机制TCP 为了保证可靠性，通过使用序号和应答机制，来解决顺序问题和丢包问题。 任何一个序号的包发过去，都要在一定的时间内得到应答，否则一旦超时，就会重发。超时时间是通过采样往返时间 RTT 不断调整的。这就会带来一个采样不准确的问题。例如：发送一个包，序号为 100，发现没有返回，于是再发送一个 100，过一阵返回一个 ACK101。这个时候客户端知道这个包肯定收到了，但往返时间是多少呢？是 ACK 达到的时间减去后一个 100 发送的时间，还是减去前一个 100 发送的时间呢？ QUIC 也有个序列号，是递增的。任何一个序列号的包只发送一次，下次就要加一了。例如，发送一个包，序号是 100，发现没有返回；再次发送的时候，序号就是 101 了；如果返回的 ACK100，就是对第一个包的响应，如果返回 ACK101 就是对第二个包的响应，RTT 计算相对准确。 但是这里有一个问题，就是这么知道包 100 和包 101 发送的是同样的内容呢？QUIC 定义了一个 stream offset 的概念。QUIC 既然面向连接，也就像 TCP 一样，是一个数据流，发送的数据在这个数据流里面有个偏移量 stream offset，可以通过 stream offset 查看数据发送到了哪里，这样只要这个 stream offset 的包没有来，就要重发；如果来了，按照 stream offset 拼接，还是能够拼成一个流。 无阻塞的多路复用有了自定义的连接和重传机制，我们可以解决上面 HTTP2.0 的多路复用问题。 同 HTTP2.0 一样，同一条 QUIC 连接上可以创建多个 stream，来发送多个 HTTP 请求。但是，QUIC 是基于 UDP 的，一个连接上的多个 stream 之间没有依赖。这样，假如 stream2 丢了一个 UDP 包，后面跟着 stream3 的一个 UDP 包，虽然 stream2 的那个包需要重传，但是 stream3 的包无需等待，就可以发给用户。 自定义流量控制TCP 的流量控制是通过滑动窗口协议。QUIC 的流量控制也是通过 window_update，来告诉对端它可以接受的字节数。但是 QUIC 的窗口是适应自己的多路复用机制的，不但在一个连接上控制窗口，还在一个连接中的每个 stream 控制窗口。 在 TCP 协议中，接受端的窗口的起始点是下一个要接收并且 ACK 的包，即使后来的包都到了，放在缓存里，窗口也不能右移，因为 TCP 的 ACK 机制是基于系列号的累计应答，一旦 ACK 了一个系列号，就说明前面的都到了，所以只要前面的没到，后面的到了也不能 ACK，就会导致后面的到了，也有可能超时重传，浪费带宽。 QUIC 的 ACK 是基于 offset 的，每个 offset 的包来了，进了缓存，就可以应答，应答后就不会重发，中间的空档会等待到来或者重发即可，而窗口的起始位置为当前收到的最大 offset，从这个 offset 到当前的 stream 所能容纳的最大缓存，是真正的窗口大小。显然，这样更加准确。 另外，还有整个连接的窗口，需要对于所有的 stream 的窗口做一个统计。","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/tags/计算机网络/"}],"keywords":[]},{"title":"InnoDB 中的锁和 MVCC","slug":"InnoDB中的锁和MVCC","date":"2019-01-27T13:20:11.000Z","updated":"2019-03-29T10:54:52.057Z","comments":true,"path":"2019/01/27/InnoDB中的锁和MVCC/","link":"","permalink":"http://yoursite.com/2019/01/27/InnoDB中的锁和MVCC/","excerpt":"","text":"锁和 MVCC 是 MySQL 控制并发访问的两种手段。InnoDB 在 MySQL 的基础上提供了更细粒度的行级锁，使用了 next-key 算法解决了幻读的问题。另外 InnoDB 提供了一套基于 MVCC 的一致性非锁定读方式，实现了“读不加锁，读写不冲突”的快照读方式。 一、锁1、表锁InnoDB 直接沿用了ＭySQL 提供的表锁。事实上，表锁的加锁和解锁都是在 MySQL server 层面的，和存储引擎没有关系。加锁的方式如下：123LOCK TABLES orders READ; // 加读锁SELECT SUM(total) FROM orders;UNLOCK TABLES; // 解锁 2、行级锁顾名思义，行级锁锁定的是某一行数据。InnoDB 中的所有数据项都保存在聚簇索引中，所以行级锁实质上锁住的是索引项。如果表中有多个索引存在，一行数据会对应到多个索引项，此时行级锁会锁住所有索引上的相应索引项。 行级锁分为共享锁（S 锁）和排他锁（X 锁）。S 锁和 S 锁可以兼容；S 锁和 X 锁，X 锁和 X 锁不能兼容。（InnoDB 存储引擎默认采用行级锁，所以下文如无指明，S 锁和 X 锁均指行锁） 3、意向锁在没有引入意向锁之前，行级锁和表锁之间的兼容有点麻烦：如果要对一张表加 X 表锁，那么首先要判断这张表是否加了 X 表锁和 S 表锁，其次要判断每一行是否加了 X 锁和 S 锁，如果表的行数比较多的话，这种判断方式会比较损失性能。因此 InnoDB 引入了意向锁。 和行锁一样，意向锁也分为意向共享锁（IS 锁）和意向排他锁（IX 锁）。事务在申请 S 或 X 锁之前，必须先申请到 IS 或 IX 锁。InnoDB 中的意向锁是一种特殊的表锁：意向锁之间互不冲突，意向锁和表锁之间会冲突，此时意向锁相当于同类型的表锁。 二、MVCCMVCC 是一种非锁定读的一致性读机制。它的特点是读不加锁，读写不冲突。InnoDB 利用 undo log 实现了 MVCC。undo log 的数据结构如图所示： 前四行是数据列，后三列是隐藏列。隐藏列的含义如下： DB_ROW_ID：行 ID。占 7 字节，他就项自增主键一样随着插入新数据自增。如果表中不存主键或者唯一索引，那么数据库就会采用 DB_ROW_ID 生成聚簇索引。否则 DB_ROW_ID 不会出现在索引中。 DB_TRX_ID：事务 ID。占 6 字节，表示这一行数据最后插入或修改的事务 id。此外删除在内部也被当作一次更新，在行的特殊位置添加一个删除标记（记录头信息有一个字节存储是否删除的标记）。 DB_ROLL_PTR：回滚指针。占 7 字节，每次对数据进行更新操作时，都会 copy 当前数据，保存到 undo log 中。并修改当前行的回滚指针指向 undo log 中的旧数据行。 MVCC 只有在隔离级别是 READ COMMITED 和 REPEATABLE READ 两个隔离级别下工作。MVCC 可以通过比较数据行的事务 ID 和当前事务 ID 来判断该记录是否对当前事务可见。但仅有 undo log 还不够。试想这样一种情况：事务 599 开始——事务 600 开始——事务 600 查询了表 A——事务 599 更新了表 A——事务 599 提交——事务 600 再次查询表 A。可知事务 600 的两次查询会得到不同的结果，无法满足 RR 隔离级别的要求。这是因为事务 599 的事务 ID 虽然比事务 600 小，但事务 599 还未结束，仍有可能改变数据项的值。 read viewInnoDB 使用了 read view 解决了这个问题。read view 是一张表，记录了当前活跃的事务 ID。InnoDB 在查询时会先对比数据行的事务 ID 和 read view 中的事务 ID。具体如下： 如果数据行事务 ID 大于 read view 中最大的 ID，表示数据行一定是在当前事务之后修改的，对当前事务不可见； 如果数据行事务 ID 小于 read view 中最小的 ID，表示数据行一定是在当前事务开始之前修改并且已提交，所以对当前事务可见。 如果数据行事务 ID 落在 read view 最大最小 ID 的区间中，则要判断数据行事务 ID 和当前事务 ID 的关系： 如果数据行事务 ID 不在活跃事务数组中，表示该事务已提交，此时和当前事务 ID 比较，若小于则可见，大于则不可见； 如果数据行事务 ID 在活跃事务数组中，表示该事务未提交，这里要判断一下数据行事务 ID 是否为当前事务 ID，若是，虽然未提交但同一事务内的修改可见，若不是，则不可见。 当前数据行若是不可见，InnoDB 会沿着 DB_ROLL_PTR 往下查找，直到找到第一个可见的数据行或者 null。 可见与否只是第一步，实际返回的数据还要经过判断。因为删除和更新共用一个字段，区别只是删除有一个字节的删除标记，那么在返回的时候 InnoDB 就要判断当前的数据行是否被标记为删除。如果标记了删除，就不会返回。 MVCC 在 READ COMMITED 和 REPEATABLE READ 两个隔离级别下共用一套逻辑，区别只是在于RC 隔离级别是在读操作开始时刻创建 read view 的，而 RR 隔离级别是在事务开始时刻，确切地说是第一个读操作创建 read view 的。由于这一点上的不同，使得 MVCC 在 RC 隔离级别下读取的是最新提交的数据，而 RR 隔离级别下读取的是事务开始前提交的数据。 三、next-key 解决幻读问题幻读是指一个事务内连续进行两次相同的 SQL 语句可能导致不同的结果，第二次的 SQL 语句可能会返回之前不存在的行。发生这种现象的原因是事务 A 两次查找的间隔中事务 B 插入了一条或多条数据并提交，导致事务 A 的第二次查询查到了新插入的数据。 InnoDB 中有三种锁算法： Record Lock：单个行记录上的锁 Gap Lock：间隙锁，锁住一个范围，但不包含记录本身 Next-Key Lock：Record Lock+Gap Lock，锁定一个范围，并且锁定记录本身 幻读现象问题的根本原因是 Record Lock 只锁住记录本身而不锁范围，导致其它事务可以在记录间插入数据。InnoDB 使用了 Next-Key Lock 来解决这个问题。Next-Key Lock 会锁住一个范围，例如一个索引有 10,11,13,20 这四个值，那么该索引可能被 Next-Key Locking 的区间为：(-∞,10](10,11](11,13](13,20](20,+∞) 当然，如果是等值查询且查询的索引是唯一索引的话，就不用担心被插入的问题，InnoDB 会对 Next-Key Lock 进行优化，将其降级为 Record Lock。 四、innodb 加锁处理分析以上是理论部分，那么实际 innodb 怎么加锁呢？我们结合 SQL 语句来分析。 在支持 MVCC 并发控制的系统中，读操作可以分成两类：快照读 (snapshot read) 与当前读 (current read)。快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。 快照读： select * from table where ?; 当前读： select * from table where ? lock in share mode; select * from table where ? for update; insert into table values (…); update table set ? where ?; delete from table where ?; 简而言之，所有的插入/更新/删除操作都是当前读，且都加了 X 锁；读取操作默认是快照读，但可以声明加 X 锁或者 S 锁。 一条简单 SQL 的加锁实现分析我们拿一条 SQL 语句：delete from t1 where id = 10; 来分析 innodb 的加锁情况。但光有这一条 SQL 是不够的，我们还需要知道一些前提： 前提 1：id 列是不是主键？ 前提 2：当前系统的隔离级别是什么？ 前提 3：如果 id 列不是主键，那么 id 列上有索引吗？ 前提 4：如果 id 列上有二级索引，那么这个索引是唯一索引吗？ 基于这些前提的不同，我们可以组合出以下几种情况（隔离级别只考虑 RC 和 RR 的情况）： 组合 1：id 列是主键，RC 隔离级别 组合 2：id 列是二级唯一索引，RC 隔离级别 组合 3：id 列是二级非唯一索引，RC 隔离级别 组合 4：id 列上没有索引，RC 隔离级别 组合 5：id 列是主键，RR 隔离级别 组合 6：id 列是二级唯一索引，RR 隔离级别 组合 7：id 列是二级非唯一索引，RR 隔离级别 组合 8：id 列上没有索引，RR 隔离级别 组合 1：id 列是主键，RC 隔离级别这个组合，是最简单，最容易分析的组合。id 是主键，Read Committed 隔离级别，给定 SQL：delete from t1 where id = 10; 只需要将主键上，id = 10 的记录加上 X 锁即可。如下图所示： id 是主键时，此 SQL 只需要在 id=10 这条记录上加 X 锁即可。 组合 2：id 列是二级唯一索引，RC 隔离级别这个组合，id 不是主键，而是一个 unique 的二级索引键值。那么在 RC 隔离级别下，delete from t1 where id = 10; 需要加什么锁呢？见下图： 此组合中，id 是 unique 索引，而主键是 name 列。此时，加锁的情况由于组合一有所不同。由于 id 是 unique 索引，因此 delete 语句会选择走 id 列的索引进行 where 条件的过滤，在找到 id=10 的记录后，首先会将 unique 索引上的 id=10 索引记录加上 X 锁，同时，会根据读取到的 name 列，回主键索引 (聚簇索引)，然后将聚簇索引上的 name = ‘d’ 对应的主键索引项加 X 锁。为什么聚簇索引上的记录也要加锁？试想一下，如果并发的一个 SQL，是通过主键索引来更新：update t1 set id = 100 where name = ‘d’; 此时，如果 delete 语句没有将主键索引上的记录加锁，那么并发的 update 就会感知不到 delete 语句的存在，违背了同一记录上的更新/删除需要串行执行的约束。 组合 3：id 列是二级非唯一索引，RC 隔离级别相对于组合一、二，组合三又发生了变化，隔离级别仍旧是 RC 不变，但是 id 列上的约束又降低了，id 列不再唯一，只有一个普通的索引。假设 delete from t1 where id = 10; 语句，仍旧选择 id 列上的索引进行过滤 where 条件，那么此时会持有哪些锁？同样见下图： 可以看到，首先，id 列索引上，满足 id = 10 查询条件的记录，均已加锁。同时，这些记录对应的主键索引上的记录也都加上了锁。与组合二唯一的区别在于，组合二最多只有一个满足等值查询的记录，而组合三会将所有满足查询条件的记录都加锁。 组合 4：id 列上没有索引，RC 隔离级别相对于前面三个组合，这是一个比较特殊的情况。id 列上没有索引，where id = 10;这个过滤条件，没法通过索引进行过滤，那么只能走全表扫描做过滤。对应于这个组合，SQL 会加什么锁？或者是换句话说，全表扫描时，会加什么锁？这个答案也有很多：有人说会在表上加 X 锁；有人说会将聚簇索引上，选择出来的 id = 10;的记录加上 X 锁。那么实际情况呢？请看下图： 由于 id 列上没有索引，因此只能走聚簇索引，进行全部扫描。从图中可以看到，满足删除条件的记录有两条，但是，聚簇索引上所有的记录，都被加上了 X 锁。无论记录是否满足条件，全部被加上 X 锁。既不是加表锁，也不是在满足条件的记录上加行锁。 有人可能会问？为什么不是只在满足条件的记录上加锁呢？这是由于 MySQL 的实现决定的。如果一个条件无法通过索引快速过滤，那么存储引擎层面就会将所有记录加锁后返回，然后由 MySQL Server 层进行过滤。因此也就把所有的记录，都锁上了。 组合 5：id 列是主键，RR 隔离级别上面的四个组合，都是在 Read Committed 隔离级别下的加锁行为，接下来的四个组合，是在 Repeatable Read 隔离级别下的加锁行为。 组合五，id 列是主键列，Repeatable Read 隔离级别，针对 delete from t1 where id = 10; 这条 SQL，加锁与组合一：[id 主键，Read Committed] 一致。 组合 6：id 列是二级唯一索引，RR 隔离级别与组合五类似，组合六的加锁，与组合二：[id 唯一索引，Read Committed] 一致。两个 X 锁，id 唯一索引满足条件的记录上一个，对应的聚簇索引上的记录一个。 组合 7：id 列是二级非唯一索引，RR 隔离级别组合七，Repeatable Read 隔离级别，id 上有一个非唯一索引，执行 delete from t1 where id = 10; 假设选择 id 列上的索引进行条件过滤，最后的加锁行为，是怎么样的呢？同样看下面这幅图： 此图，相对于组合三多了一个 GAP 锁，这是因为 RR 级别区别于 RC 级别的一点是 RR 级别要防止幻读。我们在前一节讲过 innodb 基于 Next-Lock 防止幻读，而 Next-Lock 就是 GAP Lock+Record Lock。加在索引上的是 Record Lock，而在中间的就是 GAP Lock。 那么为什么组合五、组合六，也是 RR 隔离级别，却不需要加 GAP 锁呢？这是因为组合五，id 是主键；组合六，id 是 unique 键，都能够保证唯一性。一个等值查询，最多只能返回一条记录，而且新的相同取值的记录，一定不会再新插入进来，因此也就避免了 GAP 锁的使用。 组合 8：id 列上没有索引，RR 隔离级别组合八，Repeatable Read 隔离级别下的最后一种情况，id 列上没有索引。此时 SQL：delete from t1 where id = 10; 没有其他的路径可以选择，只能进行全表扫描。最终的加锁情况，如下图所示： 五、参考资料何登成的技术博客","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"TCP 协议回顾","slug":"TCP协议回顾","date":"2019-01-23T13:28:32.000Z","updated":"2019-03-09T07:35:44.081Z","comments":true,"path":"2019/01/23/TCP协议回顾/","link":"","permalink":"http://yoursite.com/2019/01/23/TCP协议回顾/","excerpt":"","text":"本科的时候面向考试学习计算机网络，TCP 是重点中的重点，可惜当时不知道 TCP 在网络世界中的重要，考完试就把知识点扔了。最近在复习计算机网络，看到 TCP 这章，就像看到了一个老朋友。就想把它记录下来，便于复习。 TCP 协议非常复杂，标准也非常多，但核心内容就以下几个部分： 三次握手 四次挥手 可靠性传输 流量控制 拥塞控制 TCP 协议头要想了解 TCP 协议首先就必须了解 TCP 的协议头： 首先，源端口号和目标端口号是不可少的，这一点和 UDP 是一样的。如果没有这两个端口号。 数据就不知道应该发给哪个应用。 接下来是包的序号和确认序号。为了保证消息的顺序性到达，TCP 给每个包编了一个序号。初始序号在建立连接时指定，此后每个包的序号为上一个包的序号+上一个包的字节数。服务器会返回一个确认号，表示这个序号之前的包都收到了。 然后是一些状态位。例如 SYN 是发起一个连接，ACK 是回复，RST 是重新连接，FIN 是结束连接。TCP 是面向连接的，这些带状态位的包可以改变双方的状态。 窗口大小是跟流量控制有关。TCP 是全双工的协议，通信双方都会维护一个缓存空间。这个窗口大小就是告诉对方我还有多少剩余的缓存空间。 三次握手TCP 是面向连接的协议，所以在建立连接前有一系列的动作，被称为三次握手： 一开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端口，处于 LISTEN 状态。然后客户端主动发起连接 SYN，之后处于 SYN-SENT 状态。服务端收到发起的连接，返回 SYN，并且 ACK 客户端的 SYN，之后处于 SYN-RCVD 状态。客户端收到服务端发送的 SYN 和 ACK 之后，发送 ACK 的 ACK，之后处于 ESTABLISHED 状态，因为它一发一收成功了。服务端收到 ACK 的 ACK 之后，处于 ESTABLISHED 状态，因为它也一发一收了。 三次握手除了双方建立连接外，还要沟通一件事情，就是 TCP 包的序号的问题。 双方在发送 SYN 包的时候，各自需要指定一个针对这次连接的序号 seq。这个 seq 实质上可以看出一个 32 位的计时器，每 4ms 加一。为什么要这么做呢？主要是为了防止在网络中被延迟的分组在以后被重复传输，而导致某个连接的一端对它作错误的判断。如果序号不按这种方式分配，而是从 1 开始，则会出现这样的情况：AB 建立连接之后，A 发送了 1,2,3 三个包，然后掉线了。由于网络的原因三个包没有到达 B，在网络中游荡。然后 A 重连了，序号重新从 1 开始，他又发送了 1,2 两个包，但没有发送 3 号包。此时上一次连接发送的 3 号包却到达了 B，B 以为是 A 这次发送的，就产生了误判。为了避免这种情况的发生，TCP 协议规定了这种方式生成初始 seq。以这种方式生成的初始 seq，需要 4 个多小时才会重复，此时早已过了 3 号包的生存时间（TTL）。 为什么要三次握手而不是两次？ 原因一：服务器会收到客户端很早以前发送的，但因为延迟导致现在才到达的 SYN 报文。如果不采用三次握手，则服务器会认为新的连接已经建立，会白白浪费缓存等资源。原因二：三次握手需要交流双方的初始序号 seq，服务器发送的第二次握手是针对客户端在第一次握手中约定的客户端初始 seq 的确认，客户端的第三次握手是针对服务器在第二次握手中约定的服务器初始 seq 的确认。如果没有第三次握手，万一服务器的 SYN 包丢了，那么客户端无法得知服务器的初始序号，此时客户端就没法接收服务器的包，因为客户端没法辨别这个包是本次连接中发送的，还是上一次连接中发送的。 SYN 洪泛攻击 因为服务器在收到一个 SYN 报文后，会初始化连接变量和缓存，如果攻击方会发送大量 SYN 报文，而不完成第三次握手，那么就会导致服务器的连接资源被消耗殆尽。针对 SYN 洪泛攻击有一种有效的防御手段，称为SYN cookie：当服务器接收到一个 SYN 报文时，它不知道这是来自一个合法的请求还是 SYN 洪泛攻击的一部分。所以它不会为其分配资源，而是将该报文中的源、目的 IP 地址和端口和服务器自己的秘密数做哈希，将（秒级时间（5 位）+最长分段大小（3 位）+哈希（24 位））作为初始 seq 返回给客户端。如果是一个合法用户，会返回一个 ACK 包，服务器可以通过将 ACK 包中的源、目的 IP 地址和端口，和 ACK-1 对比，得知这是否是一个合法的 SYN 确认包。然后可以通过秒级时间确定这是否是一个新的包。如果是新的包且合法，服务器才会分配资源。这样就有效防止了 SYN 洪泛攻击的发生。 四次挥手有建立连接，必然也有断开连接。断开连接的动作被称为四次挥手： 区别于三次握手，四次挥手的发起方可以是客户端也可以是服务端。因此不区分客户端和服务端而是用 AB 代替。 一开始，A 和 B 都处于 ESTABLISHED 的状态。然后 A 发送 FIN 表示请求断开连接，之后 B 处于 FIN-WAIT-1 的状态。 B 在接收到 FIN 请求后，会发送一个 ACK 包表示收到了 FIN 请求，之后 B 处于 CLOSED-WAIT 状态。需要注意的是此时 B 发送的是 ACK 包而不是 FIN 包，之所以不像三次握手一样直接回应一个 FIN 包，是因为此时 B 可能还有些事情没有做完，还可能发送数据，所以称为半关闭状态。 这个时候 A 可以选择不再接收数据，也可以选择最后再接收一段数据，等待 B 也主动关闭。不论如何，A 在收到 ACK 包后都进入了 FIN-WAIT2 阶段。此时如果 B 下线，A 将永远在这个状态。TCP 协议里没有对这个状态的处理，但 Linux 有，可以调整 tcp_fin_timeout 这个参数，设置一个超时时间。 B 处理完了所有的事情，终于也准备关闭，此时会发送一个 FIN 包。之后 B 进入 LAST-ACK 状态，等待 A 的 ACK 包。 A 在收到服务器的 FIN 包后会发送一个 ACK 包表示收到了 B 的 FIN 包。按理说此时 A 就可以关闭了，但由于 A 最后的 ACK 存在丢包的可能，B 没有收到最后的 ACK 包的话，就会重发一个 FIN 包，如果这时候 A 关闭了，B 就再也收不到 ACK 了。因而 TCP 协议要求 A 最后等待一段时间 TIME_WAIT，这个时间要足够长，长到 B 没收到 ACK 的话，重发的 FIN 包还能到达 A。 A 直接关闭还有一个问题是，A 的端口就直接空出来了，但是 B 不知道，B 原来发过的很多包很可能还在路上，如果 A 的端口被一个新的应用占用了，这个新的应用会收到上个连接中 B 发过来的包，虽然序列号是重新生成的，但是这里要上一个双保险，防止产生混乱，因而也需要等足够长的时间，等到原来 B 发送的所有的包都过期了，再空出端口来。 等待的时间设为 2MSL，MSL 是 Maximum Segment Lifetime（报文最大生存时间），它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的， 而 IP 头中有一个 TTL 域，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。协议规定 MSL 为 2 分钟，实际应用中常用的是 30 秒，1 分钟和 2 分钟等。 服务器大量 CLOSE_WAIT\\TIME_WAIT 状态的原因 如果服务器出现异常，百分之八九十都是下面两种情况：1、服务器保持了大量 CLOSE_WAIT 状态。产生的原因在于：TCP Server 已经 ACK 了过来的 FIN 数据包，但是上层应用程序迟迟没有发命令关闭 Server 到 client 端的连接。所以 TCP 一直在那等啊等…..所以说如果发现自己的服务器保持了大量的 CLOSE_WAIT，问题的根源十有八九是自己的 server 端程序代码的问题。2、服务器保持了大量 TIME_WAIT 状态。产生的原因在于：服务器处理大量高并发短连接并主动关闭连接时容易出现 TIME_WAIT 积压。这是因为关闭的发起方在 TIME_WAIT 阶段需要等待 1-4 分钟才能回收资源。如果连接过多将导致资源来不及回收。解决方案是修改 Linux 内核，允许将 TIME-WAIT sockets 重新用于新的 TCP 连接，并开启 TCP 连接中 TIME-WAIT sockets 的快速回收，这些默认都是关闭的。 可靠性传输TCP 协议为了保证顺序性，每一个包都有一个 ID。在建立连接的时候，会商定起始的 ID 是什么，然后按照 ID 一个个发送。为了保证不丢包，对于发送的包都要进行应答，但是这个应答也不是一个一个来的，而是会应答某个之前的 ID，表示都收到了，这种模式称为累计确认或者累计应答（cumulative acknowledgment）。 为了记录所有发送的包和接收的包，TCP 也需要发送端和接收端分别都有缓存来保存这些记录。发送端的缓存里是按照包的 ID 一个个排列，根据处理的情况分成四个部分。 第一部分：发送了并且已经确认的。 第二部分：发送了并且尚未确认的。 第三部分：没有发送，但是已经等待发送的。 第四部分：没有发送，并且暂时还不会发送的。 为什么会有三、四部分的区分呢？这是因为接受端有个处理极限，就是剩余缓冲区的大小，如果给接收端发送的包的大小超过了剩余缓冲区的大小，那么有一部分包就会被丢弃，这是不合适的。所以超出剩余缓冲区大小的包，发送端暂时不会发。 于是，发送端需要保持下面的数据结构： 对于接收端来讲，它的缓存里记录的内容要简单一些： 第一部分：接收并且确认过的。 第二部分：还没接收，但尚在接收能力范围之内的。 第三部分：还没接收，超过能力范围的。 对应的数据结构像这样： 顺序与丢包问题还是刚才的图，在发送端来看，1、2、3 已经发送并确认；4、5、6、7、8、9 都是发送了还没确认；10、11、12 是还没发出的；13、14、15 是接收方没有空间，不准备发的。 在接收端来看，1、2、3、4、5 是已经完成 ACK，但是没读取的；6、7 是等待接收的；8、9 是已经接收，但是没有 ACK 的。 发送端和接收端当前的状态如下： 1、2、3 没有问题，双方达成了一致。 4、5 接收方说 ACK 了，但是发送方还没收到，有可能丢了，有可能在路上。 6、7、8、9 肯定都发了，但是 8、9 已经到了，但是 6、7 没到，出现了乱序，缓存着但是没办法 ACK。 根据这个例子，我们可以知道，顺序问题和丢包问题都有可能发生，所以我们先来看确认与重发的机制。 假设 4 的确认到了，不幸的是，5 的 ACK 丢了，6、7 的数据包丢了，这该怎么办呢？ 一种方法就是超时重试，TCP 会为当前最小未应答的包绑定一个定时器，当收到 ACK 时会重启定时器，并绑定到现在最小未应答的包上。若当前无未应答包，则关闭定时器。等下一个包发出去后再启动并绑定到新的包上。 如何设置往返时间 RTT 呢？TCP 采用了加权的自适应重传算法：EstimatedRTT=0.875EstimatedRTT+0.125SampleRTT 其中 EstimatedRTT 为平均往返时间，SampleRTT 为某次采样的往返时间。 如果过一段时间，5、6、7 都超时了，就会重新发送。接收方发现 5 原来接收过，于是丢弃 5；6 收到了，发送 ACK，要求下一个是 7，7 不幸又丢了。当 7 再次超时的时候，有需要重传的时候，TCP 的策略是超时间隔加倍。每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。 超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？ 有一个叫快速重传的机制：当接收方收到一个序号大于下一个所期望的报文段时，就检测到了数据流中的一个间格，于是发送三个冗余的 ACK，客户端收到后，就在定时器过期之前，重传丢失的报文段。 例如，接收方发现 6、8、9 都已经接收了，就是 7 没来，那肯定是丢了，于是发送三个 6 的 ACK，要求下一个是 7。客户端收到 3 个，就会发现 7 的确又丢了，不等超时，马上重发。 还有一种方式称为Selective Acknowledgment （SACK）。这种方式需要在 TCP 头里加一个 SACK 的东西，可以将缓存的地图发送给发送方。例如可以发送 ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是 7 丢了。 流量控制流量控制是为了平衡发送端与接收端的速度，避免出现包处理不过来的情况。在协议头里面，有一个窗口大小字段，这个就是用来进行流量控制的。 我们先假设窗口不变的情况，窗口始终为 9。4 的确认来的时候，会右移一个，这个时候第 13 个包也可以发送了。 这个时候，假设发送端发送过猛，会将第三部分的 10、11、12、13 全部发送完毕，之后就停止发送了，未发送可发送部分 0。 当对于包 5 的确认到达的时候，在客户端相当于窗口再滑动了一格，这个时候，才可以有更多的包可以发送了，例如第 14 个包才可以发送。 如果接收方实在处理的太慢，导致缓存中没有空间了，可以通过确认信息修改窗口的大小，甚至可以设置为 0，则发送方将暂时停止发送。 我们假设一个极端情况，接收端的应用一直不读取缓存中的数据，当数据包 6 确认后，窗口大小就不能再是 9 了，就要缩小一个变为 8。 这个新的窗口 8 通过 6 的确认消息到达发送端的时候，你会发现窗口没有平行右移，而是仅仅左面的边右移了，窗口的大小从 9 改成了 8。 如果接收端还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，直到为 0。 当这个窗口通过包 14 的确认到达发送端的时候，发送端的窗口也调整为 0，停止发送。 如果这样的话，发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满 了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。 拥塞控制拥塞控制是为了避免网络中传输着太多的包导致网络拥挤。这里有一个公式，即：发送但还未确认的包要小于等于滑动窗口（rwnd）和拥塞窗口（cwnd）的最小值。前者在流量控制中已经讲过，剩下的就是后者。 拥塞控制有三个时期：慢启动、拥塞避免和快速恢复。 当开始时，cwnd 设置为一个报文段，一次只能发送一个；当收到这一个确认的时候，cwnd 加一，于是一次能发送两个；当这两个确认到来的时候，每个确认 cwnd 加一，两个确认 cwnd 加二，于是一次能发送四个。当这四个的确认到来的时候，每个确认 cwnd 加一，四个确认 cwnd 加四，于是一次能够发送八个。依次类推，此时，cwnd 的增长速度是指数型的增长。 涨到什么时候是个头呢？有一个值 ssthresh 初始为 65535 个字节，当超过这个值的时候，就进入了拥塞避免状态。此时不再是一个确认对应一个 cwnd 的增长，而是一个确认对应 1/cwnd 的增长。我们接着上面的过程来，一次发送八个，当八个确认到来的时候，每个确认增加 1/8，八个确认一共 cwnd 增加 1，于是一次能够发送九个，变成了线性增长。 但 cwnd 不可能无限增长，总有一个时候网络会拥挤，拥挤的表现形式是丢包。发送端有两种方式感知到丢包：超时和收到三个相同 ACK（快速重传）。针对这两种情况的丢包，发送端的处理方式也不一样。 第一种是超时丢包，这种情况下，发送端会认为当前网络非常拥挤，因此会采取激进的限制措施：将 sshresh 设为 cwnd/2，将 cwnd 设为 1，重新开始慢启动。 第二种是三个相同 ACK 丢包，发送端会认为这个丢包是个偶然事件，因此网络并不非常拥挤，采取的措施也会温和一些：sshresh 设为 cwnd/2，cwnd 设为 cwnd/2，又因为返回了三个确认包，cwnd 再加 3。之后进入快速恢复阶段，因为当前 cwnd 仍在比较高的值，这个阶段中 cwnd 也是线性增长。 两种方式的比较如下： 总结TCP 协议的核心部分在于：三次握手、四次挥手、可靠性传输、流量控制、拥塞控制，掌握好这些知识点对网络编程很有帮助。","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/tags/计算机网络/"}],"keywords":[]},{"title":"插入缓冲、两次写和自适应哈希索引","slug":"插入缓冲、两次写和自适应哈希索引","date":"2019-01-14T03:56:25.000Z","updated":"2019-04-26T12:12:58.757Z","comments":true,"path":"2019/01/14/插入缓冲、两次写和自适应哈希索引/","link":"","permalink":"http://yoursite.com/2019/01/14/插入缓冲、两次写和自适应哈希索引/","excerpt":"","text":"InnoDB 是 MySQL 数据库从 5.5.8 版本开始的默认存储引擎，它将数据放在一个逻辑的表空间中，这个表空间就像黑盒一样由 InnoDB 存储引擎自身进行管理。从 MySQL 4.1 开始，它可以将每个 InnoDB 的表单独存在一个独立的 idb 文件中。此外，InnoDB 支持用裸设备（raw disk，不被操作系统管理的设备）建立其表空间。 InnoDB 通过使用多版本并发控制（MVCC）来获得高并发性，并且实现了 SQL 标准的四种隔离级别，默认为 REPEATABLE 级别。同时，使用一种称为 next-key locking 的策略来避免幻读。除此之外，InnoDB 还提供了插入缓冲、二次写、自适应哈希索引等高性能和高可用的功能。 缓冲池功能的实现离不开底层的配合。为了协调 CPU 速度与磁盘速度之间的鸿沟，InnoDB 在内存中开辟了一块空间叫内存池，将对数据库的修改首先保存在内存中。比如对于页的操作，首先会在内存中进行，然后后台线程会把脏页（还没有刷入磁盘的页）刷入磁盘。 缓冲池中缓存的数据页类型有：索引页、数据页、undo 页、插入缓冲、自适应哈希索引、InnoDB 存储的锁信息、数据字典信息等。不能简单地认为，缓冲池只是缓存索引页和数据页，它们只是占缓冲池很大的一部分而已。 中点插入策略缓冲池对于数据页的管理，是使用 LRU 的方式管理的。和传统的 LRU 稍有不同的是，InnoDB 使用一种称为“中点插入策略”的方式插入数据： 新页的第一次插入只会插入到 LRU 链表尾端 3/8（中点）的位置 页的再次命中（LRU 上的页被命中）才会把页插入到链表头部 可以设置一个 InnoDB_old_blocks_time 的参数，表示新页插入后过多久才有资格被插入到链表头部 这样做的好处是避免了一些冷数据对真正热点数据的干扰。比如进行扫描操作时，需要访问表中的许多页，甚至是全部页，而这些页通常来说只在这次查询中需要，并不是活跃的热点数据。如果页被放入 LRU 链表头部，那么非常可能将所需的真正热点数据刷出。InnoDB_old_blocks_time 也是出于同样的目的。可以避免临近的几次查询把页刷入热数据的情况。 插入缓冲InnoDB 底层使用聚簇索引管理数据。在进行插入操作的时候，数据页的存放是按主键顺序存放的，此时磁盘顺序访问，速度会很快。但对于非聚集索引叶子节点的插入则不再是顺序的了，这时需要离散地访问非聚集索引页，磁盘的随机读取效率很低，导致了插入操作的性能下降。 InnoDB 存储引擎创造性地设计了 Insert Buffer，对于非聚簇索引的插入或更新操作，不是每一次直接插入到索引页中，而是先判断插入的非聚簇索引页是否在缓冲池中，若在，则直接插入；若不在，则先放入到一个 Insert Buffer 对象中。然后再以一定的频率和情况进行 Insert Buffer 和辅助索引叶子节点的 merge 操作，这时通常能将多个插入合并到一个操作中（因为在一个索引页中），这就大大提高了对于非聚簇索引插入的性能。 然而 Insert Buffer 的使用需要同时满足以下两个条件： 索引是辅助索引。 索引不是唯一的。（因为在插入缓冲时，数据库并不去查找索引页来判断插入的记录的唯一性。如果去查找肯定又会有离散读取的发生，就背离了 Insert Buffer 的初衷） Insert Buffer 的数据结构是一颗 B+树，4.1 版本之前每张表都有一颗 Insert Buffer B+树，4.1 版本之后所有表共用一颗 B+树。Insert Buffer B+树的非叶子节点存放的是查询的 search key，其构造如图： 其中 space 表示待插入记录所在表的表空间 id，在 InnoDB 存储引擎中，每个表有一个唯一的 space id，可以通过 space id 查询得知是哪张表；maker 是用来兼容老版本的 Insert Buffer。offerset 表示页所在的偏移量。 当一个辅助索引要插入到页（space,offset）时，如果这个页不在缓冲池中，那么 InnoDB 存储引擎首先根据上述规则构造一个 search key，接下来查询 Insert Buffer 这棵 B+树，然后再将记录插入到 Insert Buffer B+树的叶子节点中。 两次写如果说 Insert Buffer 带给 InnoDB 存储引擎的是性能上的提升，那么 doublewrite（两次写）带给 InnoDB 存储引擎的是数据页的可靠性。 InnoDB 中有记录（Row）被更新时，先将其在 Buffer Pool 中的 page 更新，并将这次更新记录到 Redo Log file 中，这时候 Buffer Pool 中的该 page 就是被标记为 Dirty。在适当的时候（Buffer Pool 不够、Redo 不够，系统闲置等），这些 Dirty Page 会被 Checkpoint 刷新到磁盘进行持久化操作。 但尴尬的地方在于 InnoDB 的 Page Size 是 16KB，其数据校验也是针对这 16KB 来计算的，将数据写入到磁盘是以 Page 为单位进行操作的，而文件系统是以 4k 为单位写入，磁盘 IO 的最小单位是 512K，因此并不能保证数据页的写入就是原子性的。 那么可不可以通过 redo log 来进行恢复呢？答案是只能恢复校验完整（还没写）的页，不能恢复已损坏的页。比如某次 checkpoint 要刷入 4 个数据页，其中第一页写了 2KB，后三页还未写。那么根据 redo log 可以恢复后三页，但已经写了 2KB 的页没法恢复，因为没法知道在宕机前第一页到底写了多少。 为什么 redo log 不需要 doublewrite 的支持？ 因为 redo log 写入的单位就是 512 字节，也就是磁盘 IO 的最小单位，所以无所谓数据损坏。 double write 由两部分组成，一部分是内存中的 doublewrite buffer，大小为 2MB，另一部分是物理磁盘上共享表空间中连续的 128 个页，即 2 个区，大小同样为 2MB。在对缓冲池的脏页进行刷新时，并不直接写磁盘，而是会通过 memcpy 函数将脏页先复制到内存中的 doublewrite buffer，之后通过 doublewrite buffer 再分两次，每次 1MB 顺序地写入共享表空间的物理磁盘上，然后马上调用 fsync 函数，同步磁盘。在这个过程中，因为 doublewrite 页是连续的，因此这个过程是顺序写的，开销不是很大。其工作流程如下图所示： 现在我们来分析一下为什么 double write 可以生效。当宕机发生时，有那么几种情况：1、磁盘还未写，此时可以通过 redo log 恢复；2、磁盘正在进行从内存到共享表空间的写，此时数据文件中的页还没开始被写入，因此也同样可以通过 redo log 恢复；3、磁盘正在写数据文件，此时共享表空间已经写完，可以从共享表空间拷贝页的副本到数据文件实现恢复。 自适应哈希索引哈希是一种非常快的查询方法，一般只需要一次查找就能定位数据。InnoDB 存储引擎会监控对表上各索引页的查询，如果观察到建立哈希索引可以带来速度提升，则建立哈希索引，称之为自适应哈希索引。 自适应哈希索引有一个要求，即对这个页的连续访问模式必须是一样的。例如对于（a,b）这样的联合索引页，其访问模式可以是以下情况： WHERE a=xxx WHERE a=xxx and b=xxx 若交替以上两种查询，那么 InnoDB 存储引擎不会对该页构造哈希索引（这是因为哈希索引是以索引的哈希值为键值存放的，hash(a) 和 hash(a,b) 是两个完全不同的值） 在连续的查询模式一样的条件下，如果能满足以下条件，InnoDB 存储引擎就会创建相应的哈希索引： 以该连续模式连续访问了 100 次 以该模式连续访问了 页中记录总数/16 次 哈希索引只能用来搜索等值的查询，如 SELECT * FROM table WHERE index_col=’xxx’。对于其它类型的查找，如范围查找，是不能使用哈希索引的。 InnoDB 存储引擎官方文档显示，启用 AHI 后,读取和写入速度可以提高 2 倍，辅助索引的连接操作性能可以提高 5 倍。 总结InnoDB 存储引擎在 MySQL 原有的基础上做了很多优化，主要涉及到的就是缓冲池和磁盘的交互。尽可能多地读缓存，尽量少地读磁盘，于是有了自适应哈希索引；尽量多地顺序写，尽量少地离散写，于是有了插入缓冲；由于缓存的易失性，带来的数据恢复问题，又有了两次写。这些设计思想不只可以用于数据库，也可以用于程序设计的方方面面。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"Java NIO 浅析","slug":"Java-NIO浅析","date":"2019-01-09T04:10:18.000Z","updated":"2019-03-09T07:21:55.434Z","comments":true,"path":"2019/01/09/Java-NIO浅析/","link":"","permalink":"http://yoursite.com/2019/01/09/Java-NIO浅析/","excerpt":"","text":"NIO（Non-blocking I/O），是一种同步非阻塞的 I/O 模型，也是 I/O 多路复用的基础，已经被越来越多地应用到大型应用服务器，成为解决高并发与大量连接、I/O 处理问题的有效方式。Java 中的 NIO 是 jdk 1.4 之后新出的一套 IO 接口，相比传统 IO(BIO)，两者有如下区别： IO 是面向流的，NIO 是面向缓冲区的 IO 流是同步阻塞的，NIO 流是同步非阻塞的 NIO 有选择器（Selector），IO 没有 IO 的流是单向的，NIO 的通道（Channel）是双向的 IO 基本概念Linux 的内核将所有外部设备都可以看做一个文件来操作。那么我们对与外部设备的操作都可以看做对文件进行操作。我们对一个文件的读写，都通过调用内核提供的系统调用；内核给我们返回一个 file descriptor（fd,文件描述符）。对一个 socket 的读写也会有相应的描述符，称为 socketfd(socket 描述符）。描述符就是一个数字 (可以理解为一个索引)，指向内核中一个结构体（文件路径，数据区，等一些属性）。应用程序对文件的读写就通过对描述符的读写完成。 一个基本的 IO，它会涉及到两个系统对象，一个是调用这个 IO 的进程对象，另一个就是系统内核 (kernel)。当一个 read 操作发生时，它会经历四个阶段： 1、通过 read 系统调用想内核发起读请求。 2、内核向硬件发送读指令，并等待读就绪。 3、内核把将要读取的数据复制到描述符所指向的内核缓存区中。 4、将数据从内核缓存区拷贝到用户进程空间中。 同步和异步同步和异步关注的是消息通信机制 (synchronous communication / asynchronous communication)。所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。 而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 阻塞和非阻塞阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态。阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 常见 I/O 模型对比所有的系统 I/O 都分为两个阶段：等待就绪和操作。举例来说，读函数，分为等待系统可读和真正的读；同理，写函数分为等待网卡可以写和真正的写。需要说明的是等待就绪的阻塞是不使用 CPU 的，是在“空等”；而真正的读写操作的阻塞是使用 CPU 的，真正在”干活”，而且这个过程非常快，属于 memory copy，带宽通常在 1GB/s 级别以上，可以理解为基本不耗时。 以 socket.read() 为例子：传统的 BIO 里面 socket.read()，如果 TCP RecvBuffer 里没有数据，函数会一直阻塞，直到收到数据，返回读到的数据。对于 NIO，如果 TCP RecvBuffer 有数据，就把数据从网卡读到内存，并且返回给用户；反之则直接返回 0，永远不会阻塞。最新的 AIO(Async I/O) 里面会更进一步：不但等待就绪是非阻塞的，就连数据从网卡到内存的过程也是异步的。换句话说，BIO 里用户最关心“我要读”，NIO 里用户最关心”我可以读了”，在 AIO 模型里用户更需要关注的是“读完了”。NIO 一个重要的特点是：socket 主要的读、写、注册和接收函数，在等待就绪阶段都是非阻塞的，真正的 I/O 操作是同步阻塞的（消耗 CPU 但性能非常高）。 传统 BIO 模型分析了解 NIO 就要从传统 BIO 的弊端说起。 在传统的 BIO 中，一旦用户线程发起 IO 请求，则必须要等内核将数据报准备好，才能将数据从内核复制到用户空间。这是一种效率很低的方式。传统的 BIO 一般要配合线程池来使用，我们的编程范式（伪代码）一般是这样的： 123456789101112131415161718192021222324ExecutorService executor = Excutors.newFixedThreadPollExecutor(100); // 线程池ServerSocket serverSocket = new ServerSocket();serverSocket.bind(8088);while(!Thread.currentThread.isInturrupted())&#123; // 主线程死循环等待新连接到来 Socket socket = serverSocket.accept(); executor.submit(new ConnectIOnHandler(socket)); // 为新的连接创建新的线程&#125;class ConnectIOnHandler extends Thread&#123; private Socket socket; public ConnectIOnHandler(Socket socket)&#123; this.socket = socket; &#125; public void run()&#123; while(!Thread.currentThread.isInturrupted()&amp;&amp;!socket.isClosed())&#123; // 死循环处理读写事件 String someThing = socket.read()....// 读取数据 if(someThing!=null)&#123; ......//处理数据 socket.write()....// 写数据 &#125; &#125; &#125;&#125; 这是一个经典的每连接每线程的模型，之所以使用多线程，主要原因在于 socket.accept()、socket.read()、socket.write() 三个主要函数都是同步阻塞的，当一个连接在处理 I/O 的时候，系统是阻塞的，如果是单线程的话必然就挂死在那里；但 CPU 是被释放出来的，开启多线程，就可以让 CPU 去处理更多的事情。其实这也是所有使用多线程的本质： 利用多核。 当 I/O 阻塞系统，但 CPU 空闲的时候，可以利用多线程使用 CPU 资源。 现在的多线程一般都使用线程池，可以让线程的创建和回收成本相对较低。在活动连接数不是特别高（小于单机 1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。 不过，这个模型最本质的问题在于，严重依赖于线程。但线程是很”贵”的资源，主要表现在： 线程的创建和销毁成本很高，在 Linux 这样的操作系统中，线程本质上就是一个进程。创建和销毁都是重量级的系统函数。 线程本身占用较大内存，像 Java 的线程栈，一般至少分配 512K～1M 的空间，如果系统中的线程数过千，恐怕整个 JVM 的内存都会被吃掉一半。 线程的切换成本是很高的。操作系统发生线程切换的时候，需要保留线程的上下文，然后执行系统调用。如果线程数过高，可能执行线程切换的时间甚至会大于线程执行的时间，这时候带来的表现往往是系统 load 偏高、CPU sy 使用率特别高（超过 20%以上)，导致系统几乎陷入不可用的状态。 容易造成锯齿状的系统负载。因为系统负载是用活动线程数或 CPU 核心数，一旦线程数量高但外部网络环境不是很稳定，就很容易造成大量请求的结果同时返回，激活大量阻塞线程从而使系统负载压力过大。 所以，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。随着移动端应用的兴起和各种网络游戏的盛行，百万级长连接日趋普遍，此时，必然需要一种更高效的 I/O 处理模型。 NIO 是如何工作的 这是一个 NIO 基本的工作方式（但不常用），我们把一个套接口设置为非阻塞，当所请求的 I/O 操作不能满足要求时候，不把本进程投入睡眠，而是返回一个错误。也就是说当数据没有到达时并不等待，而是以一个错误返回。 事件驱动的 I/O 复用模型（常用）在 BIO 的场景下，为了避免线程长时间阻塞在等待内核准备上，我们选择了每连接每线程的方式。但在 NIO 的场景下，如果当前的连接没有准备好，可以选择下一个连接。比如我们的聊天程序，我们可以建立两个连接：一个发送端，一个接收端。程序会不断轮询这两个连接，如果接收端有数据达到，那就把它显示在屏幕上；如果发送端有数据发出，那就把它发出。但如果接收端没有数据，或者发送端的网卡没有准备好，程序也不会停下来，而是继续轮询，直到有一方准备好。这种一个进程/线程处理多个 IO 的方式，被称为 I/O 复用模型。 而如果我们把发送就绪和接收就绪当成两类事件，只有在这两类事件发生的时候才会触发轮询，其它时候（比如等待请求时），程序不会被唤醒，那么这种方式就被称为事件驱动。 Linux 中的 select,poll,epoll 是典型的事件驱动的 I/O 复用模型： select() 会把所有的 I/O 请求封装为文件描述符 (fd) 的形式给操作系统，让操作系统内核去查询这些套接字上是否有事件发生，轮询完后，再将句柄数据复制到用户态，让服务器应用程序轮询处理已发生的网络事件，以此来实现一个线程/进程管理多个 I/O 的功能。 poll() 在 select 上支持更多数量的 fd。因为 select 中使用数组形式存放文件描述符，数量有限（一般 1024 个），poll 使用链表的形式，理论上支持的描述符数量没有上限。 epoll() 在 select/poll 的基础上有了大幅改进： 它使用红黑树来存储所有需要查询的事件，事件的添加和删除对应红黑树的插入和删除，复杂度从 O(N) 降为了 O(logN)。 它使用双向链表来保存就绪的事件。所有添加到红黑树上的事件都会与设备 (网卡) 驱动程序建立回调关系，当相应的事件发生时会调用这个回调方法，回调方法会把事件放入双向链表中。 返回时返回的是就绪事件（双向链表）而不是所有事件，既减少了内核到用户空间的拷贝数据量，又省了用户程序筛选就绪事件的时间。 相比 select/poll 的水平触发模式，epoll 也支持边沿触发模式。即用户可以选择到底是接受所有就绪的事件（水平触发），还是只接受上次检查以后新就绪的事件（边沿触发）。 Java 中的 NIO 模型Java 中的 NIO 模型选用了事件驱动的 I/O 复用模型。事实上，在 Linux 上 Java 的 NIO 就是基于 select,poll,epoll 来实现的（Linux 2.6 之前是 select、poll，2.6 之后是 epoll）。 在 Java 的 NIO 中，有 4 类事件：读就绪（OP_READ），写就绪（OP_WRITE），收到请求（仅服务端有效，OP_ACCEPT），发出请求（仅客户端有效，OP_CONNECT）。我们需要注册当这几个事件到来的时候所对应的处理器。然后在合适的时机告诉事件选择器：我对这个事件感兴趣。对于写操作，就是写不出去的时候对写事件感兴趣；对于读操作，就是完成连接和系统没有办法承载新读入的数据的时；对于 accept，一般是服务器刚启动的时候；而对于 connect，一般是 connect 失败需要重连或者直接异步调用 connect 的时候。新事件到来的时候，会在 selector 上注册标记位，标示可读、可写或者有连接到来。编程范式（伪代码）一般如下： 12345678910111213141516171819202122232425262728 //处理器抽象接口interface ChannelHandler&#123; void channelReadable(Channel channel); void channelWritable(Channel channel);&#125;class Channel&#123; Socket socket; Event event;//读，写或者连接&#125;Map&lt;Channel，ChannelHandler&gt; handlerMap;//所有 channel 的对应事件处理器//IO 线程主循环:class IoThread extends Thread&#123; public void run()&#123; Channel channel; while(channel=Selector.select())&#123;//选择就绪的事件和对应的连接 if(channel.event==accept)&#123; registerNewChannelHandler(channel);//如果是新连接，则注册一个新的读写处理器 &#125; if(channel.event==write)&#123; getChannelHandler(channel).channelWritable(channel);//如果可以写，则执行写事件 &#125; if(channel.event==read)&#123; getChannelHandler(channel).channelReadable(channel);//如果可以读，则执行读事件 &#125; &#125; &#125;&#125; Buffer 的选择Java 中的 NIO 还有一个特点是面向缓冲区的。这一特性其实在传统 IO 中就有用到，这里不再赘述。但是 Buffer 的选择也是一个值得注意的地方。 通常情况下，操作系统的一次写操作分为两步： 1. 将数据从用户空间拷贝到系统空间。 2. 从系统空间往网卡写。同理，读操作也分为两步： ① 将数据从网卡拷贝到系统空间； ② 将数据从系统空间拷贝到用户空间。 对于 NIO 来说，缓存的使用可以使用 DirectByteBuffer 和 HeapByteBuffer。如果使用了 DirectByteBuffer，一般来说可以减少一次系统空间到用户空间的拷贝。但 Buffer 创建和销毁的成本更高，更不宜维护，通常会用内存池来提高性能。如果数据量比较小的中小应用情况下，可以考虑使用 heapBuffer；反之可以用 directBuffer。 使用 NIO != 高性能，当连接数 &lt;1000，并发程度不高或者局域网环境下 NIO 并没有显著的性能优势。 NIO 并没有完全屏蔽平台差异，它仍然是基于各个操作系统的 I/O 系统实现的，差异仍然存在。使用 NIO 做网络编程构建事件驱动模型并不容易，陷阱重重。 推荐大家使用成熟的 NIO 框架，如 Netty，MINA 等。解决了很多 NIO 的陷阱，并屏蔽了操作系统的差异，有较好的性能和编程模型。 总结最后总结一下 Java 中的 NIO 为我们带来了什么： 非阻塞 I/O，I/O 读写不再阻塞，而是返回 0 避免多线程，单个线程可以处理多个任务 事件驱动模型 基于 block 的传输，通常比基于流的传输更高效 IO 多路复用大大提高了 Java 网络应用的可伸缩性和实用性","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"Java 线程池浅析","slug":"Java-线程池浅析","date":"2018-09-15T05:58:26.000Z","updated":"2019-03-19T01:51:38.188Z","comments":true,"path":"2018/09/15/Java-线程池浅析/","link":"","permalink":"http://yoursite.com/2018/09/15/Java-线程池浅析/","excerpt":"","text":"Java 中的线程池是运用场景最多的并发框架，几乎所有需要异步或并发执行任务的程序都可以使用线程池。在开发过程中，合理地使用线程池能够带来 3 个好处。 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。 线程池的继承关系如下图： 线程池最顶层是Executor，这是只有一个 execute 方法的接口，也是整个 Executor 框架的顶层接口，所有 Executor 框架的组件都要实现这个接口。 ExecutorService继承了 Executor，在此基础上增加了 submit(Runnable) 和 submit(Callable)，表示任务的提交，Runnable 和 Callable 的区别在于 Callable 的 call() 方法有返回值，而 Runnable 的 run 没有。 ThreadPoolExecutor是线程池的核心实现类，大部分线程池的功能都在这个类中被定义，它有多个参数和构造函数，根据不同的构造参数可以实现不同功能的线程池。线程池的参数会在下文详细介绍。 Executors是 ThreadPoolExecutor 的工厂类，封装了一些常用的线程池，具体类型也会在下文详细介绍。 线程池基本概念创建一个线程池我们可以通过 ThreadPoolExecutor 来创建一个线程池： 1new ThreadPoolExecutor(corePoolSize,maximumPoolSize,keepAliveTime,unit,workQueue,ThreadFactory,RejectedExecutionHandler) 线程池的构造函数中需要接收 7 个参数，它们分别是： corePoolSize 核心线程数，指保留的线程池大小（不超过 maximumPoolSize 值时，线程池中最多有 corePoolSize 个线程工作） maximumPoolSize 指的是线程池的最大大小（线程池中最大有 maximumPoolSize 个线程可运行） keepAliveTime 指的是空闲线程结束的超时时间（当一个线程不工作时，过 keepAliveTime 长时间将停止该线程） unit 是一个枚举，表示 keepAliveTime 的单位（有 NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS，7 个可选值） workQueue 表示存放任务的队列（存放需要被线程池执行的线程队列）。它的类型是 BlockingQueue 就是阻塞队列，有关阻塞队列的内容可以参考这篇《阻塞队列源码阅读》 threadFactory 是一个线程工厂，负责线程的创建，一般会使用默认的 Executors.defaultThreadFactory()。 handler 拒绝策略（添加任务失败后如何处理该任务） 线程池的运行策略线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。我们可以使用execute()方法提交任务到线程池： 123456executorService.execute(new Runnable() &#123; @Override public void run() &#123; //TODO &#125;&#125;); 也可以使用submit()方法提交任务到线程池： 123456Future future = executorService.submit(new Runnable() &#123; @Override public void run() &#123; //TODO &#125;&#125;); 区别在于 submit() 会返回一个 Future 对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get() 方法来获取返回值。另外继承了 ExecutorService 接口的 ScheduledExecutorService 还可以使用schedule()方法来提交一个定时任务： 123456scheduledExecutorService.schedule(new Runnable() &#123; @Override public void run() &#123; //TODO &#125;&#125;, 1, TimeUnit.SECONDS); 上面代码就会在 1 秒后执行我们的定时任务。无论是 submit() 还是 schedule()，其底层最后都会调用 execute() 来提交执行任务。不过，就算队列里面有任务，线程池也不会马上执行它们。 当添加一个任务时，线程池会做如下判断： 如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务； 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列； 如果这时候队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么还是要创建线程运行这个任务； 如果队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会调用 reject()，这个方法会调用 handler.rejectedExecution() 方法，根据不同的 handler 策略会有不同的处理方式。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。 线程池的拒绝策略上面提到任务添加失败后，线程池会调用 reject() 方法，这个方法会调用 handler.rejectedExecution() 方法，根据不同的 handler 策略会有不同的处理方式。线程池中预设有以下几种处理方式： AbortPolicy：为 Java 线程池默认的阻塞策略，不执行此任务，而且直接抛出一个运行时异常，切记 ThreadPoolExecutor.execute 需要 try catch，否则程序会直接退出。 DiscardPolicy：直接抛弃，任务不执行，空方法。 DiscardOldestPolicy：从队列里面抛弃 head 的一个任务，并再次 execute 此 task。 CallerRunsPolicy：还给原线程自己执行，会阻塞入口。 用户自定义拒绝策略：实现 RejectedExecutionHandler，并自己定义策略模式。 关闭线程池Java 线程池提供了两个方法用于关闭一个线程池，一个是 shutdownNow()，另一个是 shutdown()。我们可以看一下这两个方法的声明： 12void shutdown();List&lt;Runnable&gt; shutdownNow(); 这两个方法的区别在于： shutdown()：当线程池调用该方法时，线程池的状态则立刻变成 SHUTDOWN 状态。我们不能再往线程池中添加任何任务，否则将会抛出 RejectedExecutionException 异常；但是，此时线程池不会立刻退出，直到添加到线程池中的任务都已经处理完成后才会退出。 shutdownNow()：当执行该方法，线程池的状态立刻变成 STOP 状态，并试图停止所有正在执行的线程，不再处理还在池队列中等待的任务，并以返回值的形式返回那些未执行的任务。此方法会通过调用 Thread.interrupt() 方法来试图停止正在运行的 Worker 线程，但是这种方法的作用有限，如果线程中没有 sleep 、wait、Condition、定时锁等操作时，interrupt() 方法是无法中断当前的线程的。所以，shutdownNow() 并不代表线程池就一定立即就能退出，可能必须要等待所有正在执行的任务都执行完成了才能退出。 Executors 提供的线程池ThreadPoolExecutor 提供的线程创建方式参数太多，对开发人员并不友好。因此 Java 在 Executors 类中封装了几种常用的线程池，它们分别是： Executors.newCachedThreadPool 这是一个会根据需要创建线程的线程池，它的 corePoolSize 被设置为 0，maximumPoolSize 被设置为 Integer.MAX_VALUE，KeepAliveTime 被设置为 60s，使用没有容量的 SynchronousQueue 作为线程池的工作队列。这就意味着，线程池中没有固定的线程数量，任何一个任务被提交时，线程池都会为它创建或者分配一个线程；而任何一个线程空闲时间超过 60s，都会关闭它。使用该线程池时要注意主线程提交任务的速度和线程池处理任务的速度，若提交速度大于处理速度，CachedThreadPool 会因为创建过多线程而耗尽 CPU 和内存资源。该线程池的吞吐量在几种预设线程池中是最大的。 Executors.newFixedThreadPool 这是被称为可重用固定线程数的线程池，它的 corePoolSize 等于 maximumPoolSize，KeepAliveTime 被设置为 0，使用最大长度的有界队列 LinkedBlockingQueue（队列容量为 Integer.MAX_VALUE）作为工作队列，这也意味着 FixedThreadPool 运行稳定后线程数量是不变的，且所有任务都会进入工作队列，不会拒绝任务。 Executors.newSingleThreadExecutor 这是一个只有一个工作线程的线程池，它的 corePoolSize 和 maximumPoolSize 都被设置为 1，其它参数与 FixedThreadPool 相同，可以把它理解为 newFixedThreadPool(1)，线程执行完任务后会无限反复从 LinkedBlockingQueue 获取任务来执行。 Executors.newScheduledThreadPool 这是一个可以定时执行的线程池，它的 maximumPoolSize 被设置为 Integer.MAX_VALUE，KeepAliveTime 被设置为 10ms，使用 DelayedWorkQueue 作为阻塞队列，这是一个类似于 DelayedQueue 和 PriorityBlockingQueue 的阻塞队列，每次会取出队列中执行时间最早的任务，如果没有到执行之间，则 await 两者的差值，以此来达到定时执行的目的。同时 newScheduledThreadPool 的 scheduleAtFixedRate 和 scheduleWithFixedRate 方法还可以实现周期执行的功能。两者都是依靠给 ScheduledFutureTask（newScheduledThreadPool 中被执行的任务）设置下一次执行时间来实现的。区别在于 scheduleAtFixedRate 中下次执行时间=本次开始时间+间隔时间，而 scheduleWithFixedRate 中下次执行时间=本次结束时间+间隔时间。 线程池代码分析线程池的属性字段在开始深入了解 ThreadPoolExecutor 代码之前, 我们先来简单地看一下 ThreadPoolExecutor 类中到底有哪些重要的字段。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class ThreadPoolExecutor extends AbstractExecutorService &#123; // 这个是一个复用字段, 它复用地表示了当前线程池的状态, 当前线程数信息. private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); // 用于存放提交到线程池中, 但是还未执行的那些任务. private final BlockingQueue&lt;Runnable&gt; workQueue; // 线程池内部锁, 对线程池内部操作加锁, 防止竞态条件 private final ReentrantLock mainLock = new ReentrantLock(); // 一个 Set 结构, 包含了当前线程池中的所有工作线程. // 对 workers 字段的操作前, 需要获取到这个锁. private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); // 条件变量, 用于支持 awaitTermination 操作 private final Condition termination = mainLock.newCondition(); // 记录线程池中曾经到达过的最大的线程数. // 这个字段在获取 mainLock 锁的前提下才能操作. private int largestPoolSize; // 记录已经完成的任务数. 仅仅当工作线程结束时才更新此字段. // 这个字段在获取 mainLock 锁的前提下才能操作. private long completedTaskCount; // 线程工厂. 当需要一个新的线程时, 这里生成. private volatile ThreadFactory threadFactory; // 任务提交失败后的处理 handler private volatile RejectedExecutionHandler handler; // 空闲线程的等待任务时间, 以纳秒为单位. // 当当前线程池中的线程数大于 corePoolSize 时, // 或者 allowCoreThreadTimeOut 为真时, 线程才有 idle 等待超时时间, // 如果超时则此线程会停止.; // 反之线程会一直等待新任务到来. private volatile long keepAliveTime; // 默认为 false. // 当为 false 时, keepAliveTime 不起作用, 线程池中的 core 线程会一直存活, // 即使这些线程是 idle 状态. // 当为 true 时, core 线程使用 keepAliveTime 作为 idle 超时 // 时间来等待新的任务. private volatile boolean allowCoreThreadTimeOut; // 核心线程数. private volatile int corePoolSize; // 最大线程数. private volatile int maximumPoolSize;&#125; ThreadPoolExecutor 中, 使用到 ctl 这个字段来维护线程池中当前线程数和线程池的状态。ctl 是一个 AtomicInteger 类型, 它的低 29 位用于存放当前的线程数，因此一个线程池在理论上最大的线程数是 536870911；高 3 位是用于表示当前线程池的状态，其中高三位的值和状态对应如下： 111: RUNNING 此时能够接收新任务，以及对已添加的任务进行处理。状态切换：线程池初始化时就是 RUNNING 状态。 000: SHUTDOWN 此时不接收新任务，但能处理已添加的任务。状态切换：调用线程池的 shutdown() 接口时，线程池由 RUNNING -&gt; SHUTDOWN。 001: STOP 此时不接收新任务，不处理已添加的任务，并且会中断正在处理的任务。状态切换：调用线程池的 shutdownNow() 接口时，线程池由 (RUNNING or SHUTDOWN ) -&gt; STOP。 010: TIDYING 当所有的任务已终止，ctl 记录的”任务数量”为 0，线程池会变为 TIDYING 状态。当线程池变为 TIDYING 状态时，会执行钩子函数 terminated()。terminated() 在 ThreadPoolExecutor 类中是空的，若用户想在线程池变为 TIDYING 时，进行相应的处理；可以通过重载 terminated() 函数来实现。状态切换：当线程池在 SHUTDOWN 状态下，阻塞队列为空并且线程池中执行的任务也为空时，就会由 SHUTDOWN -&gt; TIDYING。当线程池在 STOP 状态下，线程池中执行的任务为空时，就会由 STOP -&gt; TIDYING。 011: TERMINATED 线程池彻底终止，就变成 TERMINATED 状态。状态切换：线程池处在 TIDYING 状态时，执行完 terminated() 之后，就会由 TIDYING -&gt; TERMINATED。 提交任务到线程池123456789101112131415161718192021public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) // 策略模式，调用传入的 RejectedExecutionHandler 的 rejectedExecution 方法 reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command);&#125; 上面的代码有三个步骤，首先第一步是检查当前线程池的线程数是否小于 corePoolSize，如果小于，那么由我们前面提到的规则，线程池会创建一个新的线程来执行此任务，因此在第一个 if 语句中，会调用 addWorker(command, true) 来创建一个新 Worker 线程，并执行此任务。 如果当前线程池的线程数不小于 corePoolSize，那么会尝试将此任务插入到工作队列中，即 workQueue.offer(command)。当插入到 workQueue 成功后，我们还需要再次检查一下此时线程池是否还是 RUNNING 状态，如果不是的话就会将原来插入队列中的那个任务删除，然后调用 reject 方法拒绝此任务的提交；接着考虑到在我们插入任务到 workQueue 中的同时，如果此时线程池中的线程都执行完毕并终止了，在这样的情况下刚刚插入到 workQueue 中的任务就永远不会得到执行了。为了避免这样的情况，因此我们要再次检查一下线程池中的线程数，如果为零，则调用 addWorker(null, false) 来添加一个线程。 最后如果任务插入到工作队列失败了，就会直接调用 addWorker(command, false) 来新开一个线程。如果失败了，那么我们就知道线程池已经关闭或者饱和了，就拒绝这次添加。 关于 addWorker 方法前面我们大体分析了一下 execute 提交任务的流程，不过省略了一个关键步骤，即 addWorker 方法。现在我们来看看这个方法里究竟发生了什么。 首先看一下 addWorker 方法的签名：1private boolean addWorker(Runnable firstTask, boolean core) 这个方法接收两个参数，第一个是一个 Runnable 类型的参数，一般来说是我们调用 execute 方法所传输的参数，不过也有可能是 null 值，这样的情况我们在前面一小节中也见到过。那么第二个参数是做什么的呢？第二个参数是一个 boolean 类型的变量，它的作用是标识是否使用 corePoolSize 属性。我们知道，ThreadPoolExecutor 中，有一个 corePoolSize 属性，用于规定线程池中的核心线程数。那么当 core 这个参数是 true 时，则表示在添加新任务时，需要考虑到 corePoolSzie 的影响（例如如果此时线程数已经大于 corePoolSize 了，那么就不能再添加新线程了）；当 core 为 false 时，就不考虑 corePoolSize 的影响，而是以 maximumPoolSize 代替 corePoolSize 来做判断条件。 然后是 addWorker 的源码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (int c = ctl.get();;) &#123; // Check if queue empty only if necessary. if (runStateAtLeast(c, SHUTDOWN) &amp;&amp; (runStateAtLeast(c, STOP) || firstTask != null || workQueue.isEmpty())) return false; for (;;) &#123; // 当 core 为真, 那么就判断当前线程是否大于 corePoolSize // 当 core 为假, 那么就判断当前线程数是否大于 maximumPoolSize // 这里的 for 循环是一个自旋 CAS(CompareAndSwap) 操作, 用于确保多线程环境下的正确性 if (workerCountOf(c) &gt;= ((core ? corePoolSize : maximumPoolSize) &amp; COUNT_MASK)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateAtLeast(c, SHUTDOWN)) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int c = ctl.get(); if (isRunning(c) || (runStateLessThan(c, STOP) &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 首先在 addWorker 的一开始，有一个 for 循环，用于判断当前是否可以添加新的 Worker 线程。它的逻辑如下： 如果传入的 core 为真，那么判断当前的线程数是否大于 corePoolSize，如果大于，则不能新建 Worker 线程，返回 false。 如果传入的 core 为假，那么判断当前的线程数是否大于 maximumPoolSize，如果大于，则不能新建 Worker 线程，返回 false。 如果条件符合，那么在 for 循环内，又有一个自旋 CAS 更新逻辑，用于递增当前的线程数，即 compareAndIncrementWorkerCount(c)，这个方法会原子地更新 ctl 的值，将当前线程数的值+1。addWorker 接下来有一个 try…finally 语句块，这里就是实际上的创建线程、启动线程、添加线程到线程池中的工作了。首先可以看到 w = new Worker(firstTask)；这里是实例化一个 Worker 对象，这个类其实就是 ThreadPoolExecutor 中对工作线程的封装。Worker 类继承于 AbstractQueuedSynchronizer 并实现了 Runnable 接口，我们来看一下它的构造器： 12345Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this);&#125; 它会把我们提交的任务（firstTask）设置为自己的内部属性 firstTask，然后使用 ThreadPoolExecutor 中的 threadFactory 来创建一个新的线程，并保存在 thread 字段中，而且注意到，创建线程时，我们传递给新线程的 Runnable 其实是 Worker 对象本身（this），因此当这个线程启动时，实际上运行的是 Worker.run() 中的代码。 回过头来再看一下 addWorker 方法。当创建好 Worker 线程后，就会将这个 worker 线程存放在 workers 这个 HashSet 类型的字段中。而且注意到，正如我们在前面所提到的，mainLock 是 ThreadPoolExecutor 的内部锁，我们对 ThreadPoolExecutor 中的字段进行操作时，为了保证线程安全，需要在获取到 mainLock 的前提下才能操作。 最后，我们可以看到，在 addWorker 方法的最后，调用了 t.start()；来真正启动这个新建的线程。 任务的分配与调度线程池在执行完 firstTask 后并不会立即销毁，而是可以根据情况复用。线程的复用就涉及到任务的分配与调度。Java 线程池的调度方式很简单，就是执行完之后从 workQueue 中拿出下一个任务，如果获取到了任务，那就再次执行。 前一小节中，我们看到 addWorker 中会新建一个 Worker 对象来代表一个 worker 线程，接着会调用线程的 start() 来启动这个线程，我们也提到了当启动这个线程后，会运行到 Worker 中的 run 方法，我们来看一下 Worker.run 具体的实现：123public void run() &#123; runWorker(this);&#125; Worker.run 方法很简单，只是调用了 ThreadPoolExecutor.runWorker 方法而已。runWorker 方法比较关键，它是整个线程池任务分配的核心：12345678910111213141516171819202122232425262728293031323334353637383940414243final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125; runWorker 方法是整个工作线程的核心循环，在这个循环中，工作线程会不断的从 workerQuque 中获取新的 task，然后执行它。我们注意到在 runWorker 一开始，有一个 w.unlock()，咦, 这是为什么呢? 其实这是 Worker 类玩的一个小把戏。回想一下，Worker 类继承于 AQS 并实现了 Runnable 接口，它的构造器如下：12345Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this);&#125; setState(-1) 方法是 AQS 提供的，初始化 Worker 时，会先设置 state 为 -1，根据注释，这样做的原因是为了抑制工作线程的 interrupt 信号，直到此工作线程开始执行 task。那么在 addWorker 中的 w.unlock() 就是允许 Worker 的 interrupt 信号。 接着在 addWorker 中会进入一个 while 循环，在这里此工作线程会不断地从 workQueue 中取出一个任务，然后调用 task.run() 来执行这个任务，因此就执行到了用户所提交的 Runnable 中的 run() 方法了。 工作线程的 idle 超时处理工作线程的 idle 超出处理在底层依赖于 BlockingQueue 带超时的 poll 方法，即工作线程会不断地从 workQueue 这个 BlockingQueue 中获取任务，如果 allowCoreThreadTimeOut 字段为 true，或者当前的工作线程数大于 corePoolSize，那么线程的 idle 超时机制就生效了，此时工作线程会以带超时的 poll 方式从 workQueue 中获取任务。当超时了还没有获取到任务，那么我们就知道此线程已经到达 idle 超时时间了，就终止此工作线程。具体源码如下：12345678910111213141516171819202122232425262728293031323334353637private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; 从源码中就可以看到，一开始会判断当前的线程池状态，如果不是 SHUTDOWN 或 STOP 之类的状态，那么接着获取当前的工作线程数，然后判断工作线程数量是否已经大于了 corePoolSize。当 allowCoreThreadTimeOut 字段为 true，或者当前的工作线程数大于 corePoolSize，那么线程的 idle 超时机制就生效，此时工作线程会以带超时的 workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) 方式从 workQueue 中获取任务；反之会以 workQueue.take() 方式阻塞等待任务，直到获取一个新的任务。当从 workQueue 获取新任务超时时，会调用 compareAndDecrementWorkerCount 将当前的工作线程数-1，并返回 null。getTask 方法返回 null 后， runWorker 中的 while 循环自然也就结束了，因此也导致了 runWorker 方法的返回，最后自然整个工作线程的 run() 方法执行完毕，工作线程自然就终止了。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"Java 阻塞队列浅析","slug":"Java-阻塞队列浅析","date":"2018-09-03T13:58:36.000Z","updated":"2019-03-15T16:55:05.940Z","comments":true,"path":"2018/09/03/Java-阻塞队列浅析/","link":"","permalink":"http://yoursite.com/2018/09/03/Java-阻塞队列浅析/","excerpt":"","text":"一、BlockingQueue 接口Java 中的阻塞队列都是实现 BlockingQueue 接口的实现类。BlockingQueue 继承自 Queue，所以其实现类也可以作为 Queue 的实现来使用，而 Queue 又继承自 Collection 接口。所以 BlockingQueue 可以使用 Queue 和 Collection 的方法。 但一般我们使用 BlockingQueue 是因为它提供了获取队列元素但是队列为空时，会阻塞等待队列中有元素再返回和添加元素时，如果队列已满，那么等到队列可以放入新元素时再放入这两种功能。 BlockingQueue 对插入操作、移除操作、获取元素操作提供了四种不同的处理方式：1、抛出异常；2、返回特殊值（null 或 true/false，取决于具体的操作）；3、一直阻塞；4、超时退出。总结如下： 抛出异常 返回特殊值 一直阻塞 超时退出 插入 add(e) offer(e) put(e) offer(e, time, unit) 移除 remove() poll() take() poll(time, unit) 获取元素 element() peek() / / 我们需要关注的应该是 put(e) 和 take() 这两个方法，因为这两个方法是带阻塞的，是 BlockingQueue 的核心功能。 二、BlockingQueue 的实现JDK7 中提供了 7 个阻塞队列，分别是： ArrayBlockingQueue ：底层是数组，有界队列，元素按照先进先出排列。 LinkedBlockingQueue ：底层是单向链表，可以当做无界和有界队列来使用，元素按照先进先出排列。 PriorityBlockingQueue ：底层是数组，无界队列，元素按优先级排列，基于最小堆实现。 DelayQueue：底层是数组，无界队列，元素按可取出时间排列（元素在入队时需要指定一个延迟期，表示延迟多久才能从队列中取出），基于最小堆实现。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：底层是单向链表，无界队列，和 LinkedBlockingQueue 的不同之处在于多了 transfer 和 tryTransfer，允许元素在入队之前直接传输给消费者。 LinkedBlockingDeque：底层是双向链表，无界队列，可以在两端插入和移出元素。 1、ArrayBlockingQueueArrayBlockingQueue 是一个用数组实现的有界阻塞队列。此队列按照先进先出的原则对元素进行排序。可以支持公平和非公平两种模式，公平性是由 ReentrantLock 实现的。在非公平模式下，当一个线程请求锁时，如果在发出请求的同时锁变成可用状态，那么这个线程会跳过队列中所有的等待线程而获得锁。ArrayBlockingQueue 的属性如下： 12345678910111213// 用于存放元素的数组final Object[] items;// 下一次读取操作的位置int takeIndex;// 下一次写入操作的位置int putIndex;// 队列中的元素数量int count;// 以下几个就是控制并发用的同步器，公平性是由 ReentrantLock 实现的final ReentrantLock lock;private final Condition notEmpty;private final Condition notFull; ArrayBlockingQueue 实现并发同步的原理是：读操作和写操作都需要获取到 AQS 独占锁才能进行操作，如果队列为空，这个时候读操作的线程进入到读线程队列排队，等待写线程写入新的元素，然后唤醒读线程队列的第一个等待线程；如果队列已满，这个时候写操作的线程进入到写线程队列排队，等待读线程将队列元素移除腾出空间，然后唤醒写线程队列的第一个等待线程。 2、LinkedBlockingQueue底层基于单向链表实现的阻塞队列，可以当做无界队列也可以当做有界队列来使用。看构造方法：1234567891011// 无界队列public LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE);&#125;// 有界队列public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125; LinkedBlockingQueue 的属性：1234567891011121314151617181920// 队列容量private final int capacity;// 队列中的元素数量private final AtomicInteger count = new AtomicInteger(0);// 队头private transient Node&lt;E&gt; head;// 队尾private transient Node&lt;E&gt; last;// take, poll, peek 等读操作的方法需要获取到这个锁private final ReentrantLock takeLock = new ReentrantLock();// 如果读操作的时候队列是空的，那么等待 notEmpty 条件private final Condition notEmpty = takeLock.newCondition();// put, offer 等写操作的方法需要获取到这个锁private final ReentrantLock putLock = new ReentrantLock();// 如果写操作的时候队列是满的，那么等待 notFull 条件private final Condition notFull = putLock.newCondition(); 可以看到 LinkedBlockingQueue 使用了双锁队列来提高队列吞吐量。双锁的使用如下： takeLock 和 notEmpty：如果要获取（take）一个元素，需要获取 takeLock 锁，但是获取了锁还不够，如果队列此时为空，还需要队列不为空（notEmpty）这个条件（Condition）。 putLock 和 notFull：如果要插入（put）一个元素，需要获取 putLock 锁，但是获取了锁还不够，如果队列此时已满，还需要队列不是满的（notFull）这个条件（Condition）。 为什么 ArrayBlockingQueue 不使用双锁呢?这是因为ArrayBlockingQueue 本身的入队和出队操作已经足够轻快了。LinkedBlockingQueue 使用双锁是因为 LinkedBlockingQueue 添加需要构造节点，导致较长的等待，所以同时存取有较大优化。 而 ArrayBlockingQueue 本身的入队和出队操作就足够轻快，转成双锁之后，对比原来的存取操作，需要多竞争两次。一次是 count 变量的 cas 操作，另一次是获得另一把锁的通知操作，这部分的损耗要比并发存取带来的收益更大。 3、PriorityBlockingQueue带排序的 BlockingQueue 实现，其并发控制采用的是 ReentrantLock，队列为无界队列，开始的时候可以指定初始的队列大小，后面插入元素的时候，如果空间不够的话会自动扩容。 简单地说，它就是 PriorityQueue 的线程安全版本。不可以插入 null 值，同时，插入队列的对象必须是可比较大小的（comparable），否则报 ClassCastException 异常。它的插入操作 put 方法不会 block，因为它是无界队列（take 方法在队列为空的时候会阻塞）。 它的属性如下：1234567891011121314151617181920212223242526// 构造方法中，如果不指定大小的话，默认大小为 11private static final int DEFAULT_INITIAL_CAPACITY = 11;// 数组的最大容量private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;// 这个就是存放数据的数组private transient Object[] queue;// 队列当前大小private transient int size;// 大小比较器，如果按照自然序排序，那么此属性可设置为 nullprivate transient Comparator&lt;? super E&gt; comparator;// 并发控制所用的锁，所有的 public 且涉及到线程安全的方法，都必须先获取到这个锁private final ReentrantLock lock;// 由上面的 lock 属性创建，取出时需要判断队列是否为空private final Condition notEmpty;// 这个也是用于锁，用于数组扩容的时候，需要先获取到这个锁，才能进行扩容操作// 其使用 CAS 操作private transient volatile int allocationSpinLock;// 用于序列化和反序列化的时候用，对于 PriorityBlockingQueue 我们应该比较少使用到序列化private PriorityQueue q; 此类实现了 Collection 和 Iterator 接口中的所有接口方法，对其对象进行迭代并遍历时，不能保证有序性。如果你想要实现有序遍历，建议采用 Arrays.sort(queue.toArray()) 进行处理。PriorityBlockingQueue 提供了 drainTo 方法用于将部分或全部元素有序地填充（准确说是转移，会删除原队列中的元素）到另一个集合中。还有一个需要说明的是，如果两个对象的优先级相同（compare 方法返回 0），此队列并不保证它们之间的顺序。 PriorityBlockingQueue 使用了基于数组的最小堆来存放元素，所有的 public 方法采用同一个 lock 进行并发控制。 我们来看一下它的构造方法：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 默认构造方法，采用默认值(11)来进行初始化public PriorityBlockingQueue() &#123; this(DEFAULT_INITIAL_CAPACITY, null);&#125;// 指定数组的初始大小public PriorityBlockingQueue(int initialCapacity) &#123; this(initialCapacity, null);&#125;// 指定比较器public PriorityBlockingQueue(int initialCapacity, Comparator&lt;? super E&gt; comparator) &#123; if (initialCapacity &lt; 1) throw new IllegalArgumentException(); this.lock = new ReentrantLock(); this.notEmpty = lock.newCondition(); this.comparator = comparator; this.queue = new Object[initialCapacity];&#125;// 在构造方法中就先填充指定的集合中的元素public PriorityBlockingQueue(Collection&lt;? extends E&gt; c) &#123; this.lock = new ReentrantLock(); this.notEmpty = lock.newCondition(); // boolean heapify = true; // true if not known to be in heap order boolean screen = true; // true if must screen for nulls if (c instanceof SortedSet&lt;?&gt;) &#123; SortedSet&lt;? extends E&gt; ss = (SortedSet&lt;? extends E&gt;) c; this.comparator = (Comparator&lt;? super E&gt;) ss.comparator(); heapify = false; &#125; else if (c instanceof PriorityBlockingQueue&lt;?&gt;) &#123; PriorityBlockingQueue&lt;? extends E&gt; pq = (PriorityBlockingQueue&lt;? extends E&gt;) c; this.comparator = (Comparator&lt;? super E&gt;) pq.comparator(); screen = false; if (pq.getClass() == PriorityBlockingQueue.class) // exact match heapify = false; &#125; Object[] a = c.toArray(); int n = a.length; // If c.toArray incorrectly doesn't return Object[], copy it. if (a.getClass() != Object[].class) a = Arrays.copyOf(a, n, Object[].class); if (screen &amp;&amp; (n == 1 || this.comparator != null)) &#123; for (int i = 0; i &lt; n; ++i) if (a[i] == null) throw new NullPointerException(); &#125; this.queue = a; this.size = n; if (heapify) heapify();&#125; 接下来是内部的自动扩容实现：123456789101112131415161718192021222324252627282930313233343536373839private void tryGrow(Object[] array, int oldCap) &#123; // 释放了原来的独占锁 lock，这样的话，扩容操作和读操作可以同时进行，提高吞吐量 lock.unlock(); // must release and then re-acquire main lock Object[] newArray = null; // 用 CAS 操作将 allocationSpinLock 由 0 变为 1，也算是获取锁 if (allocationSpinLock == 0 &amp;&amp; UNSAFE.compareAndSwapInt(this, allocationSpinLockOffset, 0, 1)) &#123; try &#123; // 如果节点个数小于 64，那么增加 oldCap + 2 的容量 // 如果节点数大于等于 64，那么增加 oldCap 的一半 // 所以节点数较小时，增长得快一些 int newCap = oldCap + ((oldCap &lt; 64) ? (oldCap + 2) : (oldCap &gt;&gt; 1)); // 这里有可能溢出 if (newCap - MAX_ARRAY_SIZE &gt; 0) &#123; // possible overflow int minCap = oldCap + 1; if (minCap &lt; 0 || minCap &gt; MAX_ARRAY_SIZE) throw new OutOfMemoryError(); newCap = MAX_ARRAY_SIZE; &#125; // 如果 queue != array，那么说明有其他线程给 queue 分配了其他的空间 if (newCap &gt; oldCap &amp;&amp; queue == array) // 分配一个新的大数组 newArray = new Object[newCap]; &#125; finally &#123; // 重置，也就是释放锁 allocationSpinLock = 0; &#125; &#125; // 如果有其他的线程也在做扩容的操作 if (newArray == null) // back off if another thread is allocating Thread.yield(); // 重新获取锁 lock.lock(); // 将原来数组中的元素复制到新分配的大数组中 if (newArray != null &amp;&amp; queue == array) &#123; queue = newArray; System.arraycopy(array, 0, newArray, 0, oldCap); &#125;&#125; 下面，我们来分析下入队操作 put 方法和出队操作 take 方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// 入队操作public void put(E e) &#123; // 直接调用 offer 方法，因为前面我们也说了，在这里，put 方法不会阻塞 offer(e); &#125;public boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); final ReentrantLock lock = this.lock; // 首先获取到独占锁 lock.lock(); int n, cap; Object[] array; // 如果当前队列中的元素个数 &gt;= 数组的大小，那么需要扩容了 while ((n = size) &gt;= (cap = (array = queue).length)) tryGrow(array, cap); try &#123; Comparator&lt;? super E&gt; cmp = comparator; // 节点添加到最小堆中 if (cmp == null) siftUpComparable(n, e, array); else siftUpUsingComparator(n, e, array, cmp); // 更新 size size = n + 1; // 唤醒等待的读线程 notEmpty.signal(); &#125; finally &#123; lock.unlock(); &#125; return true;&#125;// 出队操作public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; // 独占锁 lock.lockInterruptibly(); E result; try &#123; // dequeue 出队 while ( (result = dequeue()) == null) notEmpty.await(); &#125; finally &#123; lock.unlock(); &#125; return result;&#125;private E dequeue() &#123; int n = size - 1; if (n &lt; 0) return null; else &#123; Object[] array = queue; // 取出队头 E result = (E) array[0]; // 取出队尾，调整时放到队头 E x = (E) array[n]; // 队尾置空 array[n] = null; Comparator&lt;? super E&gt; cmp = comparator; if (cmp == null) // 使用 Comparable 调整最小堆 siftDownComparable(0, x, array, n); else // 使用 Comparator 调整最小堆 siftDownUsingComparator(0, x, array, n, cmp); size = n; return result; &#125;&#125; 可以看出， put 和 take 都是基于最小堆操作的。 4、DelayQueueDelayQueue 的功能在某种程度上和 PriorityBlockingQueue 有点类似。在 PriorityBlockingQueue 中元素会按照优先级排序，而在 DelayQueue 中，元素的优先级被固定为可取出的时间。队列中的元素按可取时间排序，越早的越靠前。DelayQueue 的排序功能依赖 PriorityQueue 实现，这个队列我们提到过，是 PriorityBlockingQueue 的非线程安全版。但和 PriorityBlockingQueue 不一样的是，当有线程从 DelayQueue 中取元素（take）时，会被阻塞一直到队首元素可取出。DelayQueue 可以被用于以下地方： 缓存系统的设计：可以用 DelayQueue 保存缓存元素的有效期，使用一个线程循环查询 DelayQueue，一旦能从 DelayQueue 中获取元素时，表示缓存有效期到了。 定时任务调度。使用 DelayQueue 保存当天将会执行的任务和执行时间，一旦从 DelayQueue 中获取到任务就开始执行，从比如 TimerQueue 就是使用 DelayQueue 实现的。 我们看一下入队操作 take 的实现：123456789101112131415161718192021222324252627282930313233343536373839public E take() throws InterruptedException &#123; // 所有 public 方法的同步由 lock 保障 final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; for (;;) &#123; E first = q.peek(); if (first == null) // 队列为空，要等待有元素入队 available.await(); else &#123; // 获取队首元素的可取时间距离当前时间的间隔 long delay = first.getDelay(NANOSECONDS); if (delay &lt;= 0) // 可取时间早于当前时间，直接取出 return q.poll(); first = null; // don't retain ref while waiting if (leader != null) // 有其它更早的线程，当前线程等待 available.await(); else &#123; Thread thisThread = Thread.currentThread(); leader = thisThread; try &#123; // 有限等待，等到队首元素可取 available.awaitNanos(delay); &#125; finally &#123; if (leader == thisThread) leader = null; &#125; &#125; &#125; &#125; &#125; finally &#123; if (leader == null &amp;&amp; q.peek() != null) available.signal(); lock.unlock(); &#125;&#125; getDelay 是一个需要队列中的元素自己实现的接口，我们以 ScheduledThreadPoolExecutor 里 ScheduledFutureTask 类为例。这个类实现了 Delayed 接口：1234567891011ScheduledFutureTask(Runnable r, V result, long ns, long period) &#123; super(r, result); this.time = ns; // 这个 time 就是可取时间 this.period = period; this.sequenceNumber = sequencer.getAndIncrement();&#125;public long getDelay(TimeUnit unit) &#123; // 返回的是可取时间 - 当前时间 return unit.convert(time - now(), TimeUnit.NANOSECONDS);&#125; 5、SynchronousQueueSynchronousQueue 是一个不存储元素的阻塞队列。每一个 put 操作必须等待一个 take 操作，否则不能继续添加元素。SynchronousQueue 可以看成是一个传球手，负责把生产者线程处理的数据直接传递给消费者线程。队列本身并不存储任何元素，非常适合于传递性场景，比如在一个线程中使用的数据，传递给另外一个线程使用，SynchronousQueue 的吞吐量高于 LinkedBlockingQueue 和 ArrayBlockingQueue。 6、LinkedTransferQueueLinkedTransferQueue 是一个由链表结构组成的无界阻塞 TransferQueue 队列。TransferQueue 队列继承了 BlockingQueue，在 BlockingQueue 的基础上多了 tryTransfer 和 transfer 方法。transfer 会在元素进入阻塞队列前，先判断有没有消费者线程在等待获取，若有，则直接移交；否则将元素插入到队列尾部。tryTransfer 相比 transfer 少了插入的步骤，多了返回值 boolean，即若当前没有消费者线程空闲，则直接返回 false，不会插入到阻塞队列。 7、LinkedBlockingDequeLinkedBlockingDeque 是一个由链表结构组成的双向阻塞队列。所谓双向队列指的你可以从队列的两端插入和移出元素。双端队列因为多了一个操作队列的入口，在多线程同时入队时，也就减少了一半的竞争。相比其他的阻塞队列，LinkedBlockingDeque 多了 addFirst，addLast，offerFirst，offerLast，peekFirst，peekLast 等方法，以 First 单词结尾的方法，表示插入，获取（peek）或移除双端队列的第一个元素。以 Last 单词结尾的方法，表示插入，获取或移除双端队列的最后一个元素。另外插入方法 add 等同于 addLast，移除方法 remove 等效于 removeFirst。但是 take 方法却等同于 takeFirst，不知道是不是 Jdk 的 bug，使用时还是用带有 First 和 Last 后缀的方法更清楚。在初始化 LinkedBlockingDeque 时可以初始化队列的容量，用来防止其再扩容时过渡膨胀。另外双向阻塞队列可以运用在“工作窃取”模式中。 三、阻塞队列的实现原理通过上面的分析我们知道了，阻塞队列之所以能实现获取队列元素但是队列为空时，会阻塞等待队列中有元素再返回和添加元素时，如果队列已满，那么等到队列可以放入新元素时再放入这两种功能，主要依赖于 AQS 提供的 Condition 这个工具。当队列满时，我们可以让线程 await 于一个 notFull Condition，它会等待另一个线程的 notFull signal；当队列空时，我们可以让线程 await 于一个 notEmpty Condition，它会等待另一个线程的 notEmpty signal。那么 Condition 的 await 和 signal 在底层是怎么实现的呢？先来看一下 await 的源码：123456789101112131415161718192021222324// awaitpublic final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 加入等待队列 Node node = addConditionWaiter(); // 释放同步状态（锁） int savedState = fullyRelease(node); int interruptMode = 0; // 判断节点是否在同步队列中 while (!isOnSyncQueue(node)) &#123; // 核心部分，阻塞（和 Object.wait(0) 一样进入无限等待状态） LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 退出 while 循环说明节点被 signal() 调入同步队列中，调用 acquireQueued() 加入同步状态竞争，竞争到锁后从 await() 方法返回 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 同步队列是 AQS 提供的等待同一个 Lock 的数据结构，等待队列也是 AQS 提供的用于等待同一个 Condition 的数据结构，它们的关系如图： 一个 Lock 底下可以挂载多个 Condition，这些 Condition 之间彼此独立。当 await 调用时，相应的节点会从同步队列中取下，放入等待队列中。当 signal 被调用时，等待队列的队首节点会被取出，重新加入同步队列中；当 signalAll 被调用时，等待队列的所有节点会被取出，加入同步队列。 await 方法的核心部分是第 13 行的 LockSupport.park() 函数，该函数表示阻塞当前线程，它的源码如下：1234567public static void park(Object blocker) &#123; Thread t = Thread.currentThread(); setBlocker(t, blocker); // 核心部分 UNSAFE.park(false, 0L); setBlocker(t, null);&#125; 函数的核心部分是第 5 行 UNSAFE.park() 函数，这是一个 native 方法，park 这个方法会阻塞当前线程，只有以下四种情况中的一种发生时，该方法才会返回。 与 park 对应的 unpark 执行或已经执行时。 线程被中断时。 如果参数中的 time 不是零，等待了指定的毫秒数时。 发生异常现象时。这些异常事先无法确定。 JVM 中 park 在不同的操作系统使用不同的方式实现，在 linux 下是使用的是系统方法 pthread_cond_wait 实现。1234567891011121314151617181920212223242526272829303132void os::PlatformEvent::park() &#123; int v ; for (;;) &#123; v = _Event ; if (Atomic::cmpxchg (v-1, &amp;_Event, v) == v) break ; &#125; guarantee (v &gt;= 0, \"invariant\") ; if (v == 0) &#123; // Do this the hard way by blocking ... int status = pthread_mutex_lock(_mutex); assert_status(status == 0, status, \"mutex_lock\"); guarantee (_nParked == 0, \"invariant\") ; ++ _nParked ; while (_Event &lt; 0) &#123; // 核心部分 status = pthread_cond_wait(_cond, _mutex); // for some reason, under 2.7 lwp_cond_wait() may return ETIME ... // Treat this the same as if the wait was interrupted if (status == ETIME) &#123; status = EINTR; &#125; assert_status(status == 0 || status == EINTR, status, \"cond_wait\"); &#125; -- _nParked ; // In theory we could move the ST of 0 into _Event past the unlock(), // but then we'd need a MEMBAR after the ST. _Event = 0 ; status = pthread_mutex_unlock(_mutex); assert_status(status == 0, status, \"mutex_unlock\"); &#125; guarantee (_Event &gt;= 0, \"invariant\") ; &#125; &#125; pthread_cond_wait 是一个多线程的条件变量函数，_cond 是 condition 的缩写，字面意思可以理解为线程在等待一个条件发生，这个条件是一个全局变量。这个方法接收两个参数，一个共享变量_cond，一个互斥量_mutex。调用后它会阻塞直到另一个线程调用 pthread_cond_signal 方法，它会唤醒一个等待 _cond 的线程。这也是 signal 的原理，下面是 signal 的源码：1234567891011121314151617181920212223242526272829303132333435363738public final void signal() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); // first 指向等待队列的首节点 Node first = firstWaiter; if (first != null) // 实际执行的方法 doSignal(first);&#125;private void doSignal(AbstractQueuedSynchronizer.Node first) &#123; do &#123; if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; // 找到第一个状态不为 CANCELLED 的节点（这个状态的原因是超时或中断） // transferForSignal 是核心方法 &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null);&#125;final boolean transferForSignal(Node node) &#123; // 这里返回 false 的条件是 node 的状态为 CANCELLED if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; Node p = enq(node); int ws = p.waitStatus; if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) // 核心方法 LockSupport.unpark(node.thread); return true;&#125;public static void unpark(Thread thread) &#123; if (thread != null) // 核心方法 UNSAFE.unpark(thread); &#125; 这里又看到了一个 UNSAFE 方法，说明这是一个 JNI 方法，在 Linux 下的实现为：1234567891011121314151617181920212223void Parker::unpark() &#123; int s, status ; status = pthread_mutex_lock(_mutex); assert (status == 0, \"invariant\") ; s = _counter; _counter = 1; if (s &lt; 1) &#123; if (WorkAroundNPTLTimedWaitHang) &#123; // 核心部分 status = pthread_cond_signal (_cond) ; assert (status == 0, \"invariant\") ; status = pthread_mutex_unlock(_mutex); assert (status == 0, \"invariant\") ; &#125; else &#123; status = pthread_mutex_unlock(_mutex); assert (status == 0, \"invariant\") ; status = pthread_cond_signal (_cond) ; assert (status == 0, \"invariant\") ; &#125; &#125; else &#123; pthread_mutex_unlock(_mutex); assert (status == 0, \"invariant\") ; &#125; 我们看到果然是调用了pthread_cond_signal，从而把阻塞在 _cond 上的一个线程唤醒。同理还有 pthread_cond_broadcast 函数，它会唤醒所有阻塞在 _cond 上的线程，这也是 signalAll 的原理。 四、参考juc中ArrayBlockingQueue为什么出入用同一个锁？解读 java 并发队列 BlockingQueue","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"CAS 原理和缺陷","slug":"CAS原理和缺陷","date":"2018-09-03T12:21:30.000Z","updated":"2019-03-08T14:00:58.684Z","comments":true,"path":"2018/09/03/CAS原理和缺陷/","link":"","permalink":"http://yoursite.com/2018/09/03/CAS原理和缺陷/","excerpt":"","text":"JDK1.6 以后 JVM 对 synchronize 锁机制作了不少优化，加入了偏向锁和自旋锁，在锁的底层实现中或多或少的都借助了 CAS 操作，其实 Java 中 java.util.concurrent 包的实现也是差不多建立在 CAS 之上，可见 CAS 在 Java 同步领域的重要性。 CAS 是 Compare and Swap 的简写形式，可翻译为：比较并交换。用于在硬件层面上提供原子性操作。其实现方式是基于硬件平台的汇编指令，就是说 CAS 是靠硬件实现的，JVM 只是封装了汇编调用。比较是否和给定的数值一致，如果一致则修改，不一致则不修改。 CAS 案例分析AtomicInteger 的原子特性就是 CAS 机制的典型使用场景。 其相关的源码片段如下：123456789101112131415161718private volatile int value; public final int get() &#123; return value; &#125; public final int incrementAndGet() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; &#125; &#125; public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125; AtomicInteger 在没有锁的机制下借助 volatile 原语，保证了线程间的数据是可见的（共享的）。其 get() 方法可以获取最新的内存中的值。 在 incrementAndGet() 的操作中，使用了 CAS 操作，每次从内存中读取最新的数据然后将此数据+1，最终写入内存时，先比较内存中最新的值，同累加之前读出来的值是否一致，不一致则写失败，循环重试直到成功为止。 compareAndSet 的具体实现调用了 unsafe 类的 compareAndSwapInt 方法，它其实是一个 Java Native Interface（简称 JNI）java 本地方法，会根据不同的 JDK 环境调用不同平台的对应 C 实现，下面以 windows 操作系统，X86 处理器的实现为例，这个本地方法在 openjdk 中依次调用的 c++代码为：unsafe.cpp，atomic.cpp 和 atomic_windows_x86.inline.hpp，它的实现代码存在于：openjdk7\\hotspot\\src\\os_cpu\\windows_x86\\vm\\atomic_windows_x86.inline.hpp，下面是相关的代码片段：1234567891011121314151617181920// Adding a lock prefix to an instruction on MP machine // VC++ doesn't like the lock prefix to be on a single line // so we can't insert a label after the lock prefix. // By emitting a lock prefix, we can define a label after it. #define LOCK_IF_MP(mp) __asm cmp mp, 0 \\ __asm je L0 \\ __asm _emit 0xF0 \\ __asm L0: inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) &#123; // alternative for InterlockedCompareExchange int mp = os::is_MP(); __asm &#123; mov edx, dest mov ecx, exchange_value mov eax, compare_value LOCK_IF_MP(mp) cmpxchg dword ptr [edx], ecx &#125; &#125; 由上面源代码可见在该平台的处理器上 CAS 通过指令 cmpxchg（就是 x86 的比较并交换指令）实现，并且程序会根据当前处理器是否是多处理器 (is_MP) 来决定是否为 cmpxchg 指令添加 lock 前缀 (LOCK_IF_MP)，如果是单核处理器则省略 lock 前缀 (单处理器自身会维护单处理器内的顺序一致性，不需要 lock 前缀提供的内存屏障效果 (而在 JDK9 中，已经忽略了这种判断都会直接添加 lock 前缀，这或许是因为现代单核处理器几乎已经消亡)。关于 Lock 前缀指令： Lock 前缀指令可以通过对总线或者处理器内部缓存加锁，使得其他处理器无法读写该指令要访问的内存区域，因此能保存指令执行的原子性。 Lock 前缀指令将禁止该指令与之前和之后的读和写指令重排序。 Lock 前缀指令将会把写缓冲区中的所有数据立即刷新到主内存中。 上面的第 1 点保证了 CAS 操作是一个原子操作，第 2 点和第 3 点所具有的内存屏障效果，保证了 CAS 同时具有 volatile 读和 volatile 写的内存语义（不过一般还是认为 CAS 只具有原子性而不具有可见性，因为底层的处理器平台可能不同）。 关于总线锁定和缓存锁定 1、早期的处理器只支持通过总线锁保证原子性。所谓总线锁就是使用处理器提供的一个 LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。很显然，这会带来昂贵的开销。2、缓存锁定是改进后的方案。在同一时刻我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把 CPU 和内存之间通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，最近的处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。缓存锁定是指当两个 CPU 的缓存行同时指向一片内存区域时，如果 A CPU 希望对该内存区域进行修改并使用了缓存锁定，那么 B CPU 将无法访问自己缓存中相应的缓存行，自然也没法访问对应的内存区域，这样就 A CPU 就实现了独享内存。 但是有两种情况下处理器不会使用缓存锁定。第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行（cache line），则处理器会调用总线锁定。第二种情况是：有些处理器不支持缓存锁定。对于 Inter486 和奔腾处理器，就算锁定的内存区域在处理器的缓存行中也会调用总线锁定。 关于同样使用 Lock 前缀的 volatile 却无法保证原子性 volatile 和 cas 都是基于 lock 前缀实现，但 volatile 却无法保证原子性这是因为：Lock 前缀只能保证缓存一致性，但不能保证寄存器中数据的一致性，如果指令在 lock 的缓存刷新生效之前把数据写入了寄存器，那么寄存器中的数据不会因此失效而是继续被使用，就好像数据库中的事务执行失败却没有回滚，原子性就被破坏了。以被 volatile 修饰的 i 作 i++为例，实际上分为 4 个步骤：mov 0xc(%r10),%r8d ; 把 i 的值赋给寄存器inc %r8d ; 寄存器的值+1mov %r8d,0xc(%r10) ; 把寄存器的值写回lock addl $0x0,(%rsp) ; 内存屏障，禁止指令重排序，并同步所有缓存 如果两个线程 AB 同时把 i 读进自己的寄存器，此时 B 线程等待，A 线程继续工作，把 i++后放回内存。按照原子性的性质，此时 B 应该回滚，重新从内存中读取 i，但因为此时 i 已经拷贝到寄存器中，所以 B 线程会继续运行，原子性被破坏。 而 cas 没有这个问题，因为 cas 操作对应指令只有一个：lock cmpxchg dword ptr [edx], ecx ; 该指令确保了直接从内存拿数据（ptr [edx]），然后放回内存这一系列操作都在 lock 状态下，所以是原子性的。 总结：volatile 之所以不是原子性的原因是 jvm 对 volatile 语义的实现只是在 volatile 写后面加一个内存屏障，而内存屏障前的操作不在 lock 状态下，这些操作可能会把数据放入寄存器从而导致无法有效同步；cas 能保证原子性是因为 cas 指令只有一个，这个指令从头到尾都是在 lock 状态下而且从内存到内存，所以它是原子性的。 CAS 缺陷1、ABA 问题。因为 CAS 需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是 A，变成了 B，又变成了 A，那么使用 CAS 进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA 问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么 A－B－A 就会变成 1A - 2B－3A。 从 Java1.5 开始 JDK 的 atomic 包里提供了一个类 AtomicStampedReference 来解决 ABA 问题。这个类的 compareAndSet 方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2、循环时间长开销大。自旋 CAS 如果长时间不成功，会给 CPU 带来非常大的执行开销。如果 JVM 能支持处理器提供的 pause 指令那么效率会有一定的提升，pause 指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline），使 CPU 不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起 CPU 流水线被清空（CPU pipeline flush），从而提高 CPU 的执行效率。 3、只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量 i＝2，j=a，合并一下 ij=2a，然后用 CAS 来操作 ij。从 Java1.5 开始 JDK 提供了 AtomicReference 类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作。 4、总线风暴带来的本地延迟。在多处理架构中，所有处理器会共享一条总线，靠此总线连接主存，每个处理器核心都有自己的高速缓存，各核相对于 BUS 对称分布，这种结构称为“对称多处理器”即 SMP。当主存中的数据同时存在于多个处理器高速缓存的时候，某一个处理器的高速缓存中相应的数据更新之后，会通过总线使其它处理器的高速缓存中相应的数据失效，从而使其重新通过总线从主存中加载最新的数据，大家通过总线的来回通信称为“Cache 一致性流量”，因为总线被设计为固定的“通信能力”，如果 Cache 一致性流量过大，总线将成为瓶颈。而 CAS 恰好会导致 Cache 一致性流量，如果有很多线程都共享同一个对象，当某个核心 CAS 成功时必然会引起总线风暴，这就是所谓的本地延迟。而偏向锁就是为了消除 CAS，降低 Cache 一致性流量。 关于偏向锁如何消除 CAS 试想这样一种情况：线程 A：申请锁 - 执行临界区代码 - 释放锁 - 申请锁 - 执行临界区代码 - 释放锁。锁的申请和释放都会执行 CAS，一共执行 4 次 CAS。而在偏向锁中，线程 A：申请锁 - 执行临界区代码 - 比较对象头 - 执行临界区代码。只执行了 1 次 CAS。 关于总线风暴 其实也不是所有的 CAS 都会导致总线风暴，这跟 Cache 一致性协议有关，具体参考：http://blogs.oracle.com/dave/entry/biased_locking_in_hotspot另外与 SMP 对应还有非对称多处理器架构，现在主要应用在一些高端处理器上，主要特点是没有总线，没有公用主存，每个 Core 有自己的内存。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"《高性能 MySQL》读书笔记——查询优化","slug":"《高性能MySQL》读书笔记——查询优化","date":"2018-08-28T10:34:21.000Z","updated":"2019-03-09T08:03:00.350Z","comments":true,"path":"2018/08/28/《高性能MySQL》读书笔记——查询优化/","link":"","permalink":"http://yoursite.com/2018/08/28/《高性能MySQL》读书笔记——查询优化/","excerpt":"","text":"在设计了最优的库表结构、如何建立最好的索引，这些对于高性能来说必不可少。但这些还不够，还需要合理地设计查询。 一、查询执行的基础当 MySQL 执行一个查询语句时，会经历以下几个步骤： 客户端发送一条查询给服务器。 服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。 服务器端进行 SQL 解析、预处理，再由优化器生成对应的执行计划。 MySQL 根据优化器生成的执行计划，调用存储引擎的 API 来执行查询。 将结果返回给客户端。 二、MySQL 查询优化器MySQL 查询优化器会做大量的工作，这些工作包括但不限于： 1. 重新定义关联表的顺序MySQL 使用一种叫 “嵌套循环关联” 的方式来执行关联查询。顾名思义，这是一种嵌套式的查询。在正常情况下，最左边的表会嵌套在最外层，然后根据表中的每一行数据去遍历内层表，找到所有符合条件的行。如果外层表行数为 m，内层表行数为 n，则总共要遍历 m*n 行数据。 但如果内层表使用了索引，而关联字段恰好就被索引覆盖的话，就只需要几次查询就可以定位内层表的数据行。总行数从 m*n 变为 m*i(i 一般小于 3)。这无疑大大加快了关联查询的速度。MySQL 查询优化器会调整关联表查询的顺序来尽可能使用多的索引查询。 2. 将外连接转化成内连接并不是所有的 OUTER JOIN 语句都必须以外连接的方式执行。诸多原因，例如 WHERE 条件、库表结构都可能让一个外连接等价于一个内连接。MySQL 能够识别这点并重写查询，让其可以调整关联顺序（外连接分左右，所以无法调整顺序）。 3. 使用等价变换规则MySQL 可以合并和减少一些比较，例如（5=5 AND a&gt;5）将被改写成（a&gt;5）。 4. 优化 COUNT()、MIN() 和 MAX()索引和是否可为空可以帮助 MySQL 优化这类表达式。例如要找某一列的最小值，只需查询对应 B-tree 索引最左端的记录，而最大值只需查询对应 B-tree 索引最右端。另外，没有任何 WHERE 条件的 COUNT(*) 查询在 MyISAM 中也是 O(1)，因为 MyISAM 维护了一个变量来存放数据表的行数（不过 innodb 没有，innodb 中执行 COUNT(*) 会做全表查询）。 5. 预估并转化为常数表达式当 MySQL 检测到一个表达式或者一个子查询可以转化为常数的时候，就会一直把该表达式作为常数进行优化处理。 6. 覆盖索引扫描当索引中的列覆盖所有查询中需要使用的列时，MySQL 将直接使用索引返回所需数据而不做回表查询。 7. 提前终止查询在发现已经满足查询要求的时候，MySQL 总是能够立刻终止查询。比如 LIMIT。 8. 子查询优化MySQL 5.6 中处理子查询的思路是，基于查询重写技术的规则，尽可能将子查询转换为连接，并配合基于代价估算的 Materialize、exists 等优化策略让子查询执行更优。 9. 等值传播如果两个列的值通过等式关联，那么 MySQL 能够把其中一个列的 WHERE 语句条件传递到另一列上。例如： SELECT film.film_id FROM film INNER JOIN film_actor USING(film_id) WHERE film.film_id &gt; 500; 优化器会把它优化为： SELECT film.film_id FROM film INNER JOIN film_actor USING(film_id) WHERE film.film_id &gt; 500 AND film_actor.film_id &gt; 500; 10. 列表 IN() 的比较MySQL 会将 IN() 列表中的数据先进行排序，然后通过二分查找的形式来确定取出的值是否在列表中。 11. 索引合并当 WHERE 子句中包含多个复杂条件涉及到多个索引时，MySQL 会先根据不同索引取出多组数据，再将这些数据合并。 三、MySQL 查询优化器的局限1. 关联子查询上一节有提到 MySQL 查询优化器会把 IN 子查询变为 EXISTS 子查询的形式，大部分情况下这种优化会带来性能提升，但某些情况下，会让查询更慢。比如： SELECT * FROM film WHERE film_id IN(SELECT film_id FROM film_actor WHERE actor_id = 1); 假设 film 和 film_actor 表在 film_id 上都有索引，那么这条语句会走 film_actor 和 film 的索引，速度非常快。但查询优化器会把它 “优化” 为： SELECT * FROM film WHERE EXISTS(SELECT 1 FROM film_actor WHERE actor_id = 1 AND film.film_id = actor.film_id); 此时 MySQL 只会走 film_actor 的索引而会对 film 做全表扫描，效率大大下降。 解决的方法就是使用左外连接改造： SELECT film.* FROM film LEFT OUTER JOIN film_actor USING(film_id) WHERE film_actor.actor_id = 1; PS：5.6 及以后版本的 MySQL 对关联子查询做了大量优化，现在的思路是，基于查询重写技术的规则，尽可能将子查询转换为连接，并配合基于代价估算的 Materialize、exists 等优化策略让子查询执行更优。因此 5.6 以后将不存在这个问题。 2.UNION 的限制MySQL 无法将限制条件从外层 “下推” 到内层，其中一个典型就是 UNION： (SELECT first_name FROM actor) UNION ALL (SELECT first_name FROM customer) LIMIT 20; MySQL 会把 actor 和 customer 中的所有记录放在同一张临时表中，然后从临时表中取出前 20 条。可以通过把 LIMIT 放入内部来解决这个问题： (SELECT first_name FROM actor LIMIT 20) UNION ALL (SELECT first_name FROM customer LIMIT 20) LIMIT 20; 这样临时表的规模就缩小到 40 了。 3. 最大值和最小值优化MySQL 优化器会对不加条件的 MAX 和 MIN 做优化，但并没有对加条件的这两个函数做优化。比如： SELECT MIN(actor_id) FROM actor WHERE first_name = “PENELOPE”; actor_id 是主键，严格按照大小排序，那么其实在找到第一个满足条件的记录就可以返回了。而 MySQL 会继续遍历整张表。修改的方式是使用 LIMIT： SELECT actor_id FROM actor WHERE first_name = “PENELOPE” LIMIT 1; 此时会触发提前终止机制，返回最小的 actor_id。 四、优化特定类型的查询1. 优化 COUNT() 查询很多时候一些业务场景并不要求完全精确的 COUNT 值，可以使用 EXPLAIN SELECT * FROM film; 得到的近似值来代替。 innodb 下，如果表中单行数据量很大，且没有二级索引的话，可以对表上较短的且不为空的字段加索引，再执行 count(*），此时优化器会自动选择走二级索引，由于二级索引是短字段，单页存储的数据行数就多，减少了取页的次数，查询时间也就更短了。 2. 优化关联查询确保 ON 或者 USING 子句中的列上有索引，这样只需要全表扫描第一个表，第二个表可以走索引。 确保任何的 GROUP BY 和 ORDER BY 中的表达式只涉及到一个表中的列，这样 MySQL 才可能使用索引来优化这个过程。 3. 优化 LIMIT 分页LIMIT 分页在系统偏移量非常大的时候效果会很差。比如 LIMIT 1000,20 这样的查询，这时 MySQL 需要查询 10020 条记录然后只返回最后 20 条。 优化此类分页查询的一个最简单的办法就是尽可能利用索引覆盖扫描。考虑下面的查询： SELECT film_id, description FROM film ORDER BY title LIMIT 50,5; 这个语句可以被改写成： SELECT film.film_id,film,description FROM film INNER JOIN(SELECT film_id FROM film ORDER BY title LIMIT 50,5)AS lim USING(film_id); 这里的 “延迟关联” 操作将大大提升查询效率，它会利用覆盖索引直接定位到分页所需数据所在的位置，而不需要从头遍历。 记录上一次取数据的位置也是一个不错的主意。比如在频繁使用 “下一页” 这个功能的时候，记录下上次取数据最后的位置，然后可以把查询语句写成： SELECT film_id, description FROM film WHERE film_id&gt;16030 ORDER BY film_id LIMIT 5;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"《高性能 MySQL》读书笔记——索引优化","slug":"《高性能MySQL》读书笔记——索引优化","date":"2018-08-25T09:34:28.000Z","updated":"2019-03-08T14:00:09.624Z","comments":true,"path":"2018/08/25/《高性能MySQL》读书笔记——索引优化/","link":"","permalink":"http://yoursite.com/2018/08/25/《高性能MySQL》读书笔记——索引优化/","excerpt":"","text":"索引是数据库得以高效的关键，以最常见的 B+Tree 索引为例，至少有以下三个优点：1）索引大大减少了服务器需要扫描的数据量；2）索引可以帮助服务器避免排序和临时表；3）索引可以将随机 I/O 变为顺序 I/O。因此，如何使用索引也成了高效 MySQL 重要的一部分。 一、MySQL 中的索引1、B-Tree 索引B-Tree 和其变体 B+Tree 是绝大部分数据库引擎默认使用的数据结构，我们在谈到索引时若无特殊指代一般是指 B+Tree（B-Tree 和 B+Tree 的区别及选择这种数据结构的原因可以看这里 ） 优点B-Tree 索引适用于全键值、键值范围或键前缀查找。其中键前缀查找只适用于根据最左前缀的查找。以下表为例 family_name first_name birthday 张 三 1960-1-1 李 四 1962-3-2 王 五 1966-4-5 依次在 family_name、first_name 和 birthday 上建立 B-Tree 索引（暂不考虑主键的影响），则上述的索引对如下的索引有效： 全值匹配：全值匹配指的是和索引中的所有列进行匹配，如查找 family_name = 张，first_name = 三，birthday = 1960-1-1 的人。 匹配最左前缀：查找所有 family_name= 张 的人。 匹配列前缀：查找所有 family_name = 张，first_name = 三，birthday = 196X 的人。列前缀的语法有很多，比如 where birthday(3) = 196 或者 where birthday like “196%” 都是合法的匹配列前缀的语法，但 birthday like “%96%” 不是合法的列前缀匹配，不会使用索引查找。 匹配范围值：查找所有 family_name = 张，first_name = 三，birthday &gt; 1960-1-1 and birthday &lt; 1969-12-31 的人。 只访问索引的查询：若查询的内容在索引中便可全部找到，则无需回表查询，可以节省大量磁盘 IO，这种索引有个专有名词叫“覆盖索引”。 缺点B-Tree 索引的功能强大，但也有局限，仍以上述的索引为例： 如果不是按照索引的最左列开始查找，则无法使用索引。例如我们无法用索引查找 first_name = 四 的人。 不能跳过索引中的列。例如我们无法用索引查找 family_name = 张，birthday = 1960-1-1 的人。 查询中可以使用范围查询，但只能使用一次，且它右边所有的列都无法使用索引优化查找。如查找 family_name = 张，first_name &gt; 三 and first_name &lt; 五（字典序），birthday = 1960-1-1 的人，只会用到姓和名两个索引列，无法使用第三个索引列。（这个涉及到联合索引底层的数据结构，如下图） B-Tree 在建立联合索引的时候只会对第一个字段建立 B-Tree 索引，其它字段会在对应的叶子节点的 data 域上按给定字段的顺序作为优先级排序后储存。如上图，对 id、family_name、first_name 三个列建立索引。则底层存储时会先按 id 构造 B-Tree，再在 B-Tree 的叶子节点上按 family_name、first_name 的优先级排序后存储对应的地址。对于叶子节点上数据的查找，会采用二分查找的方式。而一旦确定了前一个字段使用范围查找后，得到的一组数据对于后一个字段而言是无序的，无法继续使用二分查找，只能遍历，此时索引失效。除了范围查询，整个 B-Tree 的最左匹配原则的原因也是和这个数据存储的方式息息相关的，理解了这个数据结构也就理解了 B-Tree 的最左匹配原则。 2、哈希索引哈希索引基于哈希表实现，使用链表法解决哈希冲突，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码，哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 在 MySQL 中，只有 Memory 引擎显式支持哈希索引，这也是 Memory 引擎表的默认索引类型。 优点 索引的结构十分紧凑，查找的速度非常快。 缺点 哈希索引只包含哈希值和指针，而不存储字段值，所以不能使用覆盖查询。 哈希索引数据并不是按照索引值顺序存储的，所以也就无法用于排序。 哈希索引也不支持部分索引列匹配查找，因为哈希值的计算是使用索引列的全部内容计算的。 哈希索引只支持等值比较，不支持任何范围查询 哈希索引的缺点也决定了哈希索引只适用于某些特定的场合，但一旦适合哈希索引，则它带来的性能提升将非常显著。 3、全文索引在标准的 MySQL 中只有 MyISAM 引擎支持全文索引，同时 innodb 也开始实验性质地支持全文索引。 MyISAM 的全文索引作用对象是一个“全文集合”，这可能是某个数据表的一列，也可能使多个列。具体的，对数据表的某一条记录，MySQL 会将需要索引的列全部拼接成一个字符串，然后进行索引。 MyISAM 的全文索引是一类特殊的 B-Tree 索引，共有两层。第一层是所有关键字，然后对于每一个关键字的第二层，包含的是一组相关的“文档指针”。 二、innodb 中的索引1、聚簇索引聚簇索引不是一种单独的索引类型，而是一种数据存储方式。innodb 的聚簇索引是在 B-tree 的叶子节点上存放了数据行。 innodb 所有表中的数据都会以这种形式保存在磁盘，这也意味着 innodb 每张表中至少要有一个主键。如果没有显式地定义主键，innodb 会选择一个唯一的非空索引代替；如果没有这样的索引，innodb 会隐式定义一个主键作为聚簇索引。 聚簇索引中，每个叶子节点称为一个数据页，相邻的数据页之间有双向指针相连，范围查找可以直接按顺序读出，速度非常快。 2、非聚簇索引（辅助索引）innodb 中每张表有且仅有一个聚簇索引，剩下的都是非聚簇索引。对于非聚簇索引，叶子节点并不会包含行记录的全部数据，而是保存指向聚簇索引中某一条记录的指针。比如 user 表中使用 user_id 作为主键，那么在它的非聚簇索引的叶子节点中，保存的就是 user_id。对于一次使用了非聚簇索引的查找，数据库引擎会先在非聚簇索引上找到 user_id，再根据 user_id 在聚簇索引上找到对应的数据行，这也就是 innodb 中的二次查询。 3、自适应哈希索引自适应哈希索引是 innodb 上的一种优化措施。InnoDB 存储引擎会监控对表上各索引页的查询。如果观察到建立哈希索引可以带来速度提升，则建立哈希索引，称之为自适应哈希索引 (Adaptive Hash Index, AHI)。AHI 是通过缓冲池的 B+树页构造而来，因此建立的速度很快，而且不需要对整张表构建哈希索引。InnoDB 存储引擎会自动根据访问的频率和模式来自动地为某些热点页建立哈希索引。 AHI 有一个要求，对这个页的连续访问模式必须是一样的。例如对于 (a,b) 这样的联合索引页，其访问模式可以是下面情况： where a=xxx where a =xxx and b=xxx 访问模式一样是指查询的条件是一样的，若交替进行上述两种查询，那么 InnoDB 存储引擎不会对该页构造 AHI。AHI 还有下面几个要求： 以该模式连续访问了 100 次 以该模式连续访问了 页中记录总数/16 次 必须同时满足上述所有要求才会建立 AHI。 InnoDB 存储引擎官方文档显示，启用 AHI 后,读取和写入速度可以提高 2 倍，辅助索引的连接操作性能可以提高 5 倍。 三、常见索引失效场景1、查询条件包含 or SELECT * FROM order WHERE order_id = 1 OR pay_method=’123’; 当 or 左右查询字段只有一个是索引，该索引失效；只有当 or 左右查询字段均为索引时，才会生效。 2、索引列上有计算、函数等操作 SELECT * FROM order WHERE order_id +1 = 2; 3、使用负向查询（!=、&lt;&gt;、not in、not exists、not like 等） SELECT * FROM order WHERE order_id &lt;&gt; 2; 4、5.7 之前的 is null 和 is not null SELECT * FROM order WHERE order_id is not null; 5.7 之后 is null 和 is not null 也会走索引，但对于使用了声明了 NOT NULL 的索引行不会。 5、不符合最左前缀原则的组合索引当查询涉及到联合索引时，查询的条件必须是联合索引的一个前缀。比如对于联合索引 A/B/C/D，查询的条件可以是 A，也可以是 A/B/C，但不能是 B/C。另外对于范围查询，只能有一个条件是范围查询且必须是最后一个。比如查询 A/B/C，只有 C 可以是范围查询。另外在 MySQL 中，IN 被定义为范围查询，但却是当作多个条件等于来处理，因此 IN 语句放在中间，也会走索引。 6、like 以通配符开头 SELECT * FROM order WHERE pay_method LIKE ‘%23’; 7、字符串不加单引号 SELECT * FROM order WHERE pay_method = 123; 8、当全表扫描速度比索引速度快时，mysql 会使用全表扫描，此时索引失效一个有意思的例子是 IN 的索引失效。MySQL 优化器对开销代价的估算方法有两种：index dive 和 index statistics。前者统计速度慢但是能得到精准的值，后者统计速度快但是数据未必精准。老版本的 MySQL 默认使用 index dive 这种统计方式，但在 IN() 组合条件过多的时候会发生很多问题。查询优化可能需要花很多时间，并消耗大量内存。因此新版本 MySQL 在组合数超过一定的数量（eq_range_index_dive_limit）就会使用 index statistics 统计。而 index statistics 统计的结果不精确，因此可能会出现 IN 不走索引的情况。此时可以尝试通过增加 eq_range_index_dive_limit 的值（5.6 中默认是 10，5.7 中默认是 200）让 IN 语句走索引。 四、高性能索引策略1、使用前缀索引在对一个比较长的字符串建立索引的时候，把字符串所有字符放入索引是比较低效的做法。前文对字符串做哈希是一种方式。也可以使用字符串的前缀做索引。比如 CREATE INDEX index_name ON table_name (column_name(10)); 表示将列的前 10 个字符做索引，这样做的好处是减少索引字段的大小，可以在一页内放入更多的数据，从而降低 B-tree 的高度，同时更短的索引字段带来更短的匹配时间，提高了查找效率。 2、使用覆盖索引覆盖索引是一种索引包含了查询所需所有数据的情况，在这种情况下，MySQL 可以使用索引来直接获取列的数据，这样就不需要再读取数据行。覆盖索引是非常有用的工具，能够极大地提升性能： 索引条目远小于数据行大小，所以如果只需要读取索引，MySQL 就会极大地减少数据访问量。这对缓存的负载非常重要，因为这种情况下响应时间大部分花费在数据拷贝上。 因为索引是按值顺序存储的（至少在单个页内如此），对于 I/O 密集型的范围查询会比随机从磁盘读取每一行数据的 I/O 要少得多。 对于 innodb 的聚簇索引，覆盖索引特别有用。innodb 的二级索引在叶子节点中保存了行的主键值，所以如果二级主键能够覆盖索引，则可以避免对主键索引的二次查询。 3、延迟关联覆盖索引可以极大地提升查找的效率，但很多时候我们会遇到 select * 这样的需求，这时使用覆盖索引就不可能了。不过我们可以使用延迟关联的方式利用覆盖索引。 比如对于语句： select from t_portal_user where create_time &gt; ‘2012-10:10’ and create_time&lt;’2017:10:10’ LIMIT 5000,10; 可以改写成： SELECT from t_portal_user INNER JOIN (select id from t_portal_user where create_time &gt; ‘2012-10:10’ and create_time&lt;’2017:10:10’ LIMIT 5000,10) as a USING(id); 对于子查询： select id from t_portal_user where create_time &gt; ‘2012-10:10’ and create_time&lt;’2017:10:10’ LIMIT 5000,10; 如果在 create_time 上做了索引（innodb 中主键会被默认添加进索引中），则可以利用覆盖索引找到符合条件的 id，再根据 id 做普通查询。 4、利用索引来做排序MySQL 支持二种方式的排序，文件排序和索引，后者效率高，它指 MySQL 扫描索引本身完成排序。文件排序方式效率较低。ORDER BY 满足以下情况，会使用 Index 方式排序: 使用覆盖索引，即通过扫描索引本身就可完成排序 ORDER BY 语句 或者 WHERE , JOIN 子句和 ORDER BY 语句组成的条件组合满足最左前缀（一个例外是 IN，IN 在没有排序的最左匹配中被视为等值查询，对排序来说是一种范围查询） ORDER BY 语句中条件的排序顺序是一样（都为正序或者都为倒序） 5、自定义哈希索引如果存储引擎不支持哈希索引，则可以在 B-tree 基础上创建一个伪哈希索引。这和真正的哈希索引不是一回事，因为还是使用 B-tree 进行查找，但它使用的是哈希值而不是键本身进行查找。如 SELECT id FROM user WHERE address = “zhejiang ningbo”; 若删除原来 URL 列上的索引，而新增一个被索引的 address_crc 列，使用 CRC32 做哈希，就可以使用下面的方式查询： SELECT id FROM user WHERE address=”zhejiang ningbo” AND url_crc=CRC32(“zhejiang ningbo”); 这样做的性能会非常高，因为 MySQL 优化器会使用这个选择性很高而体积很小的基于 url_crc 列的索引来完成查找。即使有多个记录有相同的索引值，查找仍然很快，只需要根据哈希值做快速的整数比较就能找到索引条目，然后一一比较返回对应的行。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"《高性能 MySQL》读书笔记——库表结构优化","slug":"《高性能MySQL》读书笔记——库表结构优化","date":"2018-08-18T10:08:36.000Z","updated":"2019-03-08T13:58:07.624Z","comments":true,"path":"2018/08/18/《高性能MySQL》读书笔记——库表结构优化/","link":"","permalink":"http://yoursite.com/2018/08/18/《高性能MySQL》读书笔记——库表结构优化/","excerpt":"","text":"本文介绍了 MySQL 中的常用数据类型及其适用场景，以及数据库范式和反范式化的设计。 1、MySQL 常用数据类型MySQL 常用数据类型分为：整数、实数、字符串、日期和时间、位数据几种。 整数类型整数类型可以分为：TINYINT，SMALLINT，MEDIUMINT，INT，BIGINT，分别使用：8，16，24，32，64 位的存储空间。 整数类型有可选的 UNSIGNED 属性，表示不允许负值，可以把正数的上限提高一倍。 整数可以指定宽度，比如 INT(11)，这表示在一些交互工具中 INT 会显示 11 个数字，对于存储和计算来说没有意义，INT(1) 和 INT(20) 都是 32 位。 实数类型实数类型可以是带有小数部分的数字，也可以不是。比如可以使用 DECIMAL 存储比 BIGINT 还大的整数。 实数类型分为：FLOAT，DOUBLE 和 DECIMAL，其中 FLOAT 和 DOUBLE 用于存储不精确的小数类型，分别使用 32 和 64 位的存储空间；DECIMAL 用于存储精确的小数类型，存储空间的大小根据小数的长度决定。5.0 版本以后 DECIMAL 最多允许 65 个数字。 尽量使用 FLOAT 和 DOUBLE 存储小数类型。只有需要精确计算的场合才使用 DECIMAL，但在数据量比较大的时候，可以考虑使用 BIGINT 代替 DECIMAL，将需要存储的数字乘以相应的倍数即可。 字符串类型MySQL 中的字符串类型有 CHAR，VARCHAR，TEXT 和 BLOB 四种，其中 TEXT 和 BLOB 两种类型比较特别，MySQL 把每个 BLOB 和 TEXT 值当作一个独立的对象处理，存储引擎在处理时也通常会做相应的处理。比如当 TEXT 或 BLOB 的值较大时，innodb 会使用专门的“外部”存储区域来进行存储，此时每个值在行内需要 1~4 个字节存储一个指针。 TEXT 和 BLOB 之间仅有的不同是 BLOB 类型存储的是二进制数据，没有排序规则或字符集，而 TEXT 类型有字符集和排序规则。 MySQL 对 BLOB 和 TEXT 列进行排序与其他类型是不同的：它只对每个列的最前 max_sort_length 字节而不是整个字符串做排序。 VARCHAR 是变长字符串，而 CHAR 是定长字符串。VARCHAR 比 CHAR 更省空间，因为它仅使用必要的空间。VARCHAR 需要使用 1 或 2 个额外字节记录字符串的长度：如果列的最大长度小于或等于 255 字节，则只使用 1 个字节表示，否则使用 2 个。 VARCHAR 节省了存储空间，但由于行是变长的，在 UPDATE 时可能使行变得比原来更长，这就导致需要做额外的工作。如果一个行占用空间增长，超出页的大小，innodb 会使用裂页来使行可以放进页内。对于过长的 VARCHAR，innodb 会将其存储为 BLOB。 CHAR 值在存储时，MySQL 会删除所有的末尾空格。 使用枚举（ENUM）代替字符串类型如果预计字符串可取的值范围确定且数量不大，可以使用枚举的方式替代字符串。比如存储水果，预计种类只有苹果、香蕉、梨。可以把水果种类定义为 ENUM(“apple”,”banana”,”pear”)，实际存储时 MySQL 只会在列表中保存数字，并在.frm 文件中保存一个“数字-字符串”的映射关系，可以大大减少存储的空间。 日期和时间类型MySQL 能存储的最小时间粒度为秒，但也可以通过使用 BIGINT 类型存储微秒级别的时间戳等方式绕开这一限制。MySQL 中存储时间的数据类型有两种：DATETIME 和 TIMESTAMP。两种类型的区别如下： DATETIME 占用 8 个字节 允许为空值，可以自定义值，系统不会自动修改其值。 实际格式储存，格式为 YYYYMMDDHHMMSS 的整数 与时区无关 不可以设定默认值，所以在不允许为空值的情况下，必须手动指定 datetime 字段的值才可以成功插入数据。 可以在指定 datetime 字段的值的时候使用 now() 变量来自动插入系统的当前时间。 结论：datetime 类型适合用来记录数据的原始的创建时间，因为无论你怎么更改记录中其他字段的值，datetime 字段的值都不会改变，除非你手动更改它。 TIMESTAMP 占用 4 个字节，默认为 NOT NULL TIMESTAMP 值不能早于 1970 或晚于 2037。这说明一个日期，例如 ‘1968-01-01’，虽然对于 DATETIME 或 DATE 值是有效的，但对于 TIMESTAMP 值却无效，如果分配给这样一个对象将被转换为 0。 值以 UTC 格式保存，为从 1970 年 1 月 1 日（格林尼治时间）午夜以来的秒数。 时区转化 ，存储时对当前的时区进行转换，检索时再转换回当前的时区。 默认情况下，如果插入或更新时没有指定第一个 TIMESTAMP 的值，MySQL 会设置这个列的值为当前时间。 结论：timestamp 类型适合用来记录数据的最后修改时间，因为只要你更改了记录中其他字段的值，timestamp 字段的值都会被自动更新。 位数据类型BIT 和 SET 是 MySQL 中典型的位数据类型，位数据的本质是一个二进制字符串，使用位数据类型可以在一列中存储多个”true/false”值。 2、范式和反范式数据库中的范式满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多规范要求的称为第二范式（2NF），其余范式以次类推。一般说来，数据库只需满足第三范式 (3NF）就行了。 范式的包含关系。一个数据库设计如果符合第二范式，一定也符合第一范式。如果符合第三范式，一定也符合第二范式。 1NF：属性不可分 2NF：属性完全依赖于主键 [消除部分子函数依赖 ] 3NF：属性不依赖于其它非主属性 [消除传递依赖 ] 第一范式 (1NF)符合 1NF 的关系中的每个属性都不可再分。反例： 第二范式 (2NF)2NF 在 1NF 的基础之上，消除了非主属性对于码（主键）的部分函数依赖 可以通过分解来满足。 分解前 学号 姓名 系名 系主任 课名 分数 1022211101 李小明 经济系 王强 高等数学 95 1022211101 李小明 经济系 王强 大学英语 87 1022211101 李小明 经济系 王强 普通化学 76 1022211102 张莉莉 经济系 王强 高等数学 72 1022211102 张莉莉 经济系 王强 大学英语 98 1022211102 张莉莉 经济系 王强 计算机基础 88 1022511101 高芳芳 法律系 刘玲 高等数学 82 1022511101 高芳芳 法律系 刘玲 法律基础 82 以上学生课程关系中，{学号, 课名} 为键码（主键），有如下函数依赖： （学号，课名） -&gt; 分数 学号 -&gt; 姓名 学号 -&gt; 系名 -&gt; 系主任 分数完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。 姓名、系名和系主任都部分依赖于键码，我们需要把部分依赖变成完全依赖。 分解后 关系-1 学号 姓名 系名 系主任 1022211101 李小明 经济系 王强 1022211102 张莉莉 经济系 王强 1022211101 高芳芳 法律系 刘玲 有以下函数依赖： Sno -&gt; Sname, Sdept Sdept -&gt; Mname 关系-2 学号 课名 分数 1022211101 高等数学 95 1022211101 大学英语 87 1022211101 普通化学 76 1022211102 高等数学 72 1022211102 大学英语 98 1022211102 计算机基础 88 1022511101 高等数学 82 1022511101 法学基础 82 有以下函数依赖： Sno, Cname -&gt; Grade 第三范式 (3NF)3NF 在 2NF 的基础之上，消除了非主属性对于码（主键）的传递函数依赖 上面的 关系-1 中存在以下传递函数依赖： Sno -&gt; Sdept -&gt; Mname 可以进行以下分解： 关系-11 学号 姓名 系名 1022211101 李小明 经济系 1022211102 张莉莉 经济系 1022211101 高芳芳 法律系 关系-12 系名 系主任 经济系 王强 法律系 刘玲 范式的优缺点优点： 当数据较好地范式化时，就只有很少或者没有重复数据，所以更新时只需要修改更少的数据。 范式化的表通常更小，可以更好地放在内存里，所以执行操作很更快。 很少有多余的数据意味着检索列表数据时更少需要 DISTINCT 或者 GROUP BY 语句。比如前面的例子：在非范式化的结构中必须使用 DISTINCT 或者 GROUP BY 才能获得唯一的一张系名列表，但如果使用范式，只需要单独查询系名-系主任表就可以了。 缺点： 范式化的设计通常需要关联。稍微复杂一点的查询语句在符合范式的 schema 上都有可能需要至少一次关联，这不但代价昂贵，也可能使一些索引无效。例如，范式化可能将列存放在不同的表中，而这些列如果在一个表中本可以属于同一个索引。 常见反范式化设计范式化不一定适合所有场合，很多时候，一些冗余数据有助于我们提升性能。下面列举几个常见的反范式化操作。 缓存表缓存表可以存储那些可以从其他表获取但每次获取的速度比较慢的数据。比如有时可能会需要很多不同的索引组合来加速各种类型的查询。这些矛盾的需求有时需要创建一张只包含主表中部分列的缓存表。有时候我们可能需要不同存储引擎提供的不同特性。例如，如果主表使用 innodb，用 MyISAM 作为缓存表的引擎将会得到更小的索引空间，并且可以做全文索引。 汇总表汇总表保存的是 GROUP BY 语句聚合数据的表。相比缓存表，汇总表的数据不是逻辑上冗余的，但可以通过其它表计算得到。例如，计算某网站之前 24 小时内发送的消息数。我们可以通过 COUNT() 得到，但这样需要检索全表。作为替代方案，可以每小时生成一张汇总表。这样也许一条简单的查询就可以做到，并且比实时维护计数器要高效得多。缺点是计数并不是 100% 精确。 某网站之前 24 小时内发送的消息数的汇总表： CREATE TABLE msg_per_hr (hr DATETIME NOT NULL,cnt INT UNSIGNED NOT NULL,PRIMARY KEY(hr)); 计数器表计数器在应用中很常见。比如网站的点击数，文件下载次数等。 如果其它数据保存在一起，很可能碰到并发问题。创建一张独立的表是个比较好的办法，这样可使计数器表小且快。而且使用独立的表可以帮助避免查询缓存失效。 下面是一张简单的计数器表，只有一行数据，记录网站的点击次数： mysql&gt; CREATE TABLE hit_counter( -&gt; cnt int unsigned not null -&gt; ) ENGINE=InnoDB; 网站的每次点击都会导致对计数器进行更新： mysql&gt; UPDATE hit_counter SET cnt = cnt + 1; 问题在于，对于任何想要更新这一行的事务来说，这条记录上都有一个全局的互斥锁。这会使得这些事务只能串行进行。要获得更高的并发性，可以将计数器保存在多个行中，每次随机选择一行更新： mysql&gt; CREATE TABLE hit_counter( -&gt; slot tinyint unsigned not null primary key, -&gt; cnt int unsigned not null -&gt; ) ENGINE=InnoDB; 然后预先在这张表增加 100 行数据。现在选择一个随机的槽进行更新： mysql&gt; UPDATE hit_counter SET cnt = cnt + 1 WHERE slot = RAND() * 100; 要获得统计结果，需要使用下面这样的聚合查询： mysql&gt; CREATE TABLE daily_hit_counter( -&gt; day date not null, -&gt; slot tinyint unsigned not null, -&gt; cnt int unsigned not null, -&gt; primary key(day, slot) -&gt; ) ENGINE=InnoDB; 一个常见的需求是每隔一段时间开始一个新的计算器（例如，每天一个）。再作进一步修改： mysql&gt; CREATE TABLE daily_hit_counter( -&gt; day date not null, -&gt; slot tinyint unsigned not null, -&gt; cnt int unsigned not null, -&gt; primary key(day, slot) -&gt; ) ENGINE=InnoDB; 在这个场景中可以不用预告生成行 ，而用 ON DUPLICATE KEY UPDATE（对唯一索引或主键字段的值会检查是否已存在，存在则更新，不存在则插入）代替： mysql&gt; INSERT INTO daily_hit_counter(day, slot, cnt) -&gt; VALUES(CURRENT_DATE, RAND()*100, 1) -&gt; ON DUPLICATE KEY UPDATE cnt = cnt + 1; 如果希望减少表的行数，可以写一个周期执行的任务，合并所有结果到 0 号槽，并且删除所有其他的槽： UPDATE daily_hit_counter as c -&gt; INNER JOIN ( -&gt; SELECT day, SUM(cnt) AS cnt, MIN(slot) AS mslot -&gt; FROM daily_hit_counter -&gt; GROUP BY day -&gt; ) AS X USING(day) -&gt; SET c.cnt = IF(c.slot = x.mslot, x.cnt, 0), -&gt; c.slot = IF(c.slot = x.mslot, 0, c.slot);mysql&gt; DELETE FROM daily_hit_counter WHERE slot &lt;&gt; 0 AND cnt = 0;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"GET 与 POST 的区别","slug":"GET与POST的区别","date":"2018-05-15T06:55:30.000Z","updated":"2019-03-08T14:04:53.741Z","comments":true,"path":"2018/05/15/GET与POST的区别/","link":"","permalink":"http://yoursite.com/2018/05/15/GET与POST的区别/","excerpt":"","text":"HTTP 定义了一组请求方法, 以表明要对给定资源执行的操作。而硕果仅存的只剩两个半（笑）。实际开发中我们常用到的一般是 POST 和 GET，极少数情况下会用到 PUT。RFC7231 规范了 GET 方法用于请求一个指定资源的表示形式（transfer a current representation of the target resource），而 POST 方法用于将实体提交到指定的资源（Perform resource-specific processing on the request payload）。但仅仅了解规范是不够的，很多工具比如 chrome,nginx 有它自己履行规范的方式，从开发角度看，或许这些更有价值。 参数GET 传递的参数只能带 URL 后面，文本格式 QueryString，长度受限于浏览器发送长度和服务器接收长度。各家标准不一，作为开发人员宜选取其中最短一个（2083 字节）作为开发标准，以避免不必要的兼容问题。 IE(Browser) 2,083 Bytes Firefox(Browser) 65,536 Bytes Safari(Browser) 80,000 Bytes Opera(Browser) 190,000 Bytes Google (chrome) 8,182 Bytes Apache (Server) 8,182 Bytes Microsoft Internet Information Server(IIS) 16,384 Bytes POST 的参数就比较灵活，可以传递 application/x-www-form-urlencoded 的类似 QueryString、multipart/form-data 的二进制报文格式（支持文件信息嵌入报文传输）、纯文本或二进制的 body 参数。很多时候我们把参数写在 body 里，这时参数没有长度上的限制。 幂等幂等是一个计算机术语，表示重复执行某一操作得到的结果总是相同的。在 HTTP 中，如果我们说一个 HTTP 方法是幂等的，指的是同样的请求被执行一次与连续执行多次的效果是一样的，服务器的状态也是一样的。 GET 是幂等的，我们使用 GET 获取服务器上同一份数据，拿到的数据应该都是相同的。每次请求后服务器的状态应该也是相同的（统计数据除外）。 POST 不是幂等的，多次 POST 返回的结果不一定相同，每次请求后服务器的状态也是不一样的。 缓存GET 时默认可以复用前面的请求数据作为缓存结果返回，此时以完整的 URL 作为缓存数据的 KEY。所以有时候为了强制每次请求都是新数据，我们可以在 URL 后面加上一个随机参数或时间戳或版本号来避免从缓存中读取，也可以直接设置 Cache-Control 禁用缓存。 POST 则一般不会被这些缓存因素影响。 安全性服务器的日志比如像 nginx 的 access log 会自动记录 GET 和 POST 的 URL，包括其中带的参数，但不会记录请求的报文。对于一些敏感数据，POST 更安全一些。 自动化性能测试基于上面提到的 nginx 日志，可以使用 grep GET+日期，awk 格式化，然后 sort -u 去重，从而提取到某天的所有 GET 请求 URL，使用程序模拟登陆，然后请求所有 URL 即可获取简单的性能测试数据，每个请求是否正确，响应时间多少等等。 但是对于 POST 请求，因为不知道报文，无法这样简单处理。可以通过 nginx-lua 获取报文输出到 log，这样格式化会麻烦很多，但不失为一个办法。 TCP 包的数量GET 请求稳定只发送一个包，而 POST 请求在某些浏览器里会发送两个，具体的原因还在探究。 IE 6 – 2 packets IE 7 – 2 packets IE 8 – 2 packets Firefox 3.0.13 – 1 packet Firefox 3.5.2 – 1 packet Opera 9.27 – 2 packets Safari 4.0.3 – 2 packets Chrome 2.0.172.43 – 2 packets 参考资料1、URL 最大长度问题 2、xmlhttprequest-xhr-uses-multiple-packets-for-http-post3、comparing-get-and-post","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/tags/计算机网络/"}],"keywords":[]},{"title":"idea 常用快捷键备忘","slug":"idea常用快捷键备忘","date":"2018-05-08T07:00:32.000Z","updated":"2019-03-08T14:06:14.836Z","comments":true,"path":"2018/05/08/idea常用快捷键备忘/","link":"","permalink":"http://yoursite.com/2018/05/08/idea常用快捷键备忘/","excerpt":"","text":"记录一些开发中常用但不容易记住的快捷键。 CtrlCtrl + Ｙ 删除行Ctrl + Ｒ 替换Ctrl + Ｆ 当前代码中查找Ctrl + Ｗ 选中光标所在的单词，连按范围扩大Ctrl + －／＝ 折叠／展开当前光标所在代码和注释Ctrl + Ｉ 接口方法补全 Ctrl + AltCtrl + Alt + Ｌ 代码格式化Ctrl + Alt + Ｔ 选中的地方代码环绕提示Ctrl + Alt + 空格 类名或接口名提示Ctrl + Alt + 方向左／右 回退／前进到上一个操作的地方Ctrl + Alt + Ｂ 查看一个方法的实现Ctrl + Alt + Ｏ 清除多余包 Ctrl + ShiftCtrl + Shift + Ｕ 大／小写转换Ctrl + Shift + －／＝ 折叠／展开当前文件下所有代码和注释 AltAlt + 回车 智能提示Alt + 方向上／下 上／下一方法Alt + Insert 类方法补全 Shift双击 Shift 在项目的所有目录查找特定内容Shift ＋ F6 重命名类，方法，变量","categories":[],"tags":[{"name":"备忘录","slug":"备忘录","permalink":"http://yoursite.com/tags/备忘录/"}],"keywords":[]},{"title":"Java9 新特性概述","slug":"Java9新特性概述","date":"2018-04-11T02:51:48.000Z","updated":"2019-03-08T14:07:49.031Z","comments":true,"path":"2018/04/11/Java9新特性概述/","link":"","permalink":"http://yoursite.com/2018/04/11/Java9新特性概述/","excerpt":"","text":"Java9 正式发布于 2017 年 9 月 21 日。作为 Java8 之后 3 年半才发布的新版本，Java9 带来了很多重大的变化。其中最重要的改动是 Java 平台模块系统的引入。除此之外，还有一些新的特性。本文对 Java9 中包含的新特性做了概括性的介绍，可以帮助你快速了解 Java9。 Java 平台模块系统Java 平台模块系统，也就是 Project Jigsaw，把模块化开发实践引入到了 Java 平台中。在引入了模块系统之后，JDK 被重新组织成 94 个模块。Java 应用可以通过新增的 jlink 工具，创建出只包含所依赖的 JDK 模块的自定义运行时镜像。这样可以极大的减少 Java 运行时环境的大小。这对于目前流行的不可变基础设施的实践来说，镜像的大小的减少可以节省很多存储空间和带宽资源。 模块化开发的实践在软件开发领域并不是一个新的概念。Java 开发社区已经使用这样的模块化实践有相当长的一段时间。主流的构建工具，包括 Apache Maven 和 Gradle 都支持把一个大的项目划分成若干个子项目。子项目之间通过不同的依赖关系组织在一起。每个子项目在构建之后都会产生对应的 JAR 文件。在 Java9 中，已有的这些项目可以很容易的升级转换为 Java9 模块，并保持原有的组织结构不变。 Java9 模块的重要特征是在其工件（artifact）的根目录中包含了一个描述模块的 module-info.class 文件。工件的格式可以是传统的 JAR 文件或是 Java9 新增的 JMOD 文件。这个文件由根目录中的源代码文件 module-info.java 编译而来。该模块声明文件可以描述模块的不同特征。模块声明文件中可以包含的内容如下： 模块导出的包：使用exports可以声明模块对其他模块所导出的包。包中的 public 和 protected 类型，以及这些类型的 public 和 protected 成员可以被其他模块所访问。没有声明为导出的包相当于模块中的私有成员，不能被其他模块使用。 模块的依赖关系：使用requires可以声明模块对其他模块的依赖关系。使用requires transitive可以把一个模块依赖声明为传递的。传递的模块依赖可以被依赖当前模块的其他模块所读取。如果一个模块所导出的类型的型构中包含了来自它所依赖的模块的类型，那么对该模块的依赖应该声明为传递的。 服务的提供和使用：如果一个模块中包含了可以被 ServiceLocator 发现的服务接口的实现，需要使用provides with语句来声明具体的实现类；如果一个模块需要使用服务接口，可以使用uses语句来声明。 代码清单 1 中给出了一个模块声明文件的示例。在该声明文件中，模块 com.mycompany.sample 导出了 Java 包 com.mycompany.sample。该模块依赖于模块 com.mycompany.sample。该模块也提供了服务接口 com.mycompany.common.DemoService 的实现类 com.mycompany.sample.DemoServiceImpl。 清单 1.模块声明示例：123456module com.mycompany.sample &#123; exports com.mycompany.sample; requires com.mycompany.common; provides com.mycompany.common.DemoService with com.mycompany.sample.DemoServiceImpl; &#125; 模块系统中增加了模块路径的概念。模块系统在解析模块时，会从模块路径中进行查找。为了保持与之前 Java 版本的兼容性，CLASSPATH 依然被保留。所有的类型在运行时都属于某个特定的模块。对于从 CLASSPATH 中加载的类型，它们属于加载它们的类加载器对应的未命名模块。可以通过 Class 的 getModule() 方法来获取到表示其所在模块的 Module 对象。 在 JVM 启动时，会从应用的根模块开始，根据依赖关系递归的进行解析，直到得到一个表示依赖关系的图。如果解析过程中出现找不到模块的情况，或是在模块路径的同一个地方找到了名称相同的模块，模块解析过程会终止，JVM 也会退出。Java 也提供了相应的 API 与模块系统进行交互。 Jshelljshell 是 Java9 新增的一个实用工具。jshell 为 Java 增加了类似 NodeJS 和 Python 中的读取-求值-打印循环（ Read-Evaluation-Print Loop ）。在 jshell 中可以直接输入表达式并查看其执行结果。当需要测试一个方法的运行效果，或是快速的对表达式进行求值时，jshell 都非常实用。只需要通过 jshell 命令启动 jshell，然后直接输入表达式即可。每个表达式的结果会被自动保存下来，以数字编号作为引用，类似$1 和$2 这样的名称。可以在后续的表达式中引用之前语句的运行结果。在 jshell 中，除了表达式之外，还可以创建 Java 类和方法。jshell 也有基本的代码完成功能。 在代码清单 2 中，我们直接创建了一个方法 add。 清单 2.在 jshell 中添加方法：1234jshell&gt; int add(int x, int y) &#123; ...&gt; return x + y; ...&gt; &#125; | created method add(int,int) 接着就可以在 jshell 中直接使用这个方法，如代码清单 3 所示。 清单 3.在 jshell 中使用创建的方法：12jshell&gt; add(1, 2) $19 ==&gt; 3 集合、Stream 和 Optional在集合上，Java9 增加了 List.of()、Set.of()、Map.of() 和 Map.ofEntries() 等工厂方法来创建不可变集合，如代码清单 4 所示。 清单 4.创建不可变集合：12345678List.of(); List.of(\"Hello\", \"World\"); List.of(1, 2, 3);Set.of(); Set.of(\"Hello\", \"World\"); Set.of(1, 2, 3);Map.of();Map.of(\"Hello\", 1, \"World\", 2); Stream 中增加了新的方法 ofNullable、dropWhile、takeWhile 和 iterate。在代码清单 5 中，流中包含了从 1 到 5 的元素。断言检查元素是否为奇数。第一个元素 1 被删除，结果流中包含 4 个元素。 清单 5.Stream 中的 dropWhile 方法示例：1234567@Test public void testDropWhile() throws Exception &#123; final long count = Stream.of(1, 2, 3, 4, 5) .dropWhile(i -&gt; i % 2 != 0) .count(); assertEquals(4, count); &#125; Collectors 中增加了新的方法 filtering 和 flatMapping。在代码清单 6 中，对于输入的 String 流，先通过 flatMapping 把 String 映射成 Integer 流，再把所有的 Integer 收集到一个集合中。 清单 6.Collectors 的 flatMapping 方法示例：1234567@Test public void testFlatMapping() throws Exception &#123; final Set&lt;Integer&gt; result = Stream.of(\"a\", \"ab\", \"abc\") .collect(Collectors.flatMapping(v -&gt; v.chars().boxed(), Collectors.toSet())); assertEquals(3, result.size()); &#125; Optional 类中新增了 ifPresentOrElse、or 和 stream 等方法。在代码清单 7 中，Optional 流中包含 3 个元素，其中只有 2 个有值。在使用 flatMap 之后，结果流中包含了 2 个值。 清单 7.Optional 的 stream 方法示例：12345678910@Test public void testStream() throws Exception &#123; final long count = Stream.of( Optional.of(1), Optional.empty(), Optional.of(2) ).flatMap(Optional::stream) .count(); assertEquals(2, count); &#125; 进程 APIJava9 增加了 ProcessHandle 接口，可以对原生进程进行管理，尤其适合于管理长时间运行的进程。在使用 P rocessBuilder 来启动一个进程之后，可以通过 Process.toHandle() 方法来得到一个 ProcessHandle 对象的实例。通过 ProcessHandle 可以获取到由 ProcessHandle.Info 表示的进程的基本信息，如命令行参数、可执行文件路径和启动时间等。ProcessHandle 的 onExit() 方法返回一个 CompletableFuture 对象，可以在进程结束时执行自定义的动作。代码清单 8 中给出了进程 API 的使用示例。 清单 8.进程 API 示例：123456789final ProcessBuilder processBuilder = new ProcessBuilder(\"top\").inheritIO(); final ProcessHandle processHandle = processBuilder.start().toHandle(); processHandle.onExit().whenCompleteAsync((handle, throwable) -&gt; &#123; if (throwable == null) &#123; System.out.println(handle.pid()); &#125; else &#123; throwable.printStackTrace(); &#125; &#125;); 平台日志 API 和服务Java9 允许为 JDK 和应用配置同样的日志实现。新增的 System.LoggerFinder 用来管理 JDK 使用的日志记录器实现。JVM 在运行时只有一个系统范围的 LoggerFinder 实例。LoggerFinder 通过服务查找机制来加载日志记录器实现。默认情况下，JDK 使用 java.logging 模块中的 java.util.logging 实现。通过 LoggerFinder 的 getLogger() 方法就可以获取到表示日志记录器的 System.Logger 实现。应用同样可以使用 System.Logger 来记录日志。这样就保证了 JDK 和应用使用同样的日志实现。我们也可以通过添加自己的 System.LoggerFinder 实现来让 JDK 和应用使用 SLF4J 等其他日志记录框架。代码清单 9 中给出了平台日志 API 的使用示例。 清单 9.使用平台日志 API：123456public class Main &#123; private static final System.Logger LOGGER = System.getLogger(\"Main\"); public static void main(final String[] args) &#123; LOGGER.log(Level.INFO, \"Run!\"); &#125; &#125; 反应式流（Reactive Streams）反应式编程的思想最近得到了广泛的流行。在 Java 平台上有流行的反应式库 RxJava 和 Reactor。反应式流规范的出发点是提供一个带非阻塞负压（non-blocking backpressure）的异步流处理规范。反应式流规范的核心接口已经添加到了 Java9 中的 java.util.concurrent.Flow 类中。 Flow 中包含了 Flow.Publisher、Flow.Subscriber、Flow.Subscription 和 Flow.Processor 等 4 个核心接口。Java9 还提供了 SubmissionPublisher 作为 Flow.Publisher 的一个实现。RxJava2 和 Reactor 都可以很方便的与 Flow 类的核心接口进行互操作。 变量句柄变量句柄是一个变量或一组变量的引用，包括静态域，非静态域，数组元素和堆外数据结构中的组成部分等。变量句柄的含义类似于已有的方法句柄。变量句柄由 Java 类 java.lang.invoke.VarHandle 来表示。可以使用类 java.lang.invoke.MethodHandles.Lookup 中的静态工厂方法来创建 VarHandle 对象。通过变量句柄，可以在变量上进行各种操作。这些操作称为访问模式。不同的访问模式尤其在内存排序上的不同语义。目前一共有 31 种访问模式，而每种访问模式都在 VarHandle 中有对应的方法。这些方法可以对变量进行读取、写入、原子更新、数值原子更新和比特位原子操作等。VarHandle 还可以用来访问数组中的单个元素，以及把 byte[] 数组和 ByteBuffer 当成是不同原始类型的数组来访问。 在代码清单 10 中，我们创建了访问 HandleTarget 类中的域 count 的变量句柄，并在其上进行读取操作。 清单 10.变量句柄使用示例： 123456789101112131415161718192021public class HandleTarget &#123; public int count = 1; &#125; public class VarHandleTest &#123; private HandleTarget handleTarget = new HandleTarget(); private VarHandle varHandle; @Before public void setUp() throws Exception &#123; this.handleTarget = new HandleTarget(); this.varHandle = MethodHandles .lookup() .findVarHandle(HandleTarget.class, \"count\", int.class); &#125; @Test public void testGet() throws Exception &#123; assertEquals(1, this.varHandle.get(this.handleTarget)); assertEquals(1, this.varHandle.getVolatile(this.handleTarget)); assertEquals(1, this.varHandle.getOpaque(this.handleTarget)); assertEquals(1, this.varHandle.getAcquire(this.handleTarget)); &#125; &#125; 改进方法句柄（Method Handle）类 java.lang.invoke.MethodHandles 增加了更多的静态方法来创建不同类型的方法句柄。 arrayConstructor：创建指定类型的数组。 arrayLength：获取指定类型的数组的大小。 varHandleInvoker 和 varHandleExactInvoker：调用 VarHandle 中的访问模式方法。 zero：返回一个类型的默认值。 empty：返回 MethodType 的返回值类型的默认值。 loop、countedLoop、iteratedLoop、whileLoop 和 doWhileLoop：创建不同类型的循环，包括 for 循环、while 循环和 do-while 循环。 tryFinally：把对方法句柄的调用封装在 try-finally 语句中。在代码清单 11 中，我们使用 iteratedLoop 来创建一个遍历 String 类型迭代器的方法句柄，并计算所有字符串的长度的总和。 清单 11. 循环方法句柄使用示例：123456789101112131415161718192021222324public class IteratedLoopTest &#123; static int body(final int sum, final String value) &#123; return sum + value.length(); &#125; @Test public void testIteratedLoop() throws Throwable &#123; final MethodHandle iterator = MethodHandles.constant( Iterator.class, List.of(\"a\", \"bc\", \"def\").iterator()); final MethodHandle init = MethodHandles.zero(int.class); final MethodHandle body = MethodHandles .lookup() .findStatic( IteratedLoopTest.class, \"body\", MethodType.methodType( int.class, int.class, String.class)); final MethodHandle iteratedLoop = MethodHandles .iteratedLoop(iterator, init, body); assertEquals(6, iteratedLoop.invoke()); &#125; &#125; 并发在并发方面，类 CompletableFuture 中增加了几个新的方法。completeAsync 使用一个异步任务来获取结果并完成该 CompletableFuture。orTimeout 在 CompletableFuture 没有在给定的超时时间之前完成，使用 TimeoutException 异常来完成 CompletableFuture。completeOnTimeout 与 orTimeout 类似，只不过它在超时时使用给定的值来完成 CompletableFuture。新的 Thread.onSpinWait 方法在当前线程需要使用忙循环来等待时，可以提高等待的效率。 NashornNashorn 是 Java8 中引入的新的 JavaScript 引擎。Java9 中的 Nashorn 已经实现了一些 ECMAScript6 规范中的新特性，包括模板字符串、二进制和八进制字面量、迭代器和 for..of 循环和箭头函数等。Nashorn 还提供了 API 把 ECMAScript 源代码解析成抽象语法树（Abstract Syntax Tree，AST），可以用来对 ECMAScript 源代码进行分析。 I/O 流新特性类 java.io.InputStream 中增加了新的方法来读取和复制 InputStream 中包含的数据。 readAllBytes：读取 InputStream 中的所有剩余字节。readNBytes：从 InputStream 中读取指定数量的字节到数组中。transferTo：读取 InputStream 中的全部字节并写入到指定的 OutputStream 中。代码清单 12 中给出了这些新方法的使用示例。 清单 12.InputStream 中的新方法使用示例：1234567891011121314151617181920212223242526public class TestInputStream &#123; private InputStream inputStream; private static final String CONTENT = \"Hello World\"; @Before public void setUp() throws Exception &#123; this.inputStream = TestInputStream.class.getResourceAsStream(\"/input.txt\"); &#125; @Test public void testReadAllBytes() throws Exception &#123; final String content = new String(this.inputStream.readAllBytes()); assertEquals(CONTENT, content); &#125; @Test public void testReadNBytes() throws Exception &#123; final byte[] data = new byte[5]; this.inputStream.readNBytes(data, 0, 5); assertEquals(\"Hello\", new String(data)); &#125; @Test public void testTransferTo() throws Exception &#123; final ByteArrayOutputStream outputStream = new ByteArrayOutputStream(); this.inputStream.transferTo(outputStream); assertEquals(CONTENT, outputStream.toString()); &#125; &#125; ObjectInputFilter 可以对 ObjectInputStream 中包含的内容进行检查，来确保其中包含的数据是合法的。可以使用 ObjectInputStream 的方法 setObjectInputFilter 来设置。ObjectInputFilter 在进行检查时，可以检查如对象图的最大深度、对象引用的最大数量、输入流中的最大字节数和数组的最大长度等限制，也可以对包含的类的名称进行限制。 改进应用安全性能Java9 新增了 4 个 SHA-3 哈希算法，SHA3-224、SHA3-256、SHA3-384 和 SHA3-512。另外也增加了通过 java.security.SecureRandom 生成使用 DRBG 算法的强随机数。代码清单 13 中给出了 SHA-3 哈希算法的使用示例。 清单 13.SHA-3 哈希算法使用示例：import org.apache.commons.codec.binary.Hex;public class SHA3 { public static void main(final String[] args) throws NoSuchAlgorithmException { final MessageDigest instance = MessageDigest.getInstance(“SHA3-224”); final byte[] digest = instance.digest(“”.getBytes()); System.out.println(Hex.encodeHexString(digest)); }} 用户界面类 java.awt.Desktop 增加了新的与桌面进行互动的能力。可以使用 addAppEventListener 方法来添加不同应用事件的监听器，包括应用变为前台应用、应用隐藏或显示、屏幕和系统进入休眠与唤醒、以及用户会话的开始和终止等。还可以在显示关于窗口和配置窗口时，添加自定义的逻辑。在用户要求退出应用时，可以通过自定义处理器来接受或拒绝退出请求。在 AWT 图像支持方面，可以在应用中使用多分辨率图像。 统一 JVM 日志Java9 中，JVM 有了统一的日志记录系统，可以使用新的命令行选项-Xlog 来控制 JVM 上所有组件的日志记录。该日志记录系统可以设置输出的日志消息的标签、级别、修饰符和输出目标等。Java9 移除了在 Java8 中被废弃的垃圾回收器配置组合，同时把 G1 设为默认的垃圾回收器实现。另外，CMS 垃圾回收器已经被声明为废弃。Java9 也增加了很多可以通过 jcmd 调用的诊断命令。 其他改动方面在 Java 语言本身，Java9 允许在接口中使用私有方法。在 try-with-resources 语句中可以使用 effectively-final 变量。类 java.lang.StackWalker 可以对线程的堆栈进行遍历，并且支持过滤和延迟访问。Java9 把对 Unicode 的支持升级到了 8.0。ResourceBundle 加载属性文件的默认编码从 ISO-8859-1 改成了 UTF-8，不再需要使用 native2ascii 命令来对属性文件进行额外处理。注解@Deprecated 也得到了增强，增加了 since 和 forRemoval 两个属性，可以分别指定一个程序元素被废弃的版本，以及是否会在今后的版本中被删除。 在代码清单 14 中，buildMessage 是接口 SayHi 中的私有方法，在默认方法 sayHi 中被使用。 清单 14.接口中私有方法的示例：123456789public interface SayHi &#123; private String buildMessage() &#123; return \"Hello\"; &#125; void sayHi(final String message); default void sayHi() &#123; sayHi(buildMessage()); &#125; &#125; 小结作为 Java 平台最新的一个重大更新，Java9 中的很多新特性，尤其模块系统，对于 Java 应用的开发会产生深远的影响。本文对 Java9 中的新特性做了概括的介绍，可以作为了解 Java9 的基础。这些新特性的相关内容，可以通过官方文档来进一步的了解。 参考资源 (resources) 参考Java9 官方文档 ，了解 Java9 的更多内容。 参考Java9 官方 Java 文档 ，了解 Java API 的细节。 了解反应式流规范 的更多内容。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"收藏","slug":"收藏","permalink":"http://yoursite.com/tags/收藏/"}],"keywords":[]},{"title":"Centos7 下使用 systemd 管理服务","slug":"Centos7下使用systemd管理服务","date":"2018-03-30T13:09:37.000Z","updated":"2019-03-08T14:03:07.867Z","comments":true,"path":"2018/03/30/Centos7下使用systemd管理服务/","link":"","permalink":"http://yoursite.com/2018/03/30/Centos7下使用systemd管理服务/","excerpt":"","text":"Centos7 新增了 systemd 用于为系统的启动和管理提供一套完整的解决方案，以替代原先的系统管理器 system V init（SysVInit）。相比于 SysVInit，systemd 支持服务并行启动，从而使效率大大提高；同时它还具有日志管理、快速备份与恢复、挂载点管理等多种实用功能，是一套更完善的系统管理方案。服务器后端会经常有将 mysql、redis、nginx 等部件开机启动的需求，现在可以统一交给 Systemd 管理，方便许多。 Systemd 概述在 Linux 系统中，我们经常会遇到结尾为 ‘d’ 的进程，比如 initd，mysqld。根据 Linux 惯例，字母‘d’是守护进程（daemon）的缩写。systemd 这个名字的含义，就是它是整个系统的守护进程。在 Centos7 中，它是系统的第一个进程（PID 等于 1），创建于系统启动的过程中，其他进程都是它的子进程。 在 Centos7 中，系统的启动可以汇整成如下几个过程： ①、打开计算机电源，载入 BIOS 的硬件信息与进行自我测试，并依据设置取得第一个可开机的设备；②、读取并执行第一个开机设备内 MBR 的 boot loader（亦即是 grub2，spfdisk 等程序）；③、依据 boot loader 的设置载入 Kernel，Kernel 会开始侦测硬件与载入驱动程序；④、在硬件驱动成功后，Kernel 会主动调用 systemd 程序，并以 default.target 流程开机； 1) systemd 执行 sysinit.target 初始化系统及 basic.target 准备操作系统； 2) systemd 启动 multi-user.target 下的本机与服务器服务； 3) systemd 执行 multi-user.target 下的/etc/rc.d/rc.local 文件； 4) systemd 执行 multi-user.target 下的 getty.target 及登录服务； 5) systemd 执行 graphical 需要的服务（图形化版本特有） 大概就是上面这样子了。你会发现 systemd 出现的频率很高，这是因为 systemd 负责了开机时所有的资源（unit）调度任务，操作系统只需要启动 systemd，剩下的 systemd 都会帮它处理。不仅如此，事实上，systemd 扮演的就是一个资源管理者的角色，它最主要的功能就是准备软件执行的环境，包括系统的主机名称、网络设置、语系处理、文件系统格式及其他服务的启动等。 Systemd 的基本概念和操作一、UnitSystemd 可以管理所有系统资源。不同的资源统称为 Unit（单元）。systemd 将资源归纳为以下一些不同的类型。然而，systemd 正在快速发展，新功能不断增加。所以资源类型可能在不久的将来继续增加。 Service unit：系统服务（最常见）Target unit：多个 Unit 构成的一个组Device Unit：硬件设备Mount Unit：文件系统的挂载点Automount Unit：自动挂载点Path Unit：文件或路径Scope Unit：不是由 Systemd 启动的外部进程Slice Unit：进程组Snapshot Unit：Systemd 快照，可以切回某个快照Socket Unit：进程间通信的 socketSwap Unit：swap 文件Timer Unit：定时器 Unit 配置文件每一个 unit 都有一个配置文件，配置文件一般存放在目录/usr/lib/systemd/system/，它会告诉 systemd 怎么启动这个 unit。配置文件的后缀名，就是该 unit 的种类，比如 sshd.socket。如果省略，systemd 默认后缀名为.service，所以 sshd 会被理解成 sshd.service。 上图为我系统中 redis.service 配置文件的内容。它大致包含了这些信息：1）对这个资源的描述；2）所需的前置资源；3）实际执行此 service 的指令或脚本程序；4）实际停止此 service 的指令或脚本程序；5）该资源所在的组（target）。 unit 配置文件中常用的字段整理如下： Unit 字段 参数意义说明 Description 就是当我们使用systemctl list-units时，会输出给管理员看的简易说明。使用systemctl status输出的此服务的说明，也是这个字段。 After 说明此 unit 是在哪个 daemon 启动之后才启动的意思。基本上仅是说明服务启动的顺序而已，并没有强制要求里头的服务一定要启动后此 unit 才能启动。 Before 与 After 的意义相反，是在什么服务启动前最好启动这个服务的意思。不过这仅是规范服务启动的顺序，并非强制要求的意思。 Requires 明确地定义此 unit 需要在哪个 daemon 启动后才能够启动。如果在此项设置的前导服务没有启动，那么此 unit 就不会被启动。 Wants 表示这个 unit 之后最好还要启动什么服务比较好的意思，不过并没有明确的规范。主要的目的是希望创建让使用者比较好操作的环境。因此，这个 Wants 后面接的服务如果没有启动，其实不会影响到这个 unit 本身。 Conflicts 代表冲突的服务。表示这个项目后面接的服务如果有启动，那么我们这个 unit 本身就不能启动！我们 unit 有启动，则此项目后的服务就不能启动！是一种冲突性的检查。 Service 字段（service 特有） 参数意义说明 Type Type 字段定义启动类型。它可以设置的值如下：simple（默认值）：ExecStart 字段启动的进程为主进程。forking：ExecStart 字段将以 fork() 方式启动，此时父进程将会退出，子进程将成为主进程。oneshot：类似于 simple，但只执行一次，systemd 会等它执行完，才启动其他服务。dbus：类似于 simple，但会等待 D-Bus 信号后启动。notify：类似于 simple，启动结束后会发出通知信号，然后 systemd 再启动其他服务。idle：类似于 simple，但是要等到其他任务都执行完，才会启动该服务。一种使用场合是为让该服务的输出，不与其他服务的输出相混合。 EnvironmentFile 可以指定启动脚本的环境配置文件！例如 sshd.service 的配置文件写入到 /etc/sysconfig/sshd 当中！你也可以使用 Environment=后面接多个不同的 Shell 变量来给予设置！ ExecStart 就是实际执行此 daemon 的指令或脚本程序。你也可以使用 ExecStartPre（之前）以及 ExecStartPost（之后）两个设置项目来在实际启动服务前，进行额外的指令行为。但是你得要特别注意的是，指令串仅接受“指令 参数 参数…”的格式，不能接受 &lt;,&gt;,&gt;&gt;,&amp;等特殊字符，很多的 bash 语法也不支持喔！所以，要使用这些特殊的字符时，最好直接写入到指令脚本里面去！不过，上述的语法也不是完全不能用，亦即，若要支持比较完整的 bash 语法，那你得要使用 Type=oneshot 才行喔！其他的 Type 才不能支持这些字符。 ExecStop 与systemctl stop的执行有关，关闭此服务时所进行的指令。 ExecReload 与systemctl reload有关的指令行为。 Restart 当设置 Restart=1 时，则当此 daemon 服务终止后，会再次的启动此服务。 RemainAfterExit 当设置为 RemainAfterExit=1 时，则当这个 daemon 所属的所有程序都终止之后，此服务会再尝试启动。这对于 Type=oneshot 的服务很有帮助！ TimeoutSec 若这个服务在启动或者是关闭时，因为某些缘故导致无法顺利“正常启动或正常结束”的情况下，则我们要等多久才进入“强制结束”的状态！ KillMode 可以是process,control-group,none的其中一种，如果是process则 daemon 终止时，只会终止主要的程序（ExecStart 接的后面那串指令），如果是control-group时，则由此 daemon 所产生的其他 control-group 的程序，也都会被关闭。如果是none的话，则没有程序会被关闭。 RestartSec 与 Restart 有点相关性，如果这个服务被关闭，然后需要重新启动时，大概要 sleep 多少时间再重新启动的意思。默认是 100ms（毫秒）。 Install 字段 参数意义说明 WantedBy 这个设置后面接的大部分是*.target unit！意思是，这个 unit 本身是附挂在哪一个 target unit 下面的！一般来说，大多的服务性质的 unit 都是附挂在 multi-user.target 下面！ Also 当目前这个 unit 本身被 enable 时，Also 后面接的 unit 也请 enable 的意思！也就是具有相依性的服务可以写在这里呢！ Alias 进行一个链接的别名的意思！当 systemctl enable 相关的服务时，则此服务会进行链接文件的创建/usr/lib/systemd/system/multi-user.target。 配置 unit 开机启动开机时，systemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。systemctl enable命令用于在上面两个目录之间，建立符号链接关系，相当于激活开机启动。与之对应的，systemctl disable命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。 启动和停止 unit执行systemctl start命令启动软件，执行systemctl status命令查看该服务的状态 上面的输出结果含义如下： Loaded 行：配置文件的位置，是否设为开机启动Drop-In 行：符号链接地址Active 行：表示正在运行Main PID 行：主进程 IDCGroup 块：应用的所有子进程 当不需要服务继续运行时，可以执行systemctl stop命令终止正在运行的服务。有时候，该命令可能没有响应，服务停不下来，这时候可以执行systemctl kill命令强制终止。另外，需要重启服务时可以执行systemctl restart命令。 二、Target启动计算机的时候，需要启动大量的 unit。如果每一次启动，都要一一写明本次启动需要哪些 unit，显然非常不方便。Systemd 的解决方案就是 target。 简单说，target 就是一个 unit 组，包含许多相关的 unit 。启动某个 target 的时候，systemd 就会启动里面所有的 unit。从这个意义上说，target 这个概念类似于”状态点”，启动某个 target 就好比启动到某种状态。 传统的 init 启动模式里面，有 runlevel 的概念，跟 target 的作用很类似。不同的是，runlevel 是互斥的，不可能多个 runlevel 同时启动，但是多个 target 可以同时启动。 查看 target我们可以执行指令systemctl list-unit-files --type=target查看当前系统的所有 target。也可以执行指令systemctl list-dependencies multi-user.target查看一个 target 包含的所有 unit。 系统启动时 systemctl 会根据/etc/systemd/system/default.target 规划的策略进行启动，我们可以通过执行指令systemctl get-default查看启动时默认的 target（一般是 multi-user.target）。指令systemctl set-default可以设置启动时的默认 target。切换 target 时，默认不关闭前一个 target 启动的进程，我们可以通过systemctl isolate关闭前一个 target 里面所有不属于后一个 target 的进程。 三、日志管理Systemd 统一管理所有 unit 的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。 查看日志我们可以通过指令journalctl查看所有日志（默认情况下 ，只保存本次启动的日志）。journalctl -k可以查看内核日志，journalctl -b和journalctl -b -0可以查看系统本次启动的日志，journalctl -b -1可以查看上一次启动的日志，journalctl _PID=X查看指定进程的日志，journalctl /usr/bin/bash查看某个路径的脚本的日志，journalctl -u redis.service和journalctl -u redis.service --since today查看某个 unit 的日志。 Systemd 的特性（对比 SysVInit）为了保证运行在先前 Linux 版本上的应用程序运行稳定，systemd 兼容了原先的 SysVInit 以及 LSB initscripts，但也引入了新的特性。这使得系统中已经存在的服务和进程无需修改，降低了系统向 systemd 迁移的成本。但我们也应该了解 systemd 所做的改变，以更好的适应当前的版本。大体而言，systemd 相比 SysVInit 更改了以下几个方面： 一、支持并行启动系统启动时，需要启动很多启动项目，在 SysVInit 中，每一个启动项目都由一个独立的脚本负责，它们由 SysVinit 顺序地，串行地调用。因此总的启动时间是各脚本运行时间之和。而 systemd 通过 socket/D-Bus activation 等技术，能够将启动项目同时并行启动，大大提高了系统的启动速度。 二、提供按需启动能力当 sysvinit 系统初始化的时候，它会将所有可能用到的后台服务进程全部启动运行。并且系统必须等待所有的服务都启动就绪之后，才允许用户登录。这种做法有两个缺点：首先是启动时间过长；其次是系统资源浪费。 某些服务很可能在很长一段时间内，甚至整个服务器运行期间都没有被使用过。比如 CUPS，打印服务在多数服务器上很少被真正使用到。您可能没有想到，在很多服务器上 SSHD 也是很少被真正访问到的。花费在启动这些服务上的时间是不必要的；同样，花费在这些服务上的系统资源也是一种浪费。 Systemd 可以提供按需启动的能力，只有在某个服务被真正请求的时候才启动它。当该服务结束，systemd 可以关闭它，等待下次需要时再次启动它。 三、采用 Linux 的 Cgroup 特性跟踪和管理进程的生命周期Init 系统的一个重要职责就是负责跟踪和管理服务进程的生命周期。它不仅可以启动一个服务，也必须也能够停止服务。这看上去没有什么特别的，然而在真正用代码实现的时候，我们会发现有时候停止服务比一开始想的要困难。 服务进程一般都会作为守护进程（daemon）在后台运行，为此服务程序有时候会派生 (fork) 两次。在 SysVInit 中，需要在配置文件中正确地配置 expect 小节。这样 SysVInit 通过对 fork 系统调用进行计数，从而获知真正的守护进程的 PID 号。 还有更加特殊的情况。比如，一个 CGI 程序会派生两次，从而脱离了和 Apache 的父子关系。当 Apache 进程被停止后，该 CGI 程序还在继续运行。而我们希望服务停止后，所有由它所启动的相关进程也被停止。 为了处理这类问题，SysVInit 通过 strace 来跟踪 fork、exit 等系统调用，但是这种方法很笨拙，且缺乏可扩展性。Systemd 则利用了 Linux 内核的特性即 CGroup 来完成跟踪的任务。当停止服务时，通过查询 CGroup，Systemd 可以确保找到所有的相关进程，从而干净地停止服务。 CGroup 已经出现了很久，它主要用来实现系统资源配额管理。CGroup 提供了类似文件系统的接口，使用方便。当进程创建子进程时，子进程会继承父进程的 CGroup。因此无论服务如何启动新的子进程，所有的这些相关进程都会属于同一个 CGroup，systemd 只需要简单地遍历指定的 CGroup 即可正确地找到所有的相关进程，将它们一一停止即可。 四、启动挂载点和自动挂载的管理传统的 Linux 系统中，用户可以用/etc/fstab 文件来维护固定的文件系统挂载点。这些挂载点在系统启动过程中被自动挂载，一旦启动过程结束，这些挂载点就会确保存在。这些挂载点都是对系统运行至关重要的文件系统，比如 HOME 目录。和 SysVInit 一样，Systemd 会管理这些挂载点，以便能够在系统启动时自动挂载它们。Systemd 兼容了/etc/fstab 文件，我们可以继续使用该文件管理挂载点。 有时候用户还需要动态挂载点，比如打算访问 DVD 内容时，才临时执行挂载以便访问其中的内容，而不访问光盘时该挂载点被取消 (umount)，以便节约资源。传统地，人们依赖 autofs 服务来实现这种功能。 Systemd 内建了自动挂载服务，无需另外安装 autofs 服务，可以直接使用 systemd 提供的自动挂载管理能力来实现 autofs 的功能。 五、实现事务性依赖关系管理系统启动过程是由很多的独立工作共同组成的，这些工作之间可能存在依赖关系，比如挂载一个 NFS 文件系统必须依赖网络能够正常工作。Systemd 虽然能够最大限度地并发执行很多有依赖关系的工作，但是类似”挂载 NFS”和”启动网络”这样的工作还是存在天生的先后依赖关系，无法并发执行。对于这些任务，systemd 维护一个”事务一致性”的概念，保证所有相关的服务都可以正常启动而不会出现互相依赖，以至于死锁的情况。 六、能够对系统进行快照和恢复Systemd 支持按需启动，因此系统的运行状态是动态变化的，人们无法准确地知道系统当前运行了哪些服务。Systemd 快照提供了一种将当前系统运行状态保存并恢复的能力。 比如系统当前正运行服务 A 和 B，可以用 systemd 命令行对当前系统运行状况创建快照。然后将进程 A 停止，或者做其他的任意的对系统的改变，比如启动新的进程 C。在这些改变之后，运行 systemd 的快照恢复命令，就可立即将系统恢复到快照时刻的状态，即只有服务 A，B 在运行。一个可能的应用场景是调试：比如服务器出现一些异常，为了调试用户将当前状态保存为快照，然后可以进行任意的操作，比如停止服务等等。等调试结束，恢复快照即可。 这个快照功能目前在 systemd 中并不完善，似乎开发人员也没有特别关注它，因此有报告指出它还存在一些使用上的问题，使用时尚需慎重。 七、日志服务systemd 自带日志服务 journald，该日志服务的设计初衷是克服现有的 syslog 服务的缺点。比如： syslog 不安全，消息的内容无法验证。每一个本地进程都可以声称自己是 Apache PID 4711，而 syslog 也就相信并保存到磁盘上。 数据没有严格的格式，非常随意。自动化的日志分析器需要分析人类语言字符串来识别消息。一方面此类分析困难低效；此外日志格式的变化会导致分析代码需要更新甚至重写。 Systemd Journal 用二进制格式保存所有日志信息，用户使用 journalctl 命令来查看日志信息。无需自己编写复杂脆弱的字符串分析处理程序。 Systemd Journal 的优点如下： 简单性：代码少，依赖少，抽象开销最小。 零维护：日志是除错和监控系统的核心功能，因此它自己不能再产生问题。举例说，自动管理磁盘空间，避免由于日志的不断产生而将磁盘空间耗尽。 移植性：日志文件应该在所有类型的 Linux 系统上可用，无论它使用的何种 CPU 或者字节序。 性能：添加和浏览日志非常快。 最小资源占用：日志数据文件需要较小。 统一化：各种不同的日志存储技术应该统一起来，将所有的可记录事件保存在同一个数据存储中。所以日志内容的全局上下文都会被保存并且可供日后查询。例如一条固件记录后通常会跟随一条内核记录，最终还会有一条用户态记录。重要的是当保存到硬盘上时这三者之间的关系不会丢失。Syslog 将不同的信息保存到不同的文件中，分析的时候很难确定哪些条目是相关的。 扩展性：日志的适用范围很广，从嵌入式设备到超级计算机集群都可以满足需求。 安全性：日志文件是可以验证的，让无法检测的修改不再可能。 总结Systemd 作为 Centos7 最新采用的系统管理进程，相比前任有相当多的改变。它的优点是功能强大，使用方便，缺点是过于复杂，与操作系统的其他部分强耦合，可能在某种程度上违背了 Linux 原本”keep simple, keep stupid”设计哲学。但从一个系统使用者的角度，它的确在很多方面做得都要比它的前任更好。作为一个后端，我们需要对这些改变有所了解，才能将这个系统用得更好。 参考资料1、鸟哥的 Linux 私房菜：基础学习篇 第四版 第十九章 2、Systemd 入门教程 - 阮一峰的网络日志 3、CentOS / RHEL 7 : How to set default target (default runlevel)4、IBM developerWorks Systemed","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}],"keywords":[]},{"title":"简单的密码学生成唯一邀请码","slug":"简单的密码学生成唯一邀请码","date":"2018-03-23T09:10:23.000Z","updated":"2019-03-08T14:17:32.922Z","comments":true,"path":"2018/03/23/简单的密码学生成唯一邀请码/","link":"","permalink":"http://yoursite.com/2018/03/23/简单的密码学生成唯一邀请码/","excerpt":"","text":"最近项目需要生成用户邀请码，网上找了几个算法都不太满意，就自己写了一个。里面借鉴了一些密码学里的思路，最后的算法效果还不错。想把思路记录下来，可以用在类似对加密强度要求不高的场合下。 需求分析从业务需求和市面上其它产品邀请码的使用体验上来看，邀请码有以下几个强制性的要求： 不可重复 唯一确定 这两点要求首先就排除了 hash code 的可能，因为 hash code 是可以发生碰撞的。然后在强制性要求的基础之上，我们还有一些进一步的需求： 长度不能太长，6-10 位是合适的区间 不容易被推测出 资源消耗尽可能小 在这些需求的约束下，我们先来看看常见的通用的序列码生成算法。 通用方案通用方案的解决思路可以分为两种：一种是生成一串不重复随机数，然后将其保存到数据库里。使用邀请码时从数据库里查询就可以得到邀请人；另一种是对身份信息作加密，通常是用户 id，将加密后的密文作为邀请码，使用时可以不查询数据库，直接解密得到。理论上说，第二种方式稍好一点，可以少进行一次数据库查询。但是考虑到安全性，我们还是会把解密后的 id 拿到数据库中查询，防止有人输错或者伪造邀请码产生 NPE。因此在选择算法的时候，这两种思路我都有考虑到。 1、UUID谈到不重复的随机数，最先想到的自然是 UUID。UUID 是一种软件构建的标准，也是开放软件基金会组织（OSF）在分布式计算环境领域的一部分。按照 OSF 制定的标准计算，它用到了以太网卡地址、纳秒级时间、芯片 ID 码和许多可能的数字，保证对在同一时空中的所有机器都是唯一的。Java 的工具类 java.util.UUID 是 Java 提供的一整套 UUID 生成方案，对于开发者来说可以很方便的调用。然而 UUID 并不适合用在这里，因为 UUID 的位数是固定的 32 位，这个对于我们的邀请码来说显然是太长了（想象一下用户面对面分享邀请码的时候居然需要报一串 32 位的数字+字母）。 网上也有用 UUID 的一部分当随机数的，但 UUID 只能保证完整的 32 位是不会重复的，不能保证其中的某一段不重复，因此这个方案也行不通。 2、系统当前时间系统当前时间也是一种常见的随机数生成方案。它的做法是先获取到系统当前时间，再用它和某个时间点对比，将这两段时间的间隔以毫秒或者纳秒为单位存到内存中去。最后我们程序获取到的是一串数字。Java 提供了两个系统函数用于实现这个功能：System.currentTimeMillis() 和 System.nanoTime()。然而这两个系统函数在这个业务里都有各自的问题。 System.currentTimeMillis() 返回的是从 1970.1.1 UTC 零点开始到现在的时间，精确到毫秒。它的问题在于不能支持高并发的邀请码生成。在这套方案中，只要我们的系统在某 1 秒内生成的邀请码超过 32 个，那么出现相同邀请码的概率就超过 50%（详见生日攻击 ）。显然，这个规模的并发量是不能接受的。 System.nanoTime() 返回的是从某个固定但随意的时间点开始的时间，精确到纳秒。这个固定但随意的时间，在不同 JVM 中是不一样的。这也就是说不同计算机计算出来的 nanoTime() 是有可能重合的。甚至同一台计算机重启 JVM 后生成的 nanoTime() 也是可能重合的。这违背了我们的第一个要求。 3、RC4 算法RC4 对于学过密码的同学来说肯定不会陌生。它是大名鼎鼎的 RSA 三人组中的头号人物 Ronald Rivest 在 1987 年设计的一种轻量级对称加密算法。它的特点是按字节流加密，也就是说明文多长，密文就多长。这一特点很好避免了 UUID 只能生成 32 位字符串的尴尬。而且 RC4 是一个轻量级加密算法，运行速度快，占用资源少，很好地满足了我们的第 5 点要求。乍一看 RC4 似乎是种理想的方案，然而实际一跑就出现了问题： 出现了乱码！这是因为字符的取值在 0~255，而我们熟悉的英文和数字只占了其中的 62 位，其它符号是我们不熟悉的，当然也不能作为邀请码。解决方法也很简单，把字符串转成 16 进制即可： 由于把 8 位的字符串转成了 4 位的 16 进制，字符串的长度增加了一倍，但长度尚在可接受范围之内。不太满意的一点是加密后的密文都是连续性的，高位的数字基本不变。这也意味着如果被邀请的同学输错了后几位数字，后台大概率检测不到他的这次操作失误，因为他输入的错误邀请码能在数据库里被找到。而且连续的密文容易被找出规律，安全性较低。因此这种方式也不建议。 4、用户身份标志+随机数这种方法是我在网上找到的已经被用于实际业务中的方法，它的大致思路是这样： 获取用户身份的唯一标志，比如用户 ID。 将用户 ID 补全，补全的位数取决于你希望得到的邀请码长度，如：106 可以补全为 00106. 随机生成一串大写字母串，长度和补全后的用户 ID 相同，如：SZUDF。 将随机数隔位插入用户 ID，得到邀请码：S0Z0U1D0F6。 这种方式得到的邀请码基本能满足我们的要求：由用户 ID 的唯一性保证了邀请码的唯一性；随机生成的字母串又能保证不容易被找到规律，同时又提高了用户操作的容错率；长度也在可接受范围内。因此第一版的邀请码生成算法我们采用了这种方式。 但是它仍然有改进的空间。①、字母和数字的位置是固定的，有一定的容易被察觉的规律，且对于数字来说，仍然具有连续性；②、用户 ID 直接暴露在密文中，存在风险；③、没有校验位，邀请码的校验依赖于数据库，无法对恶意伪造大量错误邀请码的攻击进行有效防御。 因此我在这种算法上作了改进，克服了以上的缺点。 我的方案为了让字母和数字的位置不再固定，我将用户 ID 作了 36 进制转换，即把用户 ID 映射为一串字母+数字的组合，高位用 0 补全。 123456int[] b = new int[CODE_LENGTH];b[0] = id;for (int i = 0; i &lt; 5; ++i) &#123; b[i + 1] = b[i] / 36; b[i] = (b[i]) % 36;&#125; 同时把随机数生成的范围扩大到字母和数字，这样密文中的每一位都可能是数字和字母，规律性就不易察觉得多。 然后是用户 ID 暴露在密文中的问题。这个问题的解决办法是我们可以加一点盐 。盐的取值最好不要太小，太小缺乏隐蔽性；也不能太大，太大会占用过多用户 ID 的取值空间。具体的取值取决于业务需求。 当然，最后是校验位的问题。这个问题我思考之后决定在随机数上作文章。目前的算法，会生成和补全后用户 ID 长度相等的随机数。这有两点问题：一是邀请码长度稍显过长，6 位用户 ID 就会产生 12 位的邀请码；二是随机数没有提供额外的信息，这对密文来说是一种资源浪费。鉴于此，我改变了随机数的生成方式，让它不再随机生成，而是承担起对密文其它部分的校验功能。同时改变了它的长度，把它固定在 2 位。当然，缩短后的校验码就没有办法隔位插入，我就把它放在了密文尾部。用这一套校验方式，理论上能保证 99.9%的误操作可以被后台检测出来而不需要查询数据库。 生成的邀请码如上，相比第一版，可以看到一些很明显的改进。而且理论上可以容纳 1000 万的用户量，比第一版的 10 万位有了很大提升。 但是这一版的算法仍有问题，细心的同学会发现 6 个验证码的 2~5 位是一样的。这是因为低位的变化不足以影响到高位，导致高位的字符没有发生变化。这样的算法在安全性上是比较薄弱的，攻击人可以利用这一规律大大降低猜测的区间。而且密文和密钥（超参数，本文中就是 salt 和 prime1）之间的关系比较直接，没有进行进一步的处理。现代密码学认为，密码的安全性应该由密钥来保障而不是加密算法，如果密钥和密文之间的联系过于直接，密码的安全性便会削弱。当然，密码学上对这些问题有解决方法，那就是扩散和混淆。 扩散和混淆扩散 (diffusion) 和混淆 (confusion) 是 C.E.Shannon 提出的设计密码体制的两种基本方法，其目的是为了抵抗对手对密码体制的统计分析。在分组密码的设计中，充分利用扩散和混淆，可以有效地抵抗对手从密文的统计特性推测明文或密钥。扩散和混淆是现代分组密码的设计基础。 所谓扩散就是让明文中的每一位影响密文中的许多位，或者说让密文中的每一位受明文中的许多位的影响。这样可以隐蔽明文的统计特性。当然，理想的情况是让明文中的每一位影响密文中的所有位，或者说让密文中的每一位受明文中所有位的影响。 所谓混淆就是将密文与密钥之间的统计关系变得尽可能复杂，使得对手即使获取了关于密文的一些统计特性，也无法推测密钥。使用复杂的非线性代替变换可以达到比较好的混淆效果，而简单的线性代替变换得到的混淆效果则不理想。可以用”揉面团”来形象地比喻扩散和混淆。当然，这个”揉面团”的过程应该是可逆的。乘积和迭代有助于实现扩散和混淆。选择某些较简单的受密钥控制的密码变换，通过乘积和迭代可以取得比较好的扩散和混淆的效果。 改进后的算法我用扩散和混淆的方式对算法进行了改进。 扩散的方式很简单，只需要将个位和其它每一位作和后取余，即可把变化传导到每一位。为了隐蔽，我还把变化进行了放大：1id = id * PRIME1; PRIME1 可以为任意随机数，最好和 36 以及 10^n（n 为用户 id 位数）互质。这是因为根据循环群 的性质：若 m 和 p 互质，则 ( id * m ) % p 的结果遍历[0, p) 的所有整数。保证了放大后结果的分布和原数据的分布同样均匀。为了使结果看起来更随机，我还给每一位分配了不同系数： 12345678910id = id * PRIME1;id = id + SALT;int[] b = new int[CODE_LENGTH];b[0] = id;for (int i = 0; i &lt; 5; ++i) &#123; b[i + 1] = b[i] / ARY; b[i] = (b[i] + b[0] * i) % ARY;&#125;b[5] = (b[0] + b[1] + b[2]) * PRIME1 % ARY;b[6] = (b[3] + b[4] + b[5]) * PRIME1 % ARY; ARY 表示进制，这里是 36，也可以设置成其它的数，比如 62（字母区分大小写）。代码的第 7、9、10 行中我分别对每一位设置了不同的系数，使得每一次的增量显得更不固定。 然后是混淆。混淆我用了P-box 的方式，其实就是将数字洗牌。比如把 1234567 洗成 5237641。这样处理之后可以隐藏密钥和密文之间的关系。洗牌的方式也很简单，选择一个和 CODE_LENGTH（本文中为 7）互质的数 PRIME2，和数组角标相乘取余即可（原理同 PRIME1）。最终的代码如下： 1234567891011121314151617public static String inviCodeGenerator(int id) &#123; id = id * PRIME1; id = id + SALT; int[] b = new int[CODE_LENGTH]; b[0] = id; for (int i = 0; i &lt; 5; ++i) &#123; b[i + 1] = b[i] / ARY; b[i] = (b[i] + b[0] * i) % ARY; &#125; b[5] = (b[0] + b[1] + b[2]) * PRIME1 % ARY; b[6] = (b[3] + b[4] + b[5]) * PRIME1 % ARY; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; sb.append(HEX_36_Array.charAt(b[(i * PRIME2) % CODE_LENGTH])); &#125; return sb.toString();&#125; 测试结果如下： 完美符合我们的需求^_^ 邀请码和用户 ID 的转换也很简单，因为加密的过程都是可逆的，所以只需将加密过程作逆变换即可。这里要提一点就是我们是设置了校验位的，所以可以在解密的过程中对邀请码进行校验，如果是用户的误输入或者有人企图构造邀请码恶意攻击，我们在业务层就可以检测出来，不需要拿到数据层去做校验。具体的解密代码如下： 1234567891011121314151617181920212223242526272829303132public static int inviDecoding(String inviCode) &#123; if (inviCode.length() != CODE_LENGTH) &#123; return -1; &#125; int res = 0; int a[] = new int[CODE_LENGTH]; int b[] = new int[CODE_LENGTH]; char[] c = new char[CODE_LENGTH]; for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; a[(i * PRIME2) % CODE_LENGTH] = i; &#125; for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; c[i] = inviCode.charAt(a[i]); &#125; for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; a[i] = HEX_36_Array.indexOf(c[i]); &#125; b[5] = (a[0] + a[1] + a[2]) * PRIME1 % ARY; b[6] = (a[3] + a[4] + a[5]) * PRIME1 % ARY; if (a[5] != b[5] || a[6] != b[6]) &#123; return -1; &#125; for (int i = 4; i &gt;= 0; --i) &#123; b[i] = (a[i] - a[0] * i + ARY * i) % ARY; &#125; for (int i = 4; i &gt; 0; --i) &#123; res += b[i]; res *= ARY; &#125; res = ((res + b[0]) - SALT) / PRIME1; return res;&#125; 代码 18~22 行就是在作校验。 总结不同的业务有不同的需求，市面上通用的方案可能只能满足大部分共性的需求，但对于某些特定的需求，市面上找不到完善的解决方案，这时候就需要我们独立解决问题的能力。本科的时候觉得密码学没用，没想到在这用上了。越来越觉得世上没有无用的知识，多积累一些总是好的^_^","categories":[],"tags":[{"name":"密码学","slug":"密码学","permalink":"http://yoursite.com/tags/密码学/"}],"keywords":[]},{"title":"分布式缓存的一致性 Hash 算法","slug":"分布式缓存的一致性Hash算法","date":"2018-03-16T03:58:32.000Z","updated":"2019-03-09T07:16:14.795Z","comments":true,"path":"2018/03/16/分布式缓存的一致性Hash算法/","link":"","permalink":"http://yoursite.com/2018/03/16/分布式缓存的一致性Hash算法/","excerpt":"","text":"分布式有利于提高网站的可用性、伸缩性和安全性。分布式缓存，顾名思义，就是将缓存服务器作分布式配置，提高集群性能和可伸缩性。然而对分布式缓存集群而言，不能像应用服务器一样使用简单的负载均衡手段来实现，因为分布式缓存服务器集群中不同服务器中缓存的数据各不相同，缓存访问请求不可以在缓存服务器集群中的任意一台处理，必须先找到缓存有需要数据的服务器，然后才能访问。这个特点会严重制约分布式缓存集群的伸缩性设计，因为新上线的缓存服务器没有缓存任何数据，而已下线的缓存服务器还缓存着网站的许多热点数据。 分布式缓存算法的主要设计目标，就是在保证负载均衡的同时，尽可能让新上线的缓存服务器对整个分布式缓存集群影响最小，也就是说新加入缓存服务器后应使整个缓存服务器集群中已经缓存的数据尽可能还被访问到，同时新增服务器对原有服务器的影响要尽可能均衡。 余数 hash 算法分布式缓存的算法要保证对于一个确定数据，它所在的服务器也必须是确定的。比如对于键值对 &lt;’BEIJING’,DATA&gt;，每一次查找 ‘BEIJING’ 这个关键词，系统总是访问相同的服务器去读取数据。这样，只要服务器还缓存着该数据，就能保证命中。 余数 hash 是一个不错的办法:用服务器数目除缓存数据 KEY 的 hash 值，余数为服务器列表下标编号。假设我们有三台服务器，要存键值对 &lt;’BEIJING’,DATA&gt; 到缓存。则先计算 ‘BEIJING’的 hash 值是 490806430（Java 中的 HashCode() 返回值），用服务器数目 3 除该值，得到余数 1，将其保存到 NODE1 上，以后想要读取数据 ‘BEIJING’ 的时候，只要服务器数量不变，一定会定位 NODE1 上。同时，由于 HashCode 具有随机性，使用余数 hash 算法可保证缓存数据在整个缓存服务器集群中比较均匀地分布。 对余数 Hash 算法稍加改进，还能满足对不同硬件性能的服务器集群作负载均衡的需求。比如 3 台服务器中，第 2 台服务器的性能是另外 2 台的 2 倍，这时我们可以调整算法，把除数设置为 4，当余数为 1、2 的时候，将数据存入 NODE2，实现加权负载均衡。 事实上，如果不需要考虑缓存服务器集群伸缩性，余数 hash 几乎可以满足绝大多数的缓存路由需求。 余数 hash 算法的不足然而，当考虑到缓存服务器集群伸缩性的时候，余数 hash 算法的不足就暴露出来了。假设由于业务发展，网站需要将 3 台缓存服务器扩容至 4 台。这时用户再次访问 ‘BEIJING’ 这个数据的时候，除数变成了 4，用 4 除‘BEIJING’的 Hash 值 490806430，余数为 2，对应 NODE2。由于数据 &lt;’BEIJING’,DATA&gt; 缓存在 NODE1，对 NODE2 的读缓存操作失败，缓存没有命中。 很容易就可以计算出，3 台服务器扩容至 4 台的时候，大约只能命中 25%的缓存（3/4），随着服务器集群规模的增大，这个比例线性上升。当 100 台服务器的集群加入一台新服务器，不能命中的概率是 99%（N/(N+1)）。 这个结果显然是无法接受的，因此我们需要改进这种算法，提高增加新机器后的缓存命中率。 一致性 hash 算法一致性 hash 算法通过一个叫作一致性 hash 环的数据结构提高了新增机器后的缓存命中率，如下图： 具体算法过程为：先构造一个长度为 2^32 的整数环（这个环被称作一致性 hash 环），根据节点名称的 hash 值（其分布范围为[0,2^32-1]）将缓存服务器节点放置在这个 hash 环上。然后根据需要缓存的数据的 KEY 值计算得到其 hash 值（其分布范围也同样为[0,2^32-1]），然后在 hash 环上顺时针查找距离这个 KEY 的 hash 值最近的缓存服务器节点，完成 KEY 到服务器的 hash 映射查找。 在上图中，假设 NODE1 的 hash 值为 3594963423，NODE2 的 hash 值为 1845328979，而 KEY0 的 hash 值为 2534256785，那么 KEY0 在环上顺时针查找，找到的最近的节点就是 NODE1。 当缓存服务器集群需要扩容的时候，只需要将新加入的节点名称（NODE3）的 hash 值放入一致性 hash 环中，由于 KEY 是顺时针查找距离其最近的节点，因此新加入的节点只影响整个环中的一小段，如下图中加粗的一段： 假设 NODE3 的 hash 值是 2790324235，那么加入 NODE3 后，KEY0（hash 值 2534256785）顺时针查找得到的节点就是 NODE3。 如上图所示，加入新节点 NODE3 后，原来的 KEY 大部分还能继续计算到原来的节点，只有 KEY3、KEY0 从原来的 NODE1 重新计算到 NODE3。这样就能保证大部分被缓存的数据还可以继续命中。3 台服务器扩容至 4 台，命中率可以达到 75%，远高于余数 hash 的 25%，而且随着集群规模增大，继续命中原有缓存数据的概率也逐渐增大，100 台服务器扩容增加 1 台，继续命中的概率是 99%。虽然仍有小部分数据缓存在服务器中不能被读到，但这个比例在可接受范围之内。 一致性 hash 算法的不足虽然上述的算法使缓存服务器集群在增加新服务器后的命中率有了大幅提高，但还存在一个小小的问题。 新加入的节点 NODE3 只影响了原来的节点 NODE1，也就是说一部分原来需要访问 NODE1 的缓存数据现在需要访问 NODE3（概率上是 50%）。但是原来的节点 NODE0 和 NODE2 不受影响，这也就意味着，新引入的 NODE3 这个节点只减轻了 NODE1 的压力，假设原先三个节点的压力是一样大的，那么在引入 NODE3 这个节点后，NODE0 和 NODE2 的缓存数据量和负载压力是 NODE1 与 NODE3 的两倍。这个是有违我们负载均衡的初衷的。 怎么办？ 改！ 改进后的一致性 hash 算法我们可以通过增加一层虚拟层的方式解决这个问题：将每台物理缓存服务器虚拟为一组虚拟缓存服务器，将虚拟缓存服务器的 hash 值放置在 hash 环上，KEY 在环上先找到虚拟服务器节点，再得到物理服务器的信息。 这样新加入物理服务器节点时，是将一组虚拟节点加入环中，如果虚拟节点的数目足够多，这组虚拟节点将会影响同样多数目的已经在环上存在的虚拟节点，这些已经存在的虚拟节点又对应不同的物理节点。最终的结果是：新加入一台缓存服务器，将会较为均匀地影响原来集群中已经存在的所有服务器，也就是说分摊原有缓存服务器集群中所有服务器的一小部分负载，其总的影响范围和上面讨论过的相同。如下图所示： 显然每个物理节点对应的虚拟节点越多，各个物理节点之间的负载越均衡，新加入物理服务器对原有的物理服务器的影响越保持一致（这就是一致性 hash 这个名称的由来）。那么在实践中，一台物理服务器虚拟为多少个虚拟服务器节点合适呢？太多会影响性能，太少又会导致负载不均衡，一般说来，经验值是 150，当然根据集群规模和负载均衡的精度需求，这个值应该根据具体情况具体对待。","categories":[],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/tags/读书笔记/"}],"keywords":[]},{"title":"MySQL 索引原理","slug":"MySQL索引原理","date":"2018-03-13T02:48:28.000Z","updated":"2019-03-08T14:11:51.863Z","comments":true,"path":"2018/03/13/MySQL索引原理/","link":"","permalink":"http://yoursite.com/2018/03/13/MySQL索引原理/","excerpt":"","text":"数据库索引是面试中的常考项，也是日常开发中提高程序可用性的实用技巧。我在美团技术点评团队 中找到了这篇文章《MySQL 索引原理及慢查询优化》，摘录了其中专讲索引原理的部分，以供随时复习。 索引目的索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到 m 字母，然后从下往下找到 y 字母，再找到剩下的 sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到 m 开头的单词呢？或者 ze 开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？ 索引原理除了词典，生活中随处可见索引的例子，如火车站的车次表、图书的目录等。它们的原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询 (&gt;、&lt;、between、in)、模糊查询 (like)、并集查询 (or) 等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果 1000 条数据，1 到 100 分成第一段，101 到 200 分成第二段，201 到 300 分成第三段……这样查第 250 条数据，只要找第三段就可以了，一下子去除了 90%的无效数据。但如果是 1 千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是 lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。 磁盘 IO 与预读前面提到了访问磁盘，那么这里先简单介绍一下磁盘 IO 和预读，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分，寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在 5ms 以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘 7200 转，表示每分钟能转 7200 次，也就是说 1 秒钟能转 120 次，旋转延迟就是 1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘 IO 的时间约等于 5+4.17 = 9ms 左右，听起来还挺不错的，但要知道一台 500 -MIPS 的机器每秒可以执行 5 亿条指令，因为指令依靠的是电的性质，换句话说执行一次 IO 的时间可以执行 40 万条指令，数据库动辄十万百万乃至千万级数据，每次 9 毫秒的时间，显然是个灾难。下图是计算机硬件延迟的对比图，供大家参考：考虑到磁盘 IO 是非常高昂的操作，计算机操作系统做了一些优化，当一次 IO 时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次 IO 读取的数据我们称之为一页 (page)。具体一页有多大数据跟操作系统有关，一般为 4k 或 8k，也就是我们读取一页内的数据时候，实际上才发生了一次 IO，这个理论对于索引的数据结构设计非常有帮助。 索引的数据结构前面讲了生活中索引的例子，索引的基本原理，数据库的复杂性，又讲了操作系统的相关知识，目的就是让大家了解，任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘 IO 次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。 详解 b+树如上图，是一颗 b+树，关于 b+树的定义可以参见 B+树，这里只说一些重点，浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块 1 包含数据项 17 和 35，包含指针 P1、P2、P3，P1 表示小于 17 的磁盘块，P2 表示在 17 和 35 之间的磁盘块，P3 表示大于 35 的磁盘块。真实的数据存在于叶子节点即 3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如 17、35 并不真实存在于数据表中。 b+树的查找过程如图所示，如果要查找数据项 29，那么首先会把磁盘块 1 由磁盘加载到内存，此时发生一次 IO，在内存中用二分查找确定 29 在 17 和 35 之间，锁定磁盘块 1 的 P2 指针，内存时间因为非常短（相比磁盘的 IO）可以忽略不计，通过磁盘块 1 的 P2 指针的磁盘地址把磁盘块 3 由磁盘加载到内存，发生第二次 IO，29 在 26 和 30 之间，锁定磁盘块 3 的 P2 指针，通过指针加载磁盘块 8 到内存，发生第三次 IO，同时内存中做二分查找找到 29，结束查询，总计三次 IO。真实的情况是，3 层的 b+树可以表示上百万的数据，如果上百万的数据查找只需要三次 IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次 IO，那么总共需要百万次的 IO，显然成本非常非常高。 b+树性质1.通过上面的分析，我们知道 IO 次数取决于 b+数的高度 h，假设当前数据表的数据为 N，每个磁盘块的数据项的数量是 m，则有 h=㏒(m+1)N，当数据量 N 一定的情况下，m 越大，h 越小；而 m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如 int 占 4 字节，要比 bigint8 字节少一半。这也是为什么 b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于 1 时将会退化成线性表。2.当 b+树的数据项是复合的数据结构，比如 (name,age,sex) 的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当 (张三,20,F) 这样的数据来检索的时候，b+树会优先比较 name 来确定下一步的所搜方向，如果 name 相同再依次比较 age 和 sex，最后得到检索的数据；但当 (20,F) 这样的没有 name 的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候 name 就是第一个比较因子，必须要先根据 name 来搜索才能知道下一步去哪里查询。比如当 (张三,F) 这样的数据来检索时，b+树可以用 name 来指定搜索方向，但下一个字段 age 的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是 F 的数据了， 这个是非常重要的性质，即索引的最左匹配特性。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"},{"name":"收藏","slug":"收藏","permalink":"http://yoursite.com/tags/收藏/"}],"keywords":[]},{"title":"MySQL 作图建库","slug":"MySQL作图建库","date":"2018-02-03T03:59:36.000Z","updated":"2019-03-09T07:13:03.758Z","comments":true,"path":"2018/02/03/MySQL作图建库/","link":"","permalink":"http://yoursite.com/2018/02/03/MySQL作图建库/","excerpt":"","text":"workbench 是 MySQL 自带的数据库可视化工具，相比另一款常用的数据库可视化工具 Navicat 最大的优势就是它是免费的。两者的操作逻辑没有太大区别，不过 workbench 英文的界面对初学者不是太友好。我花了一点时间学习了如何作图，希望能为以后节省时间。 E-R 图是描述数据库关系最常用的方式。优点是直观、详尽。workbench 当然也提供了这种图表的绘制方式，不过在里面叫 EER 图。经过我的一番考证，这个 EER 图就是我们常说的 E-R 图。 一、已有数据库，自动生成 EER 图：①、首先在 mysql workbench 里选中 Database——&gt; reverse engineering ②、然后选择你建立的连接（也就是数据库） ③、接下来一路 next，直到最后选择导出的数据库 ④、自动生成的 E-R 图大概长相如图： 二、先画 EER 图，然后自动生成数据库：①、启动软件过后，注意不需要连接数据库（我第一次就是直接连接数据库了所以找不到设计 ER 模型的地方） ②、点击”+” ,进入模型设计界面 ③、双击 Add Diagram 进入如下设计界面 ④、点击工具栏表格，并在设计区域点击，就会出现一个 table1 并双击它 ⑤、最后 执行 “File”-&gt;”Export” 按钮，选择 Forward Engineer SQL CREATE Script (ctrl+shift+G). 这样就可以把模型导出为 SQL 脚本文件。现在执行这个 SQL 文件就 OK 了","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"Java 中的异常","slug":"Java中的异常","date":"2017-12-29T11:43:59.000Z","updated":"2019-03-09T07:13:39.671Z","comments":true,"path":"2017/12/29/Java中的异常/","link":"","permalink":"http://yoursite.com/2017/12/29/Java中的异常/","excerpt":"","text":"异常是 Java 中非常基础但重要的一块内容，异常的概念理解起来简单，但系统讲起来却并不容易。专门抽了一个下午将它整理出来，以备所需。 一、异常的分类Java 中的异常是指程序在编译或者运行中遇到的问题。 Java 中的异常都继承自 Throwable 类。Throwable 是 java 语言中所有错误和异常的超类，它表示可抛（万物皆可抛）。它有两个子类：Error 和 Exception。 Error：Error 为错误，是程序无法处理的。如 OutOfMemoryError、ThreadDeath 等，出现这种情况只能听之任之，交由 JVM 处理。一般情况下 JVM 也没法子，只好终止线程。 Exception：Exception 是程序可以处理的异常。它有很多子类，比如 IOException,RuntimeException,SQLException 等等。其中 RuntimeException 比较特殊，它表示程序运行中 发生的异常，在编译时可以不接受检查。而其它异常编译时就要接受检查，对于抛出异常的部分，要么 throw 给子类，要么用 try…catch 处理。 常见异常继承关系： 二、常见的异常记住常见的异常可以让我们更高效地调试程序代码，有助于提高开发效率。 runtimeException 子类 ArrayIndexOutOfBoundsException：数组索引越界异常。当对数组的索引值为负数或大于等于数组大小时抛出 ArithmeticException：算术条件异常。譬如：整数除零等 NullPointerException：空指针异常。当应用试图在要求使用对象的地方使用了 null 时，抛出该异常。譬如：调用 null 对象的实例方法、访问 null 对象的属性、计算 null 对象的长度、使用 throw 语句抛出 null 等等 ClassNotFoundException：找不到类异常。当应用试图根据字符串形式的类名构造类，而在遍历 CLASSPAH 之后找不到对应名称的 class 文件时，抛出该异常 NegativeArraySizeException：数组长度为负异常 IOException 子类 IOException：操作输入流和输出流时可能出现的异常 EOFException：文件已结束异常 FileNotFoundException：文件未找到异常 其它 Exception 子类 ClassCastException：类型转换异常类 ArrayStoreException：数组中包含不兼容的值抛出的异常 SQLException：操作数据库异常类 三、异常处理的机制Java 中异常抛出后有两种处理方式：try…catch…finally 机制和 throws 继续抛出机制。 try…catch…finally 机制try…catch…finally 是 java 中的关键字，使用方式如下：1234567try&#123; 抛出异常的代码&#125;catch(异常类)&#123; 处理语句&#125;finally&#123; 处理完后执行语句&#125; 在使用 try…catch…finally 时，若 try 中某一语句抛出了异常，则 try 后面的代码会被屏蔽，直接进行 catch 中的语句。catch 中语句执行到 return(返回语句) 时，会先看看有没有 finally 块，若有，则优先执行 finally 块中语句。如果 catch 块和 finally 块都有 return 语句，则执行 finally 块中的。 抛出异常的代码可能抛出多种异常，处理异常的 catch 也可以有多个 catch，分别处理不同的异常。在执行时，会依次查找 catch 语句，直到找到第一个能 catch 某异常的代码块。 throw 和 throws 机制throws 用在方法声明中，表示这个方法将会抛出某一异常。使用该方法的时候必须对该异常进行处理（try…catch 或 throw）throw 用在语句中，表示抛出一个异常。抛出异常后方法会出栈，方法中后面的代码将不会执行。 四、注意事项 throws 只是再次抛出了某个异常，并没有真正处理异常。在使用中，需要有代码去真正处理抛出的异常 如果子类重写了父类的方法，则子类能够抛出的异常只能是父类的子集（父类所有异常类及它们的子类集合） 对于 runtimeException 异常及其子类，程序可以选择显式处理也可以不处理，交给程序调用者去处理；对于其它 exception（编译时异常），程序必须要显式处理（try…catch）或抛出（throws），交给调用者处理 五、异常处理规约摘自阿里 JAVA 开发手册，个人觉得有助于良好的代码风格形成。 1.【强制】不要捕获 Java 类库中定义的继承自 RuntimeException 的运行时异常类，如：IndexOutOfBoundsException/NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。正例：if(obj != null) {…}反例：try { obj.method() } catch(NullPointerException e){…} 2.【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。 3.【强制】对大段代码进行 try-catch，这是不负责任的表现。catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。 4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。 5.【强制】有 try 块放到了事务代码中，catch 异常后，如果需要回滚事务，一定要注意手动回滚事务。 6.【强制】finally 块必须对资源对象、流对象进行关闭，有异常也要做 try-catch。说明：如果 JDK7，可以使用 try-with-resources 方式。 7.【强制】不能在 finally 块中使用 return，finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。 8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。 9.【推荐】方法的返回值可以为 null，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回 null 值。调用方需要进行 null 判断防止 NPE 问题。说明：本规约明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败，运行时异常等场景返回 null 的情况。 10.【推荐】防止 NPE，是程序员的基本修养，注意 NPE 产生的场景：1） 返回类型为包装数据类型，有可能是 null，返回 int 值时注意判空。反例：public int f(){ return Integer 对象}; 如果为 null，自动解箱抛 NPE。2） 数据库的查询结果可能为 null。3） 集合里的元素即使 isNotEmpty，取出的数据元素也可能为 null。4） 远程调用返回对象，一律要求进行 NPE 判断。5） 对于 Session 中获取的数据，建议 NPE 检查，避免空指针。6） 级联调用 obj.getA().getB().getC()；一连串调用，易产生 NPE。 11.【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的 http/api 开放接口必须使用“错误码”；而应用内部推荐异常抛出；跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess、“错误码”、“错误简短信息”。说明：关于 RPC 方法返回方式使用 Result 方式的理由：1）使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。2）如果不加栈信息，只是 new 自定义异常，加入自己的理解的 error message，对于调用端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输的性能损耗也是问题。 12.【推荐】定义时区分 unchecked/checked 异常，避免直接使用 RuntimeException 抛出，更不允许抛出 Exception 或者 Throwable，应使用有业务含义的自定义异常。推荐业界已定义过的自定义异常，如：DAOException/ServiceException 等。 13.【参考】避免出现重复的代码（Don’t Repeat Yourself），即 DRY 原则。说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是共用模块。正例：一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取：private boolean checkParam(DTO dto){…}","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]}]}