{"meta":{"title":"huzb的小书斋","subtitle":"念念不忘，必有回响","description":"一枚前行的小码农","author":"huzb","url":"http://yoursite.com"},"pages":[{"title":"关于","date":"2019-03-09T04:59:43.829Z","updated":"2019-03-09T04:59:43.828Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"12345678910111213141516171819202122/** * $= * =@.$ * $- $ &amp;@ =&amp;$$$$$$$$$. * $ B$- =&amp; $#&amp;$$$&amp;- -$$$$$$$$$$$&amp; 电子科技大学计算机在读，一枚奋进的Java程序员 * $ $$ .#@$$$@#. .$$$$$$$$$#$$ 念念不忘，必有回响 * .$ @$$$$$$$$$$$ * .$ B$$$# $$$$$$$$$$ * -$ %@. $$$$$$$ * B$ =$$$= * $$ .%$$&amp;= * $&amp; $- * $- @% * &amp;$ %$ * $% B$ * =$ #$ * $$ =$ * -$. $ =$ * $$ #@ B$ * .$- &amp;% @$ * $@ $= $% */"},{"title":"标签","date":"2019-03-08T00:40:27.822Z","updated":"2019-03-08T00:40:27.822Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2019-03-08T00:40:27.819Z","updated":"2019-03-08T00:40:27.819Z","comments":false,"path":"repository/index.html","permalink":"http://yoursite.com/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"Spring 源码浅析——创建单例 bean 对象","slug":"Spring 源码浅析——创建单例 bean 对象","date":"2019-03-04T09:52:47.000Z","updated":"2019-03-08T14:15:24.616Z","comments":true,"path":"2019/03/04/Spring 源码浅析——创建单例 bean 对象/","link":"","permalink":"http://yoursite.com/2019/03/04/Spring 源码浅析——创建单例 bean 对象/","excerpt":"","text":"上一篇概览了源码中容器刷新的部分，但是略过了其中最重要的对象初始化和注入部分。这一篇就以一个普通单例对象为例，看它是如何加入到容器中的。 准备首先定义一个类：1234public class Person &#123; int age; String name;&#125; 在配置类中加入如下配置：1234@Beanpublic Person person()&#123; return new Person();&#125; 那么 Spring 会把一个 Person 类型的对象注入容器中 源码分析直接进入上一篇中略过的finishBeanFactoryInitialization(beanFactory)方法： 1234567891011121314151617181920212223protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 为容器设置一个全局类型转换器，默认没有；类型转换器可以自定义，常见的有 String 转 Date 的类型转换器 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // 检查 beanFactory 是否没有内嵌值解析器，默认是有的 if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(strVal -&gt; getEnvironment().resolvePlaceholders(strVal)); &#125; // 加载第三方模块，如 AspectJ；默认没有 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // 停用临时类加载器 beanFactory.setTempClassLoader(null); // 禁止对除了 bean definition metadata 缓存之外的配置进行修改 beanFactory.freezeConfiguration(); // 实例化所有剩余的不指定懒加载的单例对象（核心部分） beanFactory.preInstantiateSingletons();&#125; 可以看到核心的函数就是最后一行beanFactory.preInstantiateSingletons();，它会对所有剩余的不指定懒加载的单例对象进行实例化操作。我们进入beanFactory.preInstantiateSingletons();中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public void preInstantiateSingletons() throws BeansException &#123; // 获取所有的 bean 定义信息 List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); // 开始实例化所有还未实例化的不指定懒加载的单例对象 for (String beanName : beanNames) &#123; // 合并父 BeanDefinition 与子 BeanDefinition，见附录 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // 判断 bean 是否抽象、是否单例、是否懒加载 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; // FactoryBean 类型的 bean 进行实例化 if (isFactoryBean(beanName)) &#123; // 获取 FactoryBean 的工厂本身（以&amp;开头），如果没有则创建 Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); if (bean instanceof FactoryBean) &#123; final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean; boolean isEagerInit; // 这里需要判断是不是 SmartFactoryBean，因为 SmartFactoryBean 会定义一个 isEagerInit() 方法来决定 getObject() 的实例对象是否懒加载 if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; isEagerInit = AccessController.doPrivileged((PrivilegedAction&lt;Boolean&gt;) ((SmartFactoryBean&lt;?&gt;) factory)::isEagerInit, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; // 对非懒加载的 bean 实例化 if (isEagerInit) &#123; getBean(beanName); &#125; &#125; &#125; // 没有实现 FactoryBean 接口的直接实例化 else &#123; getBean(beanName); &#125; &#125; &#125; // 调用所有 SmartInitializingSingleton 类型的实现类的 afterSingletonsInstantiated 方法；通过名字可以知道它表示单例对象实例化后需要做的操作 for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125;&#125; 可看出，不管是 FactoryBean 对象本身、FactoryBean 的 getObject() 创建的对象还是普通单例对象，最终都会依赖getBean(beanName)这个方法来实例化对象。我们进入这个方法来看：```java","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring 源码浅析——容器刷新流程概览","slug":"Spring 源码浅析——容器刷新流程概览","date":"2019-03-03T09:08:49.000Z","updated":"2019-03-08T15:51:34.117Z","comments":true,"path":"2019/03/03/Spring 源码浅析——容器刷新流程概览/","link":"","permalink":"http://yoursite.com/2019/03/03/Spring 源码浅析——容器刷新流程概览/","excerpt":"","text":"本文是 Spring 源码浅析系列的第一篇。Spring 版本是 Spring Boot 2.1.2.RELEASE （即 Spring 5.1.4），以默认配置启动，分析框架工作的原理。 众所周知，Spring 以容器管理所有的 bean 对象，容器的实体是一个 BeanFactory 对象。但我们常用的容器是另一个 ApplicationContext ，它在内部持有了 BeanFactory，所有和 BeanFactory 相关的操作都会委托给内部的 BeanFactory 来完成。 ApplicationContext 的继承关系如下图所示： ApplicationContext 是一个接口，ClassPathXmlApplicationContext和AnnotationConfigApplicationContext是两个比较常用的实现类，前者基于 xml 使用，后者基于注解使用。SpringBoot 中默认后面一种。 ApplicationContext 也继承了 BeanFactory 接口，BeanFactory 的继承关系如下图所示： 从继承关系我们可以获得以下信息： ApplicationContext 继承了 ListableBeanFactory，这个 Listable 的意思就是，通过这个接口，我们可以获取多个 Bean，最顶层 BeanFactory 接口的方法都是获取单个 Bean 的。 ApplicationContext 继承了 HierarchicalBeanFactory，Hierarchical 的意思是说我们可以在应用中起多个 BeanFactory，然后可以将各个 BeanFactory 设置为父子关系。 ApplicationContext 非常重要，所以我们第一篇就看一下 ApplicationContext 初始化的过程。默认配置下，SpringBoot 中的 ApplicationContext 初始化在 refresh() 方法中，为什么叫 refresh() 而不是 init() 呢？因为 ApplicationContext 建立起来以后，其实我们是可以通过调用 refresh() 这个方法重建的，这样会将原来的 ApplicationContext 销毁，然后再重新执行一次初始化操作。 1、容器刷新概览refresh() 是个总览全局的方法，我们可以通过这个方法概览容器刷新的过程：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、检验配置文件格式 prepareRefresh(); // ClassPathXmlApplicationContext 会在这里解析 xml 配置；AnnotationConfigApplicationContext 的解析发在初始化，这里只是简单的获取 // 这里的解析是指把配置信息都提取出来了，保存在了一个 Map&lt;String,BeanDefinition&gt; 中 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean 等 prepareBeanFactory(beanFactory); try &#123; // BeanFactory 准备工作完成后进行的后置处理工作，子类可以自定义实现，Spring Boot 中是个空方法 postProcessBeanFactory(beanFactory); //=======以上是 BeanFactory 的预准备工作======= // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类（注意和之前的 BeanFactoryPostProcessor 的区别） registerBeanPostProcessors(beanFactory); // 初始化 MessageSource 组件（做国际化功能；消息绑定，消息解析） initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器 initApplicationEventMulticaster(); // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前），Spring Boot 中默认没有定义 onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口 registerListeners(); // 初始化所有的 singleton beans（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 容器刷新完成操作 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 通过上面的代码和注释我们总览了容器刷新的整个流程，下面我们来一步步探索每个环节都做了什么。 2、刷新前的准备工作：prepareRefresh()这步比较简单，直接看代码中的注释即可。 1234567891011121314151617protected void prepareRefresh() &#123; // 记录启动时间， // 将 active 属性设置为 true，closed 属性设置为 false，它们都是 AtomicBoolean 类型 this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); // Initialize any placeholder property sources in the context environment // Spring Boot 中是个空方法 initPropertySources(); // 校验配置属性的合法性 getEnvironment().validateRequiredProperties(); // 记录早期的事件 this.earlyApplicationEvents = new LinkedHashSet&lt;&gt;();&#125; 3、获取 Bean 容器前面说过 ApplicationContext 内部持有了一个 BeanFactory，这步就是获取 ApplicationContext 中的 BeanFactory。在 ClassPathXmlApplicationContext 中会做很多工作，因为一开始 ClassPathXmlApplicationContext 中的 BeanFactory 并没有创建，但在 AnnotationConfigApplicationContext 比较简单，直接返回即可。 1234567891011protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; // 通过 cas 设置刷新状态 if (!this.refreshed.compareAndSet(false, true)) &#123; throw new IllegalStateException( \"GenericApplicationContext does not support multiple refresh attempts: just call 'refresh' once\"); &#125; // 设置序列号 this.beanFactory.setSerializationId(getId()); // 返回已创建的 BeanFactory return this.beanFactory;&#125; 4、准备 Bean 容器：prepareBeanFactory()BeanFactory 获取之后并不能马上使用，还要在 BeanFactory 中做一些准备工作，包括类加载器、表达式解析器的设置，几个特殊的 BeanPostProcessor 的添加等。 总结我们大致浏览了一遍 Spring 容器刷新的过程，通过源码我们可以总结出以下几点： Spring 容器在启动的时候，先会将所有注册进来的 Bean 的定义信息保存在 beanDefinitionMap 中，然后在合适的时机创建这些 bean： 用到这个 bean 的时候，利用 getBean() 创建 bean，创建好以后保存在容器中； finishBeanFactoryInitialization() 统一创建剩下所有的 bean； Spring 的诸多功能都依赖于后置处理器（BeanFactoryPostProcessor 和 BeanPostProcessor）的实现，后置处理器工作在各种时期（容器准备完成、bean 初始化之前、bean 初始化之后、容器刷新完成等等），常见的有： AutowiredAnnotationBeanPostProcessor：处理自动注入； AnnotationAwareAspectJAutoProxyCreator：来做 AOP 功能； InstantiationAwareBeanPostProcessor：对象初始化前后； DestructionAwareBeanPostProcessor：对象销毁前； 容器的刷新过程分为两个阶段：容器准备和对象初始化注入。 容器准备阶段会对 BeanFactory 进行一些设置，然后执行 BeanFactory 的后置处理器； 对象初始化注入会依次：注册 bean 的后置处理器——初始化 MessageSource 组件——注册事件派发器——注册事件监听器——初始化和注入剩下的单例 bean——完成刷新工作； 事件驱动模型： ApplicationListener：事件监听； ApplicationEventMulticaster：事件派发； ApplicationContext.publishEvent()：事件发布；","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Redis 高可用和分布式","slug":"Redis 高可用和分布式","date":"2019-02-23T08:20:24.000Z","updated":"2019-03-08T14:13:37.716Z","comments":true,"path":"2019/02/23/Redis 高可用和分布式/","link":"","permalink":"http://yoursite.com/2019/02/23/Redis 高可用和分布式/","excerpt":"","text":"Redis 可以单机部署，但会带来单点问题和性能瓶颈。为此 Redis 提供了主从复制、哨兵和集群的方式来解决这些问题。 一、主从复制主从复制可以将两台或者多台服务器之间的数据同步，这样在主服务器下线后，从服务器可以继续对外提供服务，保证了系统的高可用；另外主从复制也可以进行读写分离，主服务器只提供写操作或少量的读，把多余读请求通过负载均衡算法分流到单个或多个从服务器上。 Redis 的复制功能分为同步和命令传播两个操作，同步操作用于将从服务器的状态更新至主服务器当前的状态；命令传播操作用于在主服务状态被修改时，让主从状态重新保持一致。 同步可以通过使用SLAVEOF &lt;host&gt; &lt;port&gt;命令来让一个服务器成为另一个服务器的从服务器。此时从服务器会自动向主服务器发送 SYNC 的命令进行同步操作，步骤如下： 1）主服务器收到 SYNC 命令后执行 BGSAVE 命令，在后台生成一个 RDB 快照文件，并用一个缓冲区记录从现在开始执行的所有写命令。 2）从服务器从主服务器接收到 RDB 文件后，丢弃所有旧数据，载入主服务器发来的快照文件。 3）主服务器把记录在缓冲区的命令发送给从服务器，从服务器执行这些命令，同步完成。 但是这种同步方式会带来一个断线后重复值效率低的问题：如果从服务器短暂掉线后重连主服务器，主从之间不得不重新同步所有数据，这是没有必要的，因为从服务器仍然保留了大部分数据。Redis 2.8 推出了部分重同步的方式解决了这个问题。 部分重同步部分重同步用于处理断线后复制的情况。Redis 部分重同步的实现由以下三个部分构成： 主服务器和从服务器的复制偏移量：复制偏移量表示当前从服务器从主服务器中接收了多少字节的数据。 主服务器的复制积压缓冲区：复制积压缓冲区以 FIFO 的形式保存了最近的写命令。 服务器运行 id：用于标识服务器 当从服务器断线重连之后，会向主服务器发送一个PSYNC &lt;runid&gt; &lt;offset&gt;的命令，offset 即当前从服务器的复制偏移量，runid 是从服务器记录的掉线之前的主服务器 id。主服务器在收到 PSYNC 后，会首先对比自己的 runid 和传过来的 runid 是否一致，如果一致，说明这台从服务器之前和自己同步过数据，然后根据 offset 的差值把复制积压缓冲区的数据同步给从服务器。这样就实现了增量更新。 命令传播在同步完成之后，主从服务器就进入了命令传播阶段。在这个阶段，主服务器发生写操作后，会把相应的命令发送给从服务器执行，这样主从之间就保持了数据一致性。 从服务器也会向主服务器发送命令REPLCONF ACK &lt;replication_offset&gt;，这是一个心跳检测命令，每隔 1 秒就会发一次，它有以下两个作用： 检测主从服务器的网络连接状态：主服务器会记录从服务器上次心跳检测的时间，如果超过 1 秒，就说明连接出了故障。如果主服务器和大量从服务器之间的连接出了故障，比如有 3 台以上的从服务器超过 10 秒没有心跳检测，则会拒绝执行写命令。 检测命令丢失：和部分重同步类似，心跳检测也会发送从服务器的当前复制偏移量（replication_offset），主服务器在接收到心跳检测后会检查这个偏移量是否和自己的一致，如果不一致会补发缺失的数据。 二、Sentinel（哨兵）Sentinel（哨兵）是 Redis 高可用的解决方案：由一个或多个 Sentinel 实例组成的 Sentinel 系统可以监视任意多个主服务器以及这些服务器下的所有从服务器。在主服务器进入下线状态时，自动将下线服务器下的某个从服务器升级成主服务器。 Sentinel 系统是由 Sentinel 实例组成的，彼此间通过命令连接相互通信： Sentinel 实例会以每秒一次的频率向自己监控的主服务器发送 PING 命令，主服务器会回复该命令，以此来监控主服务器的在线状态。当 Sentinel 实例发送 PING 命令之后等待超过配置指定时间之后，Sentinel 实例会判定该主服务器处于主观下线状态。 当一个 Sentinel 实例判定自己监控的某个主服务器为主观下线之后，它会询问其它监控该服务器的 Sentinel 实例，当超过配置指定数量的 Sentinel 实例也认为该服务器已下线时，Sentinel 实例会判定该服务器为客观下线。 当有超过配置指定数量的 Sentinel 实例认为该服务器客观下线之后，监视该服务器的各个 Sentinel 会进行协商，通过 Raft 算法选举出一个领头 Sentinel 对下线主服务器执行故障转移操作。 故障转移操作包括三个部分： 1）在已下线主服务器属下的所有从服务器中挑选一个从服务器，将其转换为主服务器。 2）让其它从服务器改为复制新的主服务器（发送 SLAVEOF 命令）。 3）将原来的主服务器设置为从服务器。 三、集群Redis 集群是 Redis 提供的分布式数据库方案，集群通过分片的方式进行数据共享，并提供复制和转移功能。 集群中的节点可以通过向其它节点发送CLUSTER MEET &lt;ip&gt; &lt;port&gt;命令邀请其它节点加入集群。 集群的整个数据库被分为 16384 个槽，数据库中的每一个键都属于这 16384 个槽中的一个，只有当集群中的每个槽都有节点在处理时，集群才处于上线状态。 我们可以通过CLUSTER ADDSLOTS [slot...]命令将一个或多个槽指派给节点。每个节点会记录自己和集群中其它节点被指派的槽。 节点在接到一个命令请求时，会先检查这个命令请求要处理的键所在的槽是否由自己负责，如果不是的话，节点将向客户端返回一个 MOVED 错误，MOVED 错误携带的信息可以指引客户端转向正确的节点。 除了可以指派槽以外，我们还可以通过 redis-trib 这个软件将槽重新分片。重新分片的关键是将属于某个槽的所有键值对从一个节点转移至另一个节点。 在重新分片期间，如果客户端向原来的节点请求键 k，而 k 已经被转移到另一个节点时，节点会返回一个 ASK 错误，指引客户端到新的节点。 MOVED 错误表示槽的负责权已经永远转移了，而 ASK 错误只是两个节点在槽迁移时的临时措施。 故障转移为了集群的高可用，集群中的节点也分主节点和从节点，从节点复制主节点的数据。 主节点之间通过 PING 消息来互相确认对方的在线状态，如果没有在规定时间内收到对方的 PONG，则会把对方标记为疑似下线状态，并将这个消息告知给集群中的其它主节点。当某个主节点发现集群中有超过半数主节点认为某个主节点疑似下线，它就会把这个主节点标记为下线，并广播给集群中其它的节点（主节点+从节点）。 当从节点发现自己正在复制的主节点已下线时，会通过 raft 算法选举出新的主节点。选举的候选节点是已下线主节点的所有从节点，投票节点是其它所有主节点。最后一个从节点会被推选为新的主节点，接管由已下线节点负责处理的槽。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"CMS,G1 和 ZGC","slug":"CMS-G1 和 ZGC","date":"2019-02-21T11:23:54.000Z","updated":"2019-03-08T14:03:47.398Z","comments":true,"path":"2019/02/21/CMS-G1 和 ZGC/","link":"","permalink":"http://yoursite.com/2019/02/21/CMS-G1 和 ZGC/","excerpt":"","text":"本文主要介绍比较常用的垃圾收集器：CMS，G1 和 ZGC。CMS 是服务器使用比较多的收集器，侧重点在低停顿；JDK 9 以后，G1 成为了默认收集器，它的设计目标是停顿可控；最新的 JDK 11 中，加入了实验性质的 ZGC，这个收集器可以将停顿时间降至 10ms 以下。 1、JDK1.8 之前默认的垃圾收集器JDK1.8 之前（包括 1.8），默认的垃圾收集器是 Parallel Scavenge（新生代）+Parallel Old（老年代）。 Parallel Scavenge 是一款基于复制算法的新生代垃圾处理器，它的设计目标是吞吐量优先。所谓吞吐量就是指：用户运行时间/(用户运行时间+垃圾回收时间)。很显然吞吐量越大越好。用户需要给 Parallel Scavenge 设置一个吞吐量的目标，然后 Parallel Scavenge 会自动控制每一次垃圾回收的时间。另外 Parallel Scavenge 还有自适应调节策略，只要打开-XX:+UseAdaptiveSizePolicy，它就会动态调整新生代大小、Eden 与 Survivor 的比例、晋升老年代对象大小等参数，以达到最大的吞吐量。 和 Parallel Scavenge 配套使用的老年代收集器是 Parallel Old，这是一款基于标记-整理算法的垃圾处理器。 为什么不用 CMS 作为老年代收集器？ 这是因为 Parallel Scavenge 的作者没有使用 HotSpot VM 给定的代码框架，而是自己独立实现了一个。这就导致 Parallel Scavenge 和当时大部分收集器都不兼容，其中就包括 CMS。所以在 1.8 时代，比较流行的有两套收集器，一套是 Parallel Scavenge（新生代）+Parallel Old（老年代），另一套是 ParNew（使用复制算法，新生代）+ CMS（老年代） 2、低停顿的 CMSCMS，Concurrent Mark Sweep，是一款老年代的收集器，它关注的是垃圾回收最短的停顿时间。命名中 Concurrent 说明这个收集器是有与工作执行并发的能力的，Mark Sweep 则代表算法用的是标记-清除算法。 CMS 的工作原理分为四步： 初始标记：单线程执行，仅仅把 GC Roots 的直接关联可达的对象标记一下，速度很快，需要停顿。 并发标记：对于初始标记过程所标记的初始对象，进行并发追踪标记，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：清除之前标记的垃圾，不需要停顿。 由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 由于 CMS 以上特性，缺点也是比较明显的： 标记-清除算法会导致内存碎片比较多。 CMS 的并发能力依赖于 CPU 资源，所以在 CPU 数少和 CPU 资源紧张的情况下，性能较差。 无法处理浮动垃圾。浮动垃圾是指在并发清除阶段用户线程继续运行而产生的垃圾，这部分垃圾只能等下次 GC 时处理。由于浮动垃圾的存在，CMS 不能等待内存耗尽的时候才进行 GC，而要预留一部分内存空间给用户线程。这里会浪费一些空间。 为什么不用标记-整理？ 因为在并发清除阶段，其它用户线程还在工作，要保证它们运行的资源不受影响。而标记-整理算法会移动对象，所以不能使用标记-整理。 3、停顿可控的 G1G1 是一款可以掌管所有堆内存空间的收集器。G1 把堆划分成多个大小相等的独立区域（Region），新生代和老年代不再物理隔离。 通过引入 Region 的概念，将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法避免了空间碎片化，也提高了回收的灵活性——G1 会根据平均每个 Region 回收需要的时间（经验预测）和各个 Region 的回收收益，制定回收计划。 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 三种 GC 模式： Young GC，发生于新生代空间不足时，回收全部新生代，可以通过控制新生代 Region 的个数来控制 Young GC 的时间开销。 Mixed GC，当堆中内存使用超过整个堆大小的 InitiatingHeapOccupancyPercent（默认 45）时启动。 回收全部新生代，并根据预期停顿时间回收部分收益较高的老年代。 Full GC（JDK 9 引入），发生于老年代空间不足时，相当于执行一次 STW 的 full gc。 整体的执行流程： 初始标记：标记了从 GC Root 开始直接关联可达的对象，速度很快，单线程，需停顿。 并发标记：对于初始标记过程所标记的初始对象，进行并发追踪标记，并记录下每个 Region 中的存活对象信息用于计算收益，不需要停顿。 最终标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 筛选回收：根据 GC 模式、回收时间和回收收益确定回收计划，回收后的空 Region 会加入到空闲列表，需要停顿。 由于 G1 会把存活的对象集中起来放到 Survivor Region 中，并通过空闲列表整理所有的空 Region，所以整体来看是基于“标记 - 整理”算法实现的收集器；但从局部（两个 Region 之间）上来看又是基于“复制”算法实现的。但不论如何，这两种算法都需要移动对象，所以 G1 的回收阶段是需要停顿的。 4、几乎无停顿的 ZGC在 JDK 11 当中，加入了实验性质的 ZGC。它的回收耗时平均不到 2 毫秒。它是一款低停顿高并发的收集器。ZGC 几乎在所有地方并发执行的，除了初始标记的是 STW 的。所以停顿时间几乎就耗费在初始标记上，这部分的实际是非常少的。那么其他阶段是怎么做到可以并发执行的呢？ZGC 主要新增了两项技术，一个是着色指针 Colored Pointer，另一个是读屏障 Load Barrier。 着色指针 Colored Pointer ZGC 利用指针的 64 位中的几位表示 Finalizable、Remapped、Marked1、Marked0（ZGC 仅支持 64 位平台），以标记该指向内存的存储状态。相当于在对象的指针上标注了对象的信息。注意，这里的指针相当于 Java 术语当中的引用。 在这个被指向的内存发生变化的时候（内存在整理时被移动），颜色就会发生变化。 读屏障 Load Barrier 由于着色指针的存在，在程序运行时访问对象的时候，可以轻易知道对象在内存的存储状态（通过指针访问对象），若请求读的内存在被着色了。那么则会触发读屏障。读屏障会更新指针再返回结果，此过程有一定的耗费，从而达到与用户线程并发的效果。 把这两项技术联合下理解，引用 R 大（RednaxelaFX）的话 与标记对象的传统算法相比，ZGC 在指针上做标记，在访问指针时加入 Load Barrier（读屏障），比如当对象正被 GC 移动，指针上的颜色就会不对，这个屏障就会先把指针更新为有效地址再返回，也就是，永远只有单个对象读取时有概率被减速，而不存在为了保持应用与 GC 一致而粗暴整体的 Stop The World。 ZGC 和 G1 一样将堆划分为 Region 来清理、移动，稍有不同的是 ZGC 中 Region 的大小是会动态变化的。 ZGC 的回收流程如下： 1、初始停顿标记 停顿 JVM 地标记 Root 对象，1，2，4 三个被标为 live。 2、并发标记 并发地递归标记其他对象，5 和 8 也被标记为 live。 3、移动对象 对比发现 3、6、7 是过期对象，也就是中间的两个灰色 region 需要被压缩清理，所以陆续将 4、5、8 对象移动到最右边的新 Region。移动过程中，有个 forward table 纪录这种转向。 活的对象都移走之后，这个 region 可以立即释放掉，并且用来当作下一个要扫描的 region 的 to region。所以理论上要收集整个堆，只需要有一个空 region 就 OK 了。 4、修正指针 最后将指针都妥帖地更新指向新地址。 ZGC 虽然目前还在 JDK 11 还在实验阶段，但由于算法与思想是一个非常大的提升，相信在未来不久会成为主流的 GC 收集器使用。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"Redis 事件、事务和 pipeline","slug":"Redis 事件、事务和 pipeline","date":"2019-02-12T11:34:57.000Z","updated":"2019-03-08T14:14:14.002Z","comments":true,"path":"2019/02/12/Redis 事件、事务和 pipeline/","link":"","permalink":"http://yoursite.com/2019/02/12/Redis 事件、事务和 pipeline/","excerpt":"","text":"本文介绍了 Redis 中事件的类型和事件的调度与执行，以及对批量事件处理的两种方式：事务和 pipeline。 一、事件Redis 服务器是一个事件驱动程序。Redis 的事件有两类： 文件事件：服务器通过套接字与客户端连接，文件事件就是服务器对套接字操作的抽象。 时间事件：服务器对定时操作的抽象。 文件事件Redis 包装了底层的 select、epoll 等来实现自己的网络事件处理器。它使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。 时间事件服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。 时间事件又分为： 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 时间事件中的属性 when 会记录下次执行的时间，周期性事件在执行后会更新 when 的值，而定时事件会被删除。 Redis 将所有时间事件都放在一个无序链表中，由时间事件执行器通过遍历整个链表查找出已到达的时间事件，并调用相应的事件处理器。 事件的调度与执行服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。 事件调度与执行由 aeProcessEvents 函数负责，伪代码如下：12345678910111213141516def aeProcessEvents(): # 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() # 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms &lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) # 处理所有已产生的文件事件 procesFileEvents() # 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下： 12345678def main(): # 初始化服务器 init_server() # 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() # 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下： 二、事务Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 一个事务包括三个步骤： 事务开始：事务以 MULTI 开始，返回 OK 命令。 命令入队：每个事务命令成功进入队列后，返回 QUEUED。 事务执行：EXEC 执行事务。 Redis 不支持事务回滚功能，事务中的一个 Redis 命令执行失败以后，会继续执行后续的命令。 三、pipeline多个命令被一次性发送给服务器，而不是一条一条发送，这种方式被称为流水线，它可以减少客户端与服务器之间的网络通信次数从而提升性能。 可以通过redis-cli --pipe的方式批量发送命令。如cat commands.txt | redis-cli --pipe，commands.txt 中的命令会被以 RESP 协议（这是一个 Redis 自行规定的协议，用于命令的批量执行）的格式发给服务器，服务器也会返回一个 RESP 格式的结果。 当然我们不用自己去实现这个协议，Jedis 为我们实现好了，我们可以很方便地调用：1234567891011Jedis jedis = new Jedis(\"localhost\", 6379);//使用 pipelinePipeline pipeline = jedis.pipelined();//删除 listspipeline.del(\"lists\");//循环添加 10000 个元素for(int i = 0; i &lt; 10000; i++)&#123; pipeline.rpush(\"lists\", i + \"\");&#125;//执行pipeline.sync();","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"Redis 持久化和过期机制","slug":"Redis 持久化和过期机制","date":"2019-02-08T06:12:30.000Z","updated":"2019-03-08T14:13:02.687Z","comments":true,"path":"2019/02/08/Redis 持久化和过期机制/","link":"","permalink":"http://yoursite.com/2019/02/08/Redis 持久化和过期机制/","excerpt":"","text":"本文主要介绍了 Redis 持久化的两种机制：RDB 和 AOF，以及键过期的策略：惰性删除和定期删除，还有 RDB、AOF 和复制功能对过期键的处理。 RDBRDB 是 Redis 持久化的第一种方式。有两个 Redis 命令可以用于生成 RDB 文件，一个是 SAVE，另一个是 BGSAVE。SAVE 会阻塞 Redis 服务器进程，执行时 Redis 服务器会阻塞所有客户端发送的命令。12redis&gt; SAVEOK BGSAVE 会派生出一个子进程执行，执行时仍可继续处理客户端的命令，但会拒绝客户端 SAVE 和 BGSAVE 的命令，延迟 BGREWRITEAOF 命令。12redis&gt; BGSAVEBackground saving started 执行条件SAVE 命令会阻塞服务器，所以只能手动执行。BGSAVE 可以在不阻塞的情况下执行，所以可以配置 save 选项让服务器每隔一段时间自动执行一次。 比如我们可以向服务器提供以下配置：123save 900 1save 300 10save 60 10000 那么只要满足以下三个条件中的任意一个即可被执行： 服务器在 900 秒之内对数据库进行了至少 1 次修改。 服务器在 300 秒之内对数据库进行了至少 10 次修改。 服务器在 60 秒之内对数据库进行了至少 10000 次修改。 为了实现这一功能，服务器会维持一个记录距离上次保存之后修改的次数的 dirty 计数器和一个记录上次保存时间的 lastsave 属性。 周期操作函数 serverCron 默认每个 100 毫秒就会执行一次，它的其中一项工作就是检查 save 选项设置的条件是否满足，如果满足的话就会执行 BGSAVE 命令。 文件内容RDB 文件有多个部分，包括握手字段 ‘REDIS’ 字符串，版本号，数据库，’EOF’ 和校验字段。 核心部分是数据库字段，数据库字段包括了握手字段 ‘SELECTDB’，数据库编号和键值对，数据库编号指示了这是第几个数据库，而键值对则保存了各项数据。 键值对中除了类型和数据，还可能会有过期时间。对于不同类型的键值对，RDB 文件会用不同的方式来保存它们。 RDB 文件本身是一个经过压缩的二进制文件，每次 SAVE 或者 BGSAVE 都会创建一个新的 RDB 文件，不支持追加操作。 AOFAOF 是 Redis 持久化的第二种方式，在 AOF 和 RDB 同时开启时，服务器会优先考虑从 AOF 恢复数据，因为 AOF 每次记录间隔的时间更短。 和 RDB 直接记录键值对不同，AOF 记录的是命令。服务器在执行完一个写命令以后，会把这条命令追加到服务器 aof_buf 缓冲区的末尾，并在一个适当的时候写入文件。重建时服务器会创建一个伪客户端，依次执行文件中的命令即可完成数据的载入。 文件的写入与同步AOF 在持久化时会调用操作系统的 write 函数，但通常该函数会把数据保存在一个内存缓冲区里面而不是立刻刷入磁盘。这就带来一个安全问题。为了避免这个问题操作系统又提供了 fsync 和 fdatasync 两个强制刷盘的同步函数。我们把 write 称为写入，把 fsync 和 fdatasync 称为同步。 服务器会在每次事件循环结束之前根据 appendfsync 选项写入和同步 aof_buf 中的数据： always：写入并同步 everysec：写入，如果距离上次同步超过 1 秒，则同步 no：只写入，何时同步由操作系统决定 AOF 重写随着服务器运行时间的流逝，AOF 文件中的内容会越来越多，文件的体积也会越来越大，不仅会对宿主计算机造成影响，也拖慢了数据恢复所需要的时间。 AOF 重写是指重新生成一个 AOF 文件替换原来的 AOF 文件。但这里的重写不会对原有的文件进行读取、分析或者写入，而是把数据库中的键值对折算成命令，重新写入新的文件。 重写是一个耗时的操作，因此 Redis 把它放到后台去操作，对应的指令是 BGREWRITEAOF。在重写过程中服务器还可能接收新的指令，因此 Redis 会维护一个 AOF 重写缓冲区，记录重写期间的写命令，在重写完成后追加到 AOF 文件末尾。 RDB 和 AOF 对比RDB 的优点： RDB 是一个非常紧凑的文件，它的体积更小，且可以选择持久化的时间，适合做备份的文件。比如每天的备份，每月的备份。 RDB 对主进程更友好，父进程只需要 fork 出一个子进程，无须执行任何磁盘 I/O 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 RDB 的缺点： 因为 RDB 文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少 5 分钟才保存一次 RDB 文件，间隔时间比较长。 RDB 虽然会把持久化的操作交给子进程，但每次都会从头开始，在数据集比较庞大时， fork() 可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 AOF 的优点： AOF 使用追加的方式，每次写入时间很短，因此可以允许更短间隔的持久化操作，比如 1 秒。 AOF 文件的可读性比较好，如果你不小心执行了一条命令，只要 AOF 文件未被重写，那么只要停止服务器，移除 AOF 文件里的该条命令然后重启 Redis 即可。 AOF 的缺点： 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 使用 fsync 会降低 Redis 的性能，导致 AOF 的速度可能会慢于 RDB 。 RDB 和 AOF 各有所长，RDB 体积小，恢复速度快，而且可以生成快照；AOF 频率更高，可以保存更新的数据。一般来说，推荐同时使用。 Redis 过期机制Redis 采取的是惰性删除和定期删除配合使用的方式。 惰性删除是指 Redis 会在访问某个键的时候检查该键是否过期，如果过期，就会将输入键从数据库中删除。但惰性删除不能及时清理内存，因此 Redis 还有定期删除的机制。 定期删除是另一种过期键删除方式。Redis 会维护一个过期字典（如下图所示），所有声明了过期时间的键都会被添加进这个字典中。周期操作函数 serverCron 执行时，会在规定时间内随机检查一部分键的过期时间，并删除其中的过期键。 RDB、AOF 和复制功能对过期键的处理RDB 对过期键的处理RDB 文件在生成时会检查每个键的过期时间，过期键不会被添加进 RDB 文件里。 载入 RDB 文件时，如果该服务器是主服务器，则不会载入文件中过期的键；如果该服务器是从服务器，则不论过期与否都会被载入。不过，因为主从服务器在同步的时候，从服务器的数据库会被清空，所以一般来讲，过期键对载入 RDB 文件的从服务器不会造成影响。 AOF 对过期键的处理AOF 文件写入时，如果数据库中的某个键已过期，但它还没被删除，那么 AOF 文件不会因为这个键产生任何影响。当它被惰性删除或者定期删除之后，程序会向 AOF 文件追加一条 DEL 命令显示记录该键已被删除。 AOF 重写时，和生成 RDB 文件一样，会过滤掉已经过期的键。 复制功能对过期键的处理主服务器在删除一个过期键后，会显式地向所有从服务器发送一个 DEL 命令，告知从服务器删除这个过期键。 为了保持主从一致性，从服务器在执行客户端发送的读命令时，即使碰到过期键也不会将过期键删除，而是继续像处理未过期键一样处理过期键。 从服务器只有在接到主服务器发来的 DEL 命令之后，才会删除过期键。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"Redis 数据结构和对象","slug":"Redis 数据结构和对象","date":"2019-02-05T05:45:44.000Z","updated":"2019-03-08T14:14:49.420Z","comments":true,"path":"2019/02/05/Redis 数据结构和对象/","link":"","permalink":"http://yoursite.com/2019/02/05/Redis 数据结构和对象/","excerpt":"","text":"本文主要介绍了 Redis 的 6 种数据结构：SDS、链表、字典、跳跃表、整数集合、压缩列表，和 5 种对象：字符串、列表、哈希、集合、有序集合。 一、数据结构SDSSDS 是对 C 字符串的封装，用于表示字符串 SDS 使用预分配的方式为字符串分配空间，free 字段表示当前未使用的空间，当字符串增长时，会优先使用未使用的空间，如果未使用的空间不足，Redis 会为 SDS 分配额外的空间。分配算法具体为：1）如果增长后的字符串长度小于 1MB，Redis 将额外分配等同于增长后的字符串长度的空间，此时 free 和 len 的大小相等；2）如果增长后的字符串长度大于 1MB，Redis 将额外分配 1MB 大小的空间。 分配后的空间不会被回收，如果字符串缩短，缩短的空间会被加入到 free 空间中。 链表 Redis 的链表是一个双向链表，它的特性如下： 双端：链表节点带有 prev 和 next 指针，获取某个节点的前置和后置节点的复杂度都是 O(1)。 无环：表头节点的 prev 和表尾节点的 next 指针都指向 NULL。 带表头指针和表尾指针：通过 head 和 tail 指针获取表头和表尾节点的复杂度为 O(1)。 计数器：获取节点个数复杂度为 O(1) 多态：利用 dup（复制节点保存的值）、free(释放节点保存的值) 和 match（比较节点的值和输入的值是否相等），来实现保存不同的值。 字典字典是一个散列表结构，使用 MurmurHash2 算法计算哈希值，使用拉链法保存哈希冲突。 Redis 的字典 dict 中包含两个哈希表 dictht，这是为了方便进行 rehash 操作。在扩容时，将其中一个 dictht 上的键值对 rehash 到另一个 dictht 上面，完成之后释放空间并交换两个 dictht 的角色。 渐进式 rehash 通过记录 dict 的 rehashidx 完成，它从 0 开始，然后每执行一次 rehash 都会递增。例如在一次 rehash 中，要把 dict[0] rehash 到 dict[1]，这一次会把 dict[0] 上 table[rehashidx] 的键值对 rehash 到 dict[1] 上，dict[0] 的 table[rehashidx] 指向 null，并令 rehashidx++。 在 rehash 期间，每次对字典执行添加、删除、查找或者更新操作时，都会执行一次渐进式 rehash。 采用渐进式 rehash 会导致字典中的数据分散在两个 dictht 上，因此对字典的查找操作也需要到对应的 dictht 去执行。 跳跃表跳跃表是有序集合的底层实现之一。 跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。每个跳跃表节点的层高都是 1-32 之间的随机数。 跳跃表中的节点按照分数大小排列，当分值相同时，节点按照成员对象的大小进行排序。 在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找。下图演示了查找 22 的过程。 与红黑树等平衡树相比，跳跃表具有以下优点： 插入速度非常快速，因为不需要进行旋转等操作来维护平衡性； 并发插入时只需锁住少数节点 支持范围查找 更容易实现 跳跃表的缺点： 重复存储分层节点，消耗内存 整数集合整数集合是集合键的底层实现之一，它的底层是一个数组，这个数组以有序、无重复的方式保存集合元素。元素的类型可以为 int16_t,int32_t 或者 int64_t。程序会根据新添加元素的类型，改变数组中元素的类型。 encoding 表示元素的类型，数组中所有元素的类型是一样的。当有更大的数加入进来的时候，数组会进行升级操作，比如从 int16_t 升级到 int32_t。 整数集合只能升级，不能降级。 压缩列表压缩列表被用作列表和哈希的底层实现之一，是一种为节约内存而开发的，由任意多个节点组成的顺序数据结构。每个节点可以保存一个字节数组或一个整数值。 entry1、entry2…是实际存储数据的节点，除此之外还有些字段记录列表的信息：zlbytes 记录整个压缩列表占用的内存字节数，zltail 指向压缩列表表尾节点，zllen 记录包含的节点数量，zlend 是个特殊值字段（0xFF）用于标记末端。 每个节点由 previous_entry_length、encoding、content 三部分组成。previous_entry_length 保存了前一个节点的长度，由此可以实现从表尾向表头的遍历。encoding 是个复用字段，记录了 content 的编码和长度。下图的 encoding 最高两位 00 表示节点保存的是一个字节数组，后六位 001011 记录了字节数组长度 11。content 保存着节点的值”hello world”。 二、对象Redis 并没有使用以上的数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统。 对象类型 底层实现 可以存储的值 操作 STRING int，sds 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 LIST 链表，压缩列表 列表 从两端压入或者弹出元素 对单个或者多个元素 进行修剪，只保留一个范围内的元素 HASH 字典，压缩列表 包含键值对的无序散列表 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 SET 整数集合，字典 无序集合 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 ZSET （字典+跳跃表），压缩列表 有序集合 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 SET 对象使用整数集合保存只包含整数的集合，使用字典保存含有字符串的集合。使用字典保存集合时，字典的键是一个元素的成员，字典的值为 NULL。 ZSET 对象同时使用字典和跳跃表保存有序集合，使用字典保存有序集合时，字典的键保存了元素的成员，字典的值保存了元素的分值。ZSET 集合元素会同时共享在字典和跳跃表中（保存的是指针，不会造成数据的重复） 为什么有序集合需要同时使用字典和跳跃表来实现？ 在理论上，有序集合可以单独使用字典或者跳跃表来实现。但两者都有不可替代的地方：字典可以在 O(1) 时间复杂度查找成员的分值，跳跃表可以执行范围型操作。因此为了同时获得这两个特性，Redis 使用了字典和跳跃表来实现有序集合。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"HTTP2.0 和 QUIC","slug":"HTTP2-0 和 QUIC","date":"2019-02-01T05:23:15.000Z","updated":"2019-03-08T14:05:38.541Z","comments":true,"path":"2019/02/01/HTTP2-0 和 QUIC/","link":"","permalink":"http://yoursite.com/2019/02/01/HTTP2-0 和 QUIC/","excerpt":"","text":"HTTP1.1 在应用层以纯文本的形式进行通信。每次通信都要带完整的 HTTP 的头，而且不考虑 pipeline 模式的话，每次都是一去一回。这样在实时性、并发性上都存在问题。http2.0 通过首部压缩、多路复用、二进制分帧、服务端推送等方式获得了更高的并发和更低的延迟。 首部压缩HTTP 2.0 将原来每次都要携带的大量请求头中的 key value 保存在服务器和客户端两端，对相同的头只发送索引表中的索引。 如果首部发生变化了，那么只需要发送变化了数据在 Headers 帧里面，新增或修改的首部帧会被追加到“首部表”。首部表在 HTTP 2.0 的连接存续期内始终存在,由客户端和服务器共同渐进地更新 。 多路复用原先的 http 会为每一个请求建立一个 tcp 连接。但由于客户端对单个域名的允许的最大连接数有限，以及三次握手和慢启动等问题，导致效率很低。pipeline 模式是一个比较好的解决办法，但同样会带来队头阻塞问题：同时发出的请求必须按顺序接收，如果第一个请求被阻塞了，则后面的请求即使处理完毕了，也需要等待。 http2.0 的多路复用完美解决了这个问题。一个 request 对应一个 stream 并分配一个 id，这样一个连接上可以有多个 stream，每个 stream 的 frame 可以随机的混杂在一起，接收方可以根据 stream id 将 frame 再归属到各自不同的 request 里面。 http2.0 还可以为每个 stream 设置优先级（Priority）和依赖（Dependency）。优先级高的 stream 会被 server 优先处理和返回给客户端，stream 还可以依赖其它的 sub streams。优先级和依赖都是可以动态调整的。动态调整在有些场景下很有用，假想用户在用你的 app 浏览商品的时候，快速的滑动到了商品列表的底部，但前面的请求先发出，如果不把后面的请求优先级设高，用户当前浏览的图片要到最后才能下载完成，而如果设置了优先级，则可以先加载后面的商品，体验会好很多。 二进制分帧HTTP 1.x 在应用层以纯文本的形式进行通信，而 HTTP 2.0 将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。二进制格式的好处在于解析更快，而且文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认 0 和 1 的组合，因此健壮性更好。 服务端推送服务端可以在发送页面 HTML 时主动推送其它资源，而不用等到浏览器解析到相应位置，发起请求再响应。例如服务端可以主动把 JS 和 CSS 文件推送给客户端，而不需要客户端解析 HTML 时再发送这些请求。 服务端可以主动推送，客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送 RST_STREAM 帧来拒收。主动推送也遵守同源策略，服务器不会随便推送第三方资源给客户端。 QUICHTTP2.0 虽然大大增加了并发性，但还是有问题的。因为 HTTP2.0 也是基于 TCP 协议的，TCP 协议在处理包时是有严格顺序的。 当其中一个数据包遇到问题，TCP 连接需要等待这个包完成重传之后才能继续进行。虽然 HTTP2.0 通过多个 stream，使得逻辑上一个 TCP 连接上的并行内容，进行多路数据的传输，然而这中间没有关联的数据。一前一后，前面 stream2 的帧没有收到，后面 stream1 的帧也会因此阻塞。 于是，就有了从 TCP 切换到 UDP 的时候。这就是 Google 的 QUIC 协议。 机制一：自定义连接机制我们都知道，一条 TCP 连接是由四元组标识的，分别是源 IP、源端口、目的 IP、目的端口。一旦一个元素发生变化时，就需要断开重连，重新连接。在移动互联的情况下，当手机信号不稳定或者在 WIFI 和移动网络切换时，都会导致重连，从而进行再次的三次握手，导致一定的时延。 QUIC 使用一个 64 位的随机数作为 ID 来标识，由于 UDP 是无连接的，所以当 IP 或者端口发生变化时，只要 ID 不变，就不需要重新建立连接。 机制二：自定义重传机制TCP 为了保证可靠性，通过使用序号和应答机制，来解决顺序问题和丢包问题。 任何一个序号的包发过去，都要在一定的时间内得到应答，否则一旦超时，就会重发。超时时间是通过采样往返时间 RTT 不断调整的。这就会带来一个采样不准确的问题。例如：发送一个包，序号为 100，发现没有返回，于是再发送一个 100，过一阵返回一个 ACK101。这个时候客户端知道这个包肯定收到了，但往返时间是多少呢？是 ACK 达到的时间减去后一个 100 发送的时间，还是减去前一个 100 发送的时间呢？ QUIC 也有个序列号，是递增的。任何一个序列号的包只发送一次，下次就要加一了。例如，发送一个包，序号是 100，发现没有返回；再次发送的时候，序号就是 101 了；如果返回的 ACK100，就是对第一个包的响应，如果返回 ACK101 就是对第二个包的响应，RTT 计算相对准确。 但是这里有一个问题，就是这么知道包 100 和包 101 发送的是同样的内容呢？QUIC 定义了一个 stream offset 的概念。QUIC 既然面向连接，也就像 TCP 一样，是一个数据流，发送的数据在这个数据流里面有个偏移量 stream offset，可以通过 stream offset 查看数据发送到了哪里，这样只要这个 stream offset 的包没有来，就要重发；如果来了，按照 stream offset 拼接，还是能够拼成一个流。 无阻塞的多路复用有了自定义的连接和重传机制，我们可以解决上面 HTTP2.0 的多路复用问题。 同 HTTP2.0 一样，同一条 QUIC 连接上可以创建多个 stream，来发送多个 HTTP 请求。但是，QUIC 是基于 UDP 的，一个连接上的多个 stream 之间没有依赖。这样，假如 stream2 丢了一个 UDP 包，后面跟着 stream3 的一个 UDP 包，虽然 stream2 的那个包需要重传，但是 stream3 的包无需等待，就可以发给用户。 自定义流量控制TCP 的流量控制是通过滑动窗口协议。QUIC 的流量控制也是通过 window_update，来告诉对端它可以接受的字节数。但是 QUIC 的窗口是适应自己的多路复用机制的，不但在一个连接上控制窗口，还在一个连接中的每个 stream 控制窗口。 在 TCP 协议中，接受端的窗口的起始点是下一个要接收并且 ACK 的包，即使后来的包都到了，放在缓存里，窗口也不能右移，因为 TCP 的 ACK 机制是基于系列号的累计应答，一旦 ACK 了一个系列号，就说明前面的都到了，所以只要前面的没到，后面的到了也不能 ACK，就会导致后面的到了，也有可能超时重传，浪费带宽。 QUIC 的 ACK 是基于 offset 的，每个 offset 的包来了，进了缓存，就可以应答，应答后就不会重发，中间的空档会等待到来或者重发即可，而窗口的起始位置为当前收到的最大 offset，从这个 offset 到当前的 stream 所能容纳的最大缓存，是真正的窗口大小。显然，这样更加准确。 另外，还有整个连接的窗口，需要对于所有的 stream 的窗口做一个统计。","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/tags/计算机网络/"}],"keywords":[]},{"title":"InnoDB 中的锁和 MVCC","slug":"InnoDB 中的锁和 MVCC","date":"2019-01-27T13:20:11.000Z","updated":"2019-03-08T14:07:16.288Z","comments":true,"path":"2019/01/27/InnoDB 中的锁和 MVCC/","link":"","permalink":"http://yoursite.com/2019/01/27/InnoDB 中的锁和 MVCC/","excerpt":"","text":"锁和 MVCC 是 MySQL 控制并发访问的两种手段。InnoDB 在 MySQL 的基础上提供了更细粒度的行级锁，使用了 next-key 算法解决了幻读的问题。另外 InnoDB 提供了一套基于 MVCC 的一致性非锁定读方式，实现了“读不加锁，读写不冲突”的快照读方式。 锁1、表锁InnoDB 直接沿用了ＭySQL 提供的表锁。事实上，表锁的加锁和解锁都是在 MySQL server 层面的，和存储引擎没有关系。加锁的方式如下：123LOCK TABLES orders READ; // 加读锁SELECT SUM(total) FROM orders;UNLOCK TABLES; // 解锁 2、行级锁顾名思义，行级锁锁定的是某一行数据。InnoDB 中的所有数据项都保存在聚簇索引中，所以行级锁实质上锁住的是索引项。如果表中有多个索引存在，一行数据会对应到多个索引项，此时行级锁会锁住所有索引上的相应索引项。 行级锁分为共享锁（S 锁）和排他锁（X 锁）。S 锁和 S 锁可以兼容；S 锁和 X 锁，X 锁和 X 锁不能兼容。（InnoDB 存储引擎默认采用行级锁，所以下文如无指明，S 锁和 X 锁均指行锁） 3、意向锁在没有引入意向锁之前，行级锁和表锁之间的兼容有点麻烦：如果要对一张表加 X 表锁，那么首先要判断这张表是否加了 X 表锁和 S 表锁，其次要判断每一行是否加了 X 锁和 S 锁，如果表的行数比较多的话，这种判断方式会比较损失性能。因此 InnoDB 引入了意向锁。 和行锁一样，意向锁也分为意向共享锁（IS 锁）和意向排他锁（IX 锁）。事务在申请 S 或 X 锁之前，必须先申请到 IS 或 IX 锁。InnoDB 中的意向锁是一种特殊的表锁：意向锁之间互不冲突，意向锁和表锁之间会冲突，此时意向锁相当于同类型的表锁。 MVCCMVCC 是一种非锁定读的一致性读机制。它的特点是读不加锁，读写不冲突。InnoDB 利用 undo log 实现了 MVCC。undo log 的数据结构如图所示： 前四行是数据列，后三列是隐藏列。隐藏列的含义如下： DB_ROW_ID：行 ID。占 7 字节，他就项自增主键一样随着插入新数据自增。如果表中不存主键或者唯一索引，那么数据库就会采用 DB_ROW_ID 生成聚簇索引。否则 DB_ROW_ID 不会出现在索引中。 DB_TRX_ID：事务 ID。占 6 字节，表示这一行数据最后插入或修改的事务 id。此外删除在内部也被当作一次更新，在行的特殊位置添加一个删除标记（记录头信息有一个字节存储是否删除的标记）。 DB_ROLL_PTR：回滚指针。占 7 字节，每次对数据进行更新操作时，都会 copy 当前数据，保存到 undo log 中。并修改当前行的回滚指针指向 undo log 中的旧数据行。 MVCC 只有在隔离级别是 READ COMMITED 和 REPEATABLE READ 两个隔离级别下工作。MVCC 可以通过比较数据行的事务 ID 和当前事务 ID 来判断该记录是否对当前事务可见。但仅有 undo log 还不够。试想这样一种情况：事务 599 开始——事务 600 开始——事务 600 查询了表 A——事务 599 更新了表 A——事务 599 提交——事务 600 再次查询表 A。可知事务 600 的两次查询会得到不同的结果，无法满足 RR 隔离级别的要求。这是因为事务 599 的事务 ID 虽然比事务 600 小，但事务 599 还未结束，仍有可能改变数据项的值。 read viewInnoDB 使用了 read view 解决了这个问题。read view 是一张表，记录了当前活跃的事务 ID。InnoDB 在查询时会先对比数据行的事务 ID 和 read view 中的事务 ID。具体如下： 如果数据行事务 ID 大于 read view 中最大的 ID，表示数据行一定是在当前事务之后修改的，对当前事务不可见； 如果数据行事务 ID 小于 read view 中最小的 ID，表示数据行一定是在当前事务开始之前修改并且已提交，所以对当前事务可见。 如果数据行事务 ID 落在 read view 最大最小 ID 的区间中，则要判断数据行事务 ID 和当前事务 ID 的关系： 如果数据行事务 ID 不在活跃事务数组中，表示该事务已提交，此时和当前事务 ID 比较，若小于则可见，大于则不可见； 如果数据行事务 ID 在活跃事务数组中，表示该事务未提交，这里要判断一下数据行事务 ID 是否为当前事务 ID，若是，虽然未提交但同一事务内的修改可见，若不是，则不可见。 当前数据行若是不可见，InnoDB 会沿着 DB_ROLL_PTR 往下查找，直到找到第一个可见的数据行或者 null。 可见与否只是第一步，实际返回的数据还要经过判断。因为删除和更新共用一个字段，区别只是删除有一个字节的删除标记，那么在返回的时候 InnoDB 就要判断当前的数据行是否被标记为删除。如果标记了删除，就不会返回。 MVCC 在 READ COMMITED 和 REPEATABLE READ 两个隔离级别下共用一套逻辑，区别只是在于RC 隔离级别是在读操作开始时刻创建 read view 的，而 RR 隔离级别是在事务开始时刻，确切地说是第一个读操作创建 read view 的。由于这一点上的不同，使得 MVCC 在 RC 隔离级别下读取的是最新提交的数据，而 RR 隔离级别下读取的是事务开始前提交的数据。 next-key 解决幻读问题幻读是指一个事务内连续进行两次相同的 SQL 语句可能导致不同的结果，第二次的 SQL 语句可能会返回之前不存在的行。发生这种现象的原因是事务 A 两次查找的间隔中事务 B 插入了一条或多条数据并提交，导致事务 A 的第二次查询查到了新插入的数据。 InnoDB 中有三种锁算法： Record Lock：单个行记录上的锁 Gap Lock：间隙锁，锁住一个范围，但不包含记录本身 Next-Key Lock：Record Lock+Gap Lock，锁定一个范围，并且锁定记录本身 幻读现象问题的根本原因是 Record Lock 只锁住记录本身而不锁范围，导致其它事务可以在记录间插入数据。InnoDB 使用了 Next-Key Lock 来解决这个问题。Next-Key Lock 会锁住一个范围，例如一个索引有 10,11,13,20 这四个值，那么该索引可能被 Next-Key Locking 的区间为：(-∞,10](10,11](11,13](13,20](20,+∞) 当然，如果查询的索引是唯一索引的话，就不用担心被插入的问题，InnoDB 会对 Next-Key Lock 进行优化，将其降级为 Record Lock。 innodb 加锁处理分析以上是理论部分，那么实际 innodb 怎么加锁呢？我们结合 SQL 语句来分析。 在支持 MVCC 并发控制的系统中，读操作可以分成两类：快照读 (snapshot read) 与当前读 (current read)。快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。 快照读： select * from table where ?; 当前读： select * from table where ? lock in share mode; select * from table where ? for update; insert into table values (…); update table set ? where ?; delete from table where ?; 简而言之，所有的插入/更新/删除操作都是当前读，且都加了 X 锁；读取操作默认是快照读，但可以声明加 X 锁或者 S 锁。 一条简单 SQL 的加锁实现分析我们拿一条 SQL 语句：delete from t1 where id = 10; 来分析 innodb 的加锁情况。但光有这一条 SQL 是不够的，我们还需要知道一些前提： 前提 1：id 列是不是主键？ 前提 2：当前系统的隔离级别是什么？ 前提 3：如果 id 列不是主键，那么 id 列上有索引吗？ 前提 4：如果 id 列上有二级索引，那么这个索引是唯一索引吗？ 基于这些前提的不同，我们可以组合出以下几种情况（隔离级别只考虑 RC 和 RR 的情况）： 组合 1：id 列是主键，RC 隔离级别 组合 2：id 列是二级唯一索引，RC 隔离级别 组合 3：id 列是二级非唯一索引，RC 隔离级别 组合 4：id 列上没有索引，RC 隔离级别 组合 5：id 列是主键，RR 隔离级别 组合 6：id 列是二级唯一索引，RR 隔离级别 组合 7：id 列是二级非唯一索引，RR 隔离级别 组合 8：id 列上没有索引，RR 隔离级别 组合 1：id 列是主键，RC 隔离级别这个组合，是最简单，最容易分析的组合。id 是主键，Read Committed 隔离级别，给定 SQL：delete from t1 where id = 10; 只需要将主键上，id = 10 的记录加上 X 锁即可。如下图所示： id 是主键时，此 SQL 只需要在 id=10 这条记录上加 X 锁即可。 组合 2：id 列是二级唯一索引，RC 隔离级别这个组合，id 不是主键，而是一个 unique 的二级索引键值。那么在 RC 隔离级别下，delete from t1 where id = 10; 需要加什么锁呢？见下图： 此组合中，id 是 unique 索引，而主键是 name 列。此时，加锁的情况由于组合一有所不同。由于 id 是 unique 索引，因此 delete 语句会选择走 id 列的索引进行 where 条件的过滤，在找到 id=10 的记录后，首先会将 unique 索引上的 id=10 索引记录加上 X 锁，同时，会根据读取到的 name 列，回主键索引 (聚簇索引)，然后将聚簇索引上的 name = ‘d’ 对应的主键索引项加 X 锁。为什么聚簇索引上的记录也要加锁？试想一下，如果并发的一个 SQL，是通过主键索引来更新：update t1 set id = 100 where name = ‘d’; 此时，如果 delete 语句没有将主键索引上的记录加锁，那么并发的 update 就会感知不到 delete 语句的存在，违背了同一记录上的更新/删除需要串行执行的约束。 组合 3：id 列是二级非唯一索引，RC 隔离级别相对于组合一、二，组合三又发生了变化，隔离级别仍旧是 RC 不变，但是 id 列上的约束又降低了，id 列不再唯一，只有一个普通的索引。假设 delete from t1 where id = 10; 语句，仍旧选择 id 列上的索引进行过滤 where 条件，那么此时会持有哪些锁？同样见下图： 可以看到，首先，id 列索引上，满足 id = 10 查询条件的记录，均已加锁。同时，这些记录对应的主键索引上的记录也都加上了锁。与组合二唯一的区别在于，组合二最多只有一个满足等值查询的记录，而组合三会将所有满足查询条件的记录都加锁。 组合 4：id 列上没有索引，RC 隔离级别相对于前面三个组合，这是一个比较特殊的情况。id 列上没有索引，where id = 10;这个过滤条件，没法通过索引进行过滤，那么只能走全表扫描做过滤。对应于这个组合，SQL 会加什么锁？或者是换句话说，全表扫描时，会加什么锁？这个答案也有很多：有人说会在表上加 X 锁；有人说会将聚簇索引上，选择出来的 id = 10;的记录加上 X 锁。那么实际情况呢？请看下图： 由于 id 列上没有索引，因此只能走聚簇索引，进行全部扫描。从图中可以看到，满足删除条件的记录有两条，但是，聚簇索引上所有的记录，都被加上了 X 锁。无论记录是否满足条件，全部被加上 X 锁。既不是加表锁，也不是在满足条件的记录上加行锁。 有人可能会问？为什么不是只在满足条件的记录上加锁呢？这是由于 MySQL 的实现决定的。如果一个条件无法通过索引快速过滤，那么存储引擎层面就会将所有记录加锁后返回，然后由 MySQL Server 层进行过滤。因此也就把所有的记录，都锁上了。 组合 5：id 列是主键，RR 隔离级别上面的四个组合，都是在 Read Committed 隔离级别下的加锁行为，接下来的四个组合，是在 Repeatable Read 隔离级别下的加锁行为。 组合五，id 列是主键列，Repeatable Read 隔离级别，针对 delete from t1 where id = 10; 这条 SQL，加锁与组合一：[id 主键，Read Committed] 一致。 组合 6：id 列是二级唯一索引，RR 隔离级别与组合五类似，组合六的加锁，与组合二：[id 唯一索引，Read Committed] 一致。两个 X 锁，id 唯一索引满足条件的记录上一个，对应的聚簇索引上的记录一个。 组合 7：id 列是二级非唯一索引，RR 隔离级别组合七，Repeatable Read 隔离级别，id 上有一个非唯一索引，执行 delete from t1 where id = 10; 假设选择 id 列上的索引进行条件过滤，最后的加锁行为，是怎么样的呢？同样看下面这幅图： 此图，相对于组合三多了一个 GAP 锁，这是因为 RR 级别区别于 RC 级别的一点是 RR 级别要防止幻读。我们在前一节讲过 innodb 基于 Next-Lock 防止幻读，而 Next-Lock 就是 GAP Lock+Record Lock。加在索引上的是 Record Lock，而在中间的就是 GAP Lock。 那么为什么组合五、组合六，也是 RR 隔离级别，却不需要加 GAP 锁呢？这是因为组合五，id 是主键；组合六，id 是 unique 键，都能够保证唯一性。一个等值查询，最多只能返回一条记录，而且新的相同取值的记录，一定不会再新插入进来，因此也就避免了 GAP 锁的使用。 组合 8：id 列上没有索引，RR 隔离级别组合八，Repeatable Read 隔离级别下的最后一种情况，id 列上没有索引。此时 SQL：delete from t1 where id = 10; 没有其他的路径可以选择，只能进行全表扫描。最终的加锁情况，如下图所示：","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"TCP 协议回顾","slug":"TCP 协议回顾","date":"2019-01-23T13:28:32.000Z","updated":"2019-03-08T14:16:21.899Z","comments":true,"path":"2019/01/23/TCP 协议回顾/","link":"","permalink":"http://yoursite.com/2019/01/23/TCP 协议回顾/","excerpt":"","text":"本科的时候面向考试学习计算机网络，TCP 是重点中的重点，可惜当时不知道 TCP 在网络世界中的重要，考完试就把知识点扔了。最近在复习计算机网络，看到 TCP 这章，就像看到了一个老朋友。就想把它记录下来，便于复习。 TCP 协议非常复杂，标准也非常多，但核心内容就以下几个部分： 三次握手 四次挥手 可靠性传输 流量控制 拥塞控制 TCP 协议头要想了解 TCP 协议首先就必须了解 TCP 的协议头： 首先，源端口号和目标端口号是不可少的，这一点和 UDP 是一样的。如果没有这两个端口号。 数据就不知道应该发给哪个应用。 接下来是包的序号和确认序号。为了保证消息的顺序性到达，TCP 给每个包编了一个序号。初始序号在建立连接时指定，此后每个包的序号为上一个包的序号+上一个包的字节数。服务器会返回一个确认号，表示这个序号之前的包都收到了。 然后是一些状态位。例如 SYN 是发起一个连接，ACK 是回复，RST 是重新连接，FIN 是结束连接。TCP 是面向连接的，这些带状态位的包可以改变双方的状态。 窗口大小是跟流量控制有关。TCP 是全双工的协议，通信双方都会维护一个缓存空间。这个窗口大小就是告诉对方我还有多少剩余的缓存空间。 三次握手TCP 是面向连接的协议，所以在建立连接前有一系列的动作，被称为三次握手： 一开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端口，处于 LISTEN 状态。然后客户端主动发起连接 SYN，之后处于 SYN-SENT 状态。服务端收到发起的连接，返回 SYN，并且 ACK 客户端的 SYN，之后处于 SYN-RCVD 状态。客户端收到服务端发送的 SYN 和 ACK 之后，发送 ACK 的 ACK，之后处于 ESTABLISHED 状态，因为它一发一收成功了。服务端收到 ACK 的 ACK 之后，处于 ESTABLISHED 状态，因为它也一发一收了。 三次握手除了双方建立连接外，还要沟通一件事情，就是 TCP 包的序号的问题。 双方在发送 SYN 包的时候，各自需要指定一个针对这次连接的序号 seq。这个 seq 实质上可以看出一个 32 位的计时器，每 4ms 加一。为什么要这么做呢？主要是为了防止在网络中被延迟的分组在以后被重复传输，而导致某个连接的一端对它作错误的判断。如果序号不按这种方式分配，而是从 1 开始，则会出现这样的情况：AB 建立连接之后，A 发送了 1,2,3 三个包，然后掉线了。由于网络的原因三个包没有到达 B，在网络中游荡。然后 A 重连了，序号重新从 1 开始，他又发送了 1,2 两个包，但没有发送 3 号包。此时上一次连接发送的 3 号包却到达了 B，B 以为是 A 这次发送的，就产生了误判。为了避免这种情况的发生，TCP 协议规定了这种方式生成初始 seq。以这种方式生成的初始 seq，需要 4 个多小时才会重复，此时早已过了 3 号包的生存时间（TTL）。 为什么要三次握手而不是两次？ 原因一：服务器会收到客户端很早以前发送的，但因为延迟导致现在才到达的 SYN 报文。如果不采用三次握手，则服务器会认为新的连接已经建立，会白白浪费缓存等资源。原因二：三次握手需要交流双方的初始序号 seq，服务器发送的第二次握手是针对客户端在第一次握手中约定的客户端初始 seq 的确认，客户端的第三次握手是针对服务器在第二次握手中约定的服务器初始 seq 的确认。如果没有第三次握手，万一服务器的 SYN 包丢了，那么客户端无法得知服务器的初始序号，此时客户端就没法接收服务器的包，因为客户端没法辨别这个包是本次连接中发送的，还是上一次连接中发送的。 SYN 洪泛攻击 因为服务器在收到一个 SYN 报文后，会初始化连接变量和缓存，如果攻击方会发送大量 SYN 报文，而不完成第三次握手，那么就会导致服务器的连接资源被消耗殆尽。针对 SYN 洪泛攻击有一种有效的防御手段，称为SYN cookie：当服务器接收到一个 SYN 报文时，它不知道这是来自一个合法的请求还是 SYN 洪泛攻击的一部分。所以它不会为其分配资源，而是将该报文中的源、目的 IP 地址和端口和服务器自己的秘密数做哈希，将（秒级时间（5 位）+最长分段大小（3 位）+哈希（24 位））作为初始 seq 返回给客户端。如果是一个合法用户，会返回一个 ACK 包，服务器可以通过将 ACK 包中的源、目的 IP 地址和端口，和 ACK-1 对比，得知这是否是一个合法的 SYN 确认包。然后可以通过秒级时间确定这是否是一个新的包。如果是新的包且合法，服务器才会分配资源。这样就有效防止了 SYN 洪泛攻击的发生。 四次挥手有建立连接，必然也有断开连接。断开连接的动作被称为四次挥手： 区别于三次握手，四次挥手的发起方可以是客户端也可以是服务端。因此不区分客户端和服务端而是用 AB 代替。 一开始，A 和 B 都处于 ESTABLISHED 的状态。然后 A 发送 FIN 表示请求断开连接，之后 B 处于 FIN-WAIT-1 的状态。 B 在接收到 FIN 请求后，会发送一个 ACK 包表示收到了 FIN 请求，之后 B 处于 CLOSED-WAIT 状态。需要注意的是此时 B 发送的是 ACK 包而不是 FIN 包，之所以不像三次握手一样直接回应一个 FIN 包，是因为此时 B 可能还有些事情没有做完，还可能发送数据，所以称为半关闭状态。 这个时候 A 可以选择不再接收数据，也可以选择最后再接收一段数据，等待 B 也主动关闭。不论如何，A 在收到 ACK 包后都进入了 FIN-WAIT2 阶段。此时如果 B 下线，A 将永远在这个状态。TCP 协议里没有对这个状态的处理，但 Linux 有，可以调整 tcp_fin_timeout 这个参数，设置一个超时时间。 B 处理完了所有的事情，终于也准备关闭，此时会发送一个 FIN 包。之后 B 进入 LAST-ACK 状态，等待 A 的 ACK 包。 A 在收到服务器的 FIN 包后会发送一个 ACK 包表示收到了 B 的 FIN 包。按理说此时 A 就可以关闭了，但由于 A 最后的 ACK 存在丢包的可能，B 没有收到最后的 ACK 包的话，就会重发一个 FIN 包，如果这时候 A 关闭了，B 就再也收不到 ACK 了。因而 TCP 协议要求 A 最后等待一段时间 TIME_WAIT，这个时间要足够长，长到 B 没收到 ACK 的话，重发的 FIN 包还能到达 A。 A 直接关闭还有一个问题是，A 的端口就直接空出来了，但是 B 不知道，B 原来发过的很多包很可能还在路上，如果 A 的端口被一个新的应用占用了，这个新的应用会收到上个连接中 B 发过来的包，虽然序列号是重新生成的，但是这里要上一个双保险，防止产生混乱，因而也需要等足够长的时间，等到原来 B 发送的所有的包都过期了，再空出端口来。 等待的时间设为 2MSL，MSL 是 Maximum Segment Lifetime（报文最大生存时间），它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的， 而 IP 头中有一个 TTL 域，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。协议规定 MSL 为 2 分钟，实际应用中常用的是 30 秒，1 分钟和 2 分钟等。 服务器大量 CLOSE_WAIT\\TIME_WAIT 状态的原因 如果服务器出现异常，百分之八九十都是下面两种情况：1、服务器保持了大量 CLOSE_WAIT 状态。产生的原因在于：TCP Server 已经 ACK 了过来的 FIN 数据包，但是上层应用程序迟迟没有发命令关闭 Server 到 client 端的连接。所以 TCP 一直在那等啊等…..所以说如果发现自己的服务器保持了大量的 CLOSE_WAIT，问题的根源十有八九是自己的 server 端程序代码的问题。2、服务器保持了大量 TIME_WAIT 状态。产生的原因在于：服务器处理大量高并发短连接并主动关闭连接时容易出现 TIME_WAIT 积压。这是因为关闭的发起方在 TIME_WAIT 阶段需要等待 1-4 分钟才能回收资源。如果连接过多将导致资源来不及回收。解决方案是修改 Linux 内核，允许将 TIME-WAIT sockets 重新用于新的 TCP 连接，并开启 TCP 连接中 TIME-WAIT sockets 的快速回收，这些默认都是关闭的。 可靠性传输TCP 协议为了保证顺序性，每一个包都有一个 ID。在建立连接的时候，会商定起始的 ID 是什么，然后按照 ID 一个个发送。为了保证不丢包，对于发送的包都要进行应答，但是这个应答也不是一个一个来的，而是会应答某个之前的 ID，表示都收到了，这种模式称为累计确认或者累计应答（cumulative acknowledgment）。 为了记录所有发送的包和接收的包，TCP 也需要发送端和接收端分别都有缓存来保存这些记录。发送端的缓存里是按照包的 ID 一个个排列，根据处理的情况分成四个部分。 第一部分：发送了并且已经确认的。 第二部分：发送了并且尚未确认的。 第三部分：没有发送，但是已经等待发送的。 第四部分：没有发送，并且暂时还不会发送的。 为什么会有三、四部分的区分呢？这是因为接受端有个处理极限，就是剩余缓冲区的大小，如果给接收端发送的包的大小超过了剩余缓冲区的大小，那么有一部分包就会被丢弃，这是不合适的。所以超出剩余缓冲区大小的包，发送端暂时不会发。 于是，发送端需要保持下面的数据结构： 对于接收端来讲，它的缓存里记录的内容要简单一些： 第一部分：接收并且确认过的。 第二部分：还没接收，但尚在接收能力范围之内的。 第三部分：还没接收，超过能力范围的。 对应的数据结构像这样： 顺序与丢包问题还是刚才的图，在发送端来看，1、2、3 已经发送并确认；4、5、6、7、8、9 都是发送了还没确认；10、11、12 是还没发出的；13、14、15 是接收方没有空间，不准备发的。 在接收端来看，1、2、3、4、5 是已经完成 ACK，但是没读取的；6、7 是等待接收的；8、9 是已经接收，但是没有 ACK 的。 发送端和接收端当前的状态如下： 1、2、3 没有问题，双方达成了一致。 4、5 接收方说 ACK 了，但是发送方还没收到，有可能丢了，有可能在路上。 6、7、8、9 肯定都发了，但是 8、9 已经到了，但是 6、7 没到，出现了乱序，缓存着但是没办法 ACK。 根据这个例子，我们可以知道，顺序问题和丢包问题都有可能发生，所以我们先来看确认与重发的机制。 假设 4 的确认到了，不幸的是，5 的 ACK 丢了，6、7 的数据包丢了，这该怎么办呢？ 一种方法就是超时重试，TCP 会为当前最小未应答的包绑定一个定时器，当收到 ACK 时会重启定时器，并绑定到现在最小未应答的包上。若当前无未应答包，则关闭定时器。等下一个包发出去后再启动并绑定到新的包上。 如何设置往返时间 RTT 呢？TCP 采用了加权的自适应重传算法：EstimatedRTT=0.875EstimatedRTT+0.125SampleRTT 其中 EstimatedRTT 为平均往返时间，SampleRTT 为某次采样的往返时间。 如果过一段时间，5、6、7 都超时了，就会重新发送。接收方发现 5 原来接收过，于是丢弃 5；6 收到了，发送 ACK，要求下一个是 7，7 不幸又丢了。当 7 再次超时的时候，有需要重传的时候，TCP 的策略是超时间隔加倍。每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。 超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？ 有一个叫快速重传的机制：当接收方收到一个序号大于下一个所期望的报文段时，就检测到了数据流中的一个间格，于是发送三个冗余的 ACK，客户端收到后，就在定时器过期之前，重传丢失的报文段。 例如，接收方发现 6、8、9 都已经接收了，就是 7 没来，那肯定是丢了，于是发送三个 6 的 ACK，要求下一个是 7。客户端收到 3 个，就会发现 7 的确又丢了，不等超时，马上重发。 还有一种方式称为Selective Acknowledgment （SACK）。这种方式需要在 TCP 头里加一个 SACK 的东西，可以将缓存的地图发送给发送方。例如可以发送 ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是 7 丢了。 流量控制流量控制是为了平衡发送端与接收端的速度，避免出现包处理不过来的情况。在协议头里面，有一个窗口大小字段，这个就是用来进行流量控制的。 我们先假设窗口不变的情况，窗口始终为 9。4 的确认来的时候，会右移一个，这个时候第 13 个包也可以发送了。 这个时候，假设发送端发送过猛，会将第三部分的 10、11、12、13 全部发送完毕，之后就停止发送了，未发送可发送部分 0。 当对于包 5 的确认到达的时候，在客户端相当于窗口再滑动了一格，这个时候，才可以有更多的包可以发送了，例如第 14 个包才可以发送。 如果接收方实在处理的太慢，导致缓存中没有空间了，可以通过确认信息修改窗口的大小，甚至可以设置为 0，则发送方将暂时停止发送。 我们假设一个极端情况，接收端的应用一直不读取缓存中的数据，当数据包 6 确认后，窗口大小就不能再是 9 了，就要缩小一个变为 8。 这个新的窗口 8 通过 6 的确认消息到达发送端的时候，你会发现窗口没有平行右移，而是仅仅左面的边右移了，窗口的大小从 9 改成了 8。 如果接收端还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，直到为 0。 当这个窗口通过包 14 的确认到达发送端的时候，发送端的窗口也调整为 0，停止发送。 如果这样的话，发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满 了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。 拥塞控制拥塞控制是为了避免网络中传输着太多的包导致网络拥挤。这里有一个公式，即：发送但还未确认的包要小于等于滑动窗口（rwnd）和拥塞窗口（cwnd）的最小值。前者在流量控制中已经讲过，剩下的就是后者。 拥塞控制有三个时期：慢启动、拥塞避免和快速恢复。 当开始时，cwnd 设置为一个报文段，一次只能发送一个；当收到这一个确认的时候，cwnd 加一，于是一次能发送两个；当这两个确认到来的时候，每个确认 cwnd 加一，两个确认 cwnd 加二，于是一次能发送四个。当这四个的确认到来的时候，每个确认 cwnd 加一，四个确认 cwnd 加四，于是一次能够发送八个。依次类推，此时，cwnd 的增长速度是指数型的增长。 涨到什么时候是个头呢？有一个值 ssthresh 初始为 65535 个字节，当超过这个值的时候，就进入了拥塞避免状态。此时不再是一个确认对应一个 cwnd 的增长，而是一个确认对应 1/cwnd 的增长。我们接着上面的过程来，一次发送八个，当八个确认到来的时候，每个确认增加 1/8，八个确认一共 cwnd 增加 1，于是一次能够发送九个，变成了线性增长。 但 cwnd 不可能无限增长，总有一个时候网络会拥挤，拥挤的表现形式是丢包。发送端有两种方式感知到丢包：超时和收到三个相同 ACK（快速重传）。针对这两种情况的丢包，发送端的处理方式也不一样。 第一种是超时丢包，这种情况下，发送端会认为当前网络非常拥挤，因此会采取激进的限制措施：将 sshresh 设为 cwnd/2，将 cwnd 设为 1，重新开始慢启动。 第二种是三个相同 ACK 丢包，发送端会认为这个丢包是个偶然事件，因此网络并不非常拥挤，采取的措施也会温和一些：sshresh 设为 cwnd/2，cwnd 设为 cwnd/2，又因为返回了三个确认包，cwnd 再加 3。之后进入快速恢复阶段，因为当前 cwnd 仍在比较高的值，这个阶段中 cwnd 也是线性增长。 两种方式的比较如下： 总结TCP 协议的核心部分在于：三次握手、四次挥手、可靠性传输、流量控制、拥塞控制，掌握好这些知识点对网络编程很有帮助。","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/tags/计算机网络/"}],"keywords":[]},{"title":"插入缓冲、两次写和自适应哈希索引","slug":"插入缓冲、两次写和自适应哈希索引","date":"2019-01-14T03:56:25.000Z","updated":"2019-03-08T14:16:48.572Z","comments":true,"path":"2019/01/14/插入缓冲、两次写和自适应哈希索引/","link":"","permalink":"http://yoursite.com/2019/01/14/插入缓冲、两次写和自适应哈希索引/","excerpt":"","text":"InnoDB 是 MySQL 数据库从 5.5.8 版本开始的默认存储引擎，它将数据放在一个逻辑的表空间中，这个表空间就像黑盒一样由 InnoDB 存储引擎自身进行管理。从 MySQL 4.1 开始，它可以将每个 InnoDB 的表单独存在一个独立的 idb 文件中。此外，InnoDB 支持用裸设备（raw disk，不被操作系统管理的设备）建立其表空间。 InnoDB 通过使用多版本并发控制（MVCC）来获得高并发性，并且实现了 SQL 标准的四种隔离级别，默认为 REPEATABLE 级别。同时，使用一种称为 next-key locking 的策略来避免幻读。除此之外，InnoDB 还提供了插入缓冲、二次写、自适应哈希索引等高性能和高可用的功能。 缓冲池功能的实现离不开底层的配合。为了协调 CPU 速度与磁盘速度之间的鸿沟，InnoDB 在内存中开辟了一块空间叫内存池，将对数据库的修改首先保存在内存中。比如对于页的操作，首先会在内存中进行，然后后台线程会把脏页（还没有刷入磁盘的页）刷入磁盘。 缓冲池中缓存的数据页类型有：索引页、数据页、undo 页、插入缓冲、自适应哈希索引、InnoDB 存储的锁信息、数据字典信息等。不能简单地认为，缓冲池只是缓存索引页和数据页，它们只是占缓冲池很大的一部分而已。 中点插入策略缓冲池对于数据页的管理，是使用 LRU 的方式管理的。和传统的 LRU 稍有不同的是，InnoDB 使用一种称为“中点插入策略”的方式插入数据： 新页的第一次插入只会插入到 LRU 链表尾端 3/8（中点）的位置 页的再次命中（LRU 上的页被命中）才会把页插入到链表头部 可以设置一个 InnoDB_old_blocks_time 的参数，表示新页插入后过多久才有资格被插入到链表头部 这样做的好处是避免了一些冷数据对真正热点数据的干扰。比如进行扫描操作时，需要访问表中的许多页，甚至是全部页，而这些页通常来说只在这次查询中需要，并不是活跃的热点数据。如果页被放入 LRU 链表头部，那么非常可能将所需的真正热点数据刷出。InnoDB_old_blocks_time 也是出于同样的目的。可以避免临近的几次查询把页刷入热数据的情况。 插入缓冲InnoDB 底层使用聚簇索引管理数据。在进行插入操作的时候，数据页的存放是按主键顺序存放的，此时磁盘顺序访问，速度会很快。但对于非聚集索引叶子节点的插入则不再是顺序的了，这时需要离散地访问非聚集索引页，磁盘的随机读取效率很低，导致了插入操作的性能下降。 InnoDB 存储引擎创造性地设计了 Insert Buffer，对于非聚簇索引的插入或更新操作，不是每一次直接插入到索引页中，而是先判断插入的非聚簇索引页是否在缓冲池中，若在，则直接插入；若不在，则先放入到一个 Insert Buffer 对象中。然后再以一定的频率和情况进行 Insert Buffer 和辅助索引叶子节点的 merge 操作，这时通常能将多个插入合并到一个操作中（因为在一个索引页中），这就大大提高了对于非聚簇索引插入的性能。 然而 Insert Buffer 的使用需要同时满足以下两个条件： 索引是辅助索引。 索引不是唯一的。（因为在插入缓冲时，数据库并不去查找索引页来判断插入的记录的唯一性。如果去查找肯定又会有离散读取的发生，就背离了 Insert Buffer 的初衷） Insert Buffer 的数据结构是一颗 B+树，4.1 版本之前每张表都有一颗 Insert Buffer B+树，4.1 版本之后所有表共用一颗 B+树。Insert Buffer B+树的非叶子节点存放的是查询的 search key，其构造如图： 其中 space 表示待插入记录所在表的表空间 id，在 InnoDB 存储引擎中，每个表有一个唯一的 space id，可以通过 space id 查询得知是哪张表；maker 是用来兼容老版本的 Insert Buffer。offerset 表示页所在的偏移量。 当一个辅助索引要插入到页（space,offset）时，如果这个页不在缓冲池中，那么 InnoDB 存储引擎首先根据上述规则构造一个 search key，接下来查询 Insert Buffer 这棵 B+树，然后再将记录插入到 Insert Buffer B+树的叶子节点中。 两次写如果说 Insert Buffer 带给 InnoDB 存储引擎的是性能上的提升，那么 doublewrite（两次写）带给 InnoDB 存储引擎的是数据页的可靠性。 InnoDB 中有记录（Row）被更新时，先将其在 Buffer Pool 中的 page 更新，并将这次更新记录到 Redo Log file 中，这时候 Buffer Pool 中的该 page 就是被标记为 Dirty。在适当的时候（Buffer Pool 不够、Redo 不够，系统闲置等），这些 Dirty Page 会被 Checkpoint 刷新到磁盘进行持久化操作。 但尴尬的地方在于 InnoDB 的 Page Size 是 16KB，其数据校验也是针对这 16KB 来计算的，将数据写入到磁盘是以 Page 为单位进行操作的，而文件系统是以 4k 为单位写入，磁盘 IO 的最小单位是 512K，因此并不能保证数据页的写入就是原子性的。 那么可不可以通过 redo log 来进行恢复呢？答案是只能恢复校验完整（还没写）的页，不能恢复已损坏的页。比如某次 checkpoint 要刷入 4 个数据页，其中第一页写了 2KB，后三页还未写。那么根据 redo log 可以恢复后三页，但已经写了 2KB 的页没法恢复，因为没法知道在宕机前第一页到底写了多少。 为什么 redo log 不需要 doublewrite 的支持？ 因为 redolog 写入的单位就是 512 字节，也就是磁盘 IO 的最小单位，所以无所谓数据损坏。 double write 由两部分组成，一部分是内存中的 doublewrite buffer，大小为 2MB，另一部分是物理磁盘上共享表空间中连续的 128 个页，即 2 个区，大小同样为 2MB。在对缓冲池的脏页进行刷新时，并不直接写磁盘，而是会通过 memcpy 函数将脏页先复制到内存中的 doublewrite buffer，之后通过 doublewrite buffer 再分两次，每次 1MB 顺序地写入共享表空间的物理磁盘上，然后马上调用 fsync 函数，同步磁盘。在这个过程中，因为 doublewrite 页是连续的，因此这个过程是顺序写的，开销不是很大。其工作流程如下图所示： 现在我们来分析一下为什么 double write 可以生效。当宕机发生时，有那么几种情况：1、磁盘还未写，此时可以通过 redo log 恢复；2、磁盘正在进行从内存到共享表空间的写，此时数据文件中的页还没开始被写入，因此也同样可以通过 redo log 恢复；3、磁盘正在写数据文件，此时共享表空间已经写完，可以从共享表空间拷贝页的副本到数据文件实现恢复。 自适应哈希索引哈希是一种非常快的查询方法，一般只需要一次查找就能定位数据。InnoDB 存储引擎会监控对表上各索引页的查询，如果观察到建立哈希索引可以带来速度提升，则建立哈希索引，称之为自适应哈希索引。 自适应哈希索引有一个要求，即对这个页的连续访问模式必须是一样的。例如对于（a,b）这样的联合索引页，其访问模式可以是以下情况： WHERE a=xxx WHERE a=xxx and b=xxx 若交替以上两种查询，那么 InnoDB 存储引擎不会对该页构造哈希索引（这是因为哈希索引是以索引的哈希值为键值存放的，hash(a) 和 hash(a,b) 是两个完全不同的值） 在连续的查询模式一样的条件下，如果能满足以下条件，InnoDB 存储引擎就会创建相应的哈希索引： 以该连续模式连续访问了 100 次 以该模式连续访问了 页中记录总数/16 次 哈希索引只能用来搜索等值的查询，如 SELECT * FROM table WHERE index_col=’xxx’。对于其它类型的查找，如范围查找，是不能使用哈希索引的。 InnoDB 存储引擎官方文档显示，启用 AHI 后,读取和写入速度可以提高 2 倍，辅助索引的连接操作性能可以提高 5 倍。 总结InnoDB 存储引擎在 MySQL 原有的基础上做了很多优化，主要涉及到的就是缓冲池和磁盘的交互。尽可能多地读缓存，尽量少地读磁盘，于是有了自适应哈希索引；尽量多地顺序写，尽量少地离散写，于是有了插入缓冲；由于缓存的易失性，带来的数据恢复问题，又有了两次写。这些设计思想不只可以用于数据库，也可以用于程序设计的方方面面。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"Java NIO 浅析","slug":"Java-NIO 浅析","date":"2019-01-09T04:10:18.000Z","updated":"2019-03-08T14:08:50.656Z","comments":true,"path":"2019/01/09/Java-NIO 浅析/","link":"","permalink":"http://yoursite.com/2019/01/09/Java-NIO 浅析/","excerpt":"","text":"NIO（Non-blocking I/O），是一种同步非阻塞的 I/O 模型，也是 I/O 多路复用的基础，已经被越来越多地应用到大型应用服务器，成为解决高并发与大量连接、I/O 处理问题的有效方式。Java 中的 NIO 是 jdk 1.4 之后新出的一套 IO 接口，相比传统 IO(BIO)，两者有如下区别： IO 是面向流的，NIO 是面向缓冲区的 IO 流是同步阻塞的，NIO 流是同步非阻塞的 NIO 有选择器（Selector），IO 没有 IO 的流是单向的，NIO 的通道（Channel）是双向的 IO 基本概念Linux 的内核将所有外部设备都可以看做一个文件来操作。那么我们对与外部设备的操作都可以看做对文件进行操作。我们对一个文件的读写，都通过调用内核提供的系统调用；内核给我们返回一个 file descriptor（fd,文件描述符）。对一个 socket 的读写也会有相应的描述符，称为 socketfd(socket 描述符）。描述符就是一个数字 (可以理解为一个索引)，指向内核中一个结构体（文件路径，数据区，等一些属性）。应用程序对文件的读写就通过对描述符的读写完成。 一个基本的 IO，它会涉及到两个系统对象，一个是调用这个 IO 的进程对象，另一个就是系统内核 (kernel)。当一个 read 操作发生时，它会经历四个阶段： 1、通过 read 系统调用想内核发起读请求。 2、内核向硬件发送读指令，并等待读就绪。 3、内核把将要读取的数据复制到描述符所指向的内核缓存区中。 4、将数据从内核缓存区拷贝到用户进程空间中。 同步和异步同步和异步关注的是消息通信机制 (synchronous communication / asynchronous communication)。所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。 而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 阻塞和非阻塞阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态。阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 常见 I/O 模型对比所有的系统 I/O 都分为两个阶段：等待就绪和操作。举例来说，读函数，分为等待系统可读和真正的读；同理，写函数分为等待网卡可以写和真正的写。需要说明的是等待就绪的阻塞是不使用 CPU 的，是在“空等”；而真正的读写操作的阻塞是使用 CPU 的，真正在”干活”，而且这个过程非常快，属于 memory copy，带宽通常在 1GB/s 级别以上，可以理解为基本不耗时。 以 socket.read() 为例子：传统的 BIO 里面 socket.read()，如果 TCP RecvBuffer 里没有数据，函数会一直阻塞，直到收到数据，返回读到的数据。对于 NIO，如果 TCP RecvBuffer 有数据，就把数据从网卡读到内存，并且返回给用户；反之则直接返回 0，永远不会阻塞。最新的 AIO(Async I/O) 里面会更进一步：不但等待就绪是非阻塞的，就连数据从网卡到内存的过程也是异步的。换句话说，BIO 里用户最关心“我要读”，NIO 里用户最关心”我可以读了”，在 AIO 模型里用户更需要关注的是“读完了”。NIO 一个重要的特点是：socket 主要的读、写、注册和接收函数，在等待就绪阶段都是非阻塞的，真正的 I/O 操作是同步阻塞的（消耗 CPU 但性能非常高）。 传统 BIO 模型分析了解 NIO 就要从传统 BIO 的弊端说起。 在传统的 BIO 中，一旦用户线程发起 IO 请求，则必须要等内核将数据报准备好，才能将数据从内核复制到用户空间。这是一种效率很低的方式。传统的 BIO 一般要配合线程池来使用，我们的编程范式（伪代码）一般是这样的： 123456789101112131415161718192021222324ExecutorService executor = Excutors.newFixedThreadPollExecutor(100); // 线程池ServerSocket serverSocket = new ServerSocket();serverSocket.bind(8088);while(!Thread.currentThread.isInturrupted())&#123; // 主线程死循环等待新连接到来 Socket socket = serverSocket.accept(); executor.submit(new ConnectIOnHandler(socket)); // 为新的连接创建新的线程&#125;class ConnectIOnHandler extends Thread&#123; private Socket socket; public ConnectIOnHandler(Socket socket)&#123; this.socket = socket; &#125; public void run()&#123; while(!Thread.currentThread.isInturrupted()&amp;&amp;!socket.isClosed())&#123; // 死循环处理读写事件 String someThing = socket.read()....// 读取数据 if(someThing!=null)&#123; ......//处理数据 socket.write()....// 写数据 &#125; &#125; &#125;&#125; 这是一个经典的每连接每线程的模型，之所以使用多线程，主要原因在于 socket.accept()、socket.read()、socket.write() 三个主要函数都是同步阻塞的，当一个连接在处理 I/O 的时候，系统是阻塞的，如果是单线程的话必然就挂死在那里；但 CPU 是被释放出来的，开启多线程，就可以让 CPU 去处理更多的事情。其实这也是所有使用多线程的本质： 利用多核。 当 I/O 阻塞系统，但 CPU 空闲的时候，可以利用多线程使用 CPU 资源。 现在的多线程一般都使用线程池，可以让线程的创建和回收成本相对较低。在活动连接数不是特别高（小于单机 1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。 不过，这个模型最本质的问题在于，严重依赖于线程。但线程是很”贵”的资源，主要表现在： 线程的创建和销毁成本很高，在 Linux 这样的操作系统中，线程本质上就是一个进程。创建和销毁都是重量级的系统函数。 线程本身占用较大内存，像 Java 的线程栈，一般至少分配 512K～1M 的空间，如果系统中的线程数过千，恐怕整个 JVM 的内存都会被吃掉一半。 线程的切换成本是很高的。操作系统发生线程切换的时候，需要保留线程的上下文，然后执行系统调用。如果线程数过高，可能执行线程切换的时间甚至会大于线程执行的时间，这时候带来的表现往往是系统 load 偏高、CPU sy 使用率特别高（超过 20%以上)，导致系统几乎陷入不可用的状态。 容易造成锯齿状的系统负载。因为系统负载是用活动线程数或 CPU 核心数，一旦线程数量高但外部网络环境不是很稳定，就很容易造成大量请求的结果同时返回，激活大量阻塞线程从而使系统负载压力过大。 所以，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。随着移动端应用的兴起和各种网络游戏的盛行，百万级长连接日趋普遍，此时，必然需要一种更高效的 I/O 处理模型。 NIO 是如何工作的 这是一个 NIO 基本的工作方式（但不常用），我们把一个套接口设置为非阻塞，当所请求的 I/O 操作不能满足要求时候，不把本进程投入睡眠，而是返回一个错误。也就是说当数据没有到达时并不等待，而是以一个错误返回。 事件驱动的 I/O 复用模型（常用）在 BIO 的场景下，为了避免线程长时间阻塞在等待内核准备上，我们选择了每连接每线程的方式。但在 NIO 的场景下，如果当前的连接没有准备好，可以选择下一个连接。比如我们的聊天程序，我们可以建立两个连接：一个发送端，一个接收端。程序会不断轮询这两个连接，如果接收端有数据达到，那就把它显示在屏幕上；如果发送端有数据发出，那就把它发出。但如果接收端没有数据，或者发送端的网卡没有准备好，程序也不会停下来，而是继续轮询，直到有一方准备好。这种一个进程/线程处理多个 IO 的方式，被称为 I/O 复用模型。 而如果我们把发送就绪和接收就绪当成两类事件，只有在这两类事件发生的时候才会触发轮询，其它时候（比如等待请求时），程序不会被唤醒，那么这种方式就被称为事件驱动。 Linux 中的 select,poll,epoll 是典型的事件驱动的 I/O 复用模型： select() 会把所有的 I/O 请求封装为文件描述符 (fd) 的形式给操作系统，让操作系统内核去查询这些套接字上是否有事件发生，轮询完后，再将句柄数据复制到用户态，让服务器应用程序轮询处理已发生的网络事件，以此来实现一个线程/进程管理多个 I/O 的功能。 poll() 在 select 上支持更多数量的 fd。因为 select 中使用数组形式存放文件描述符，数量有限（一般 1024 个），poll 使用链表的形式，理论上支持的描述符数量没有上限。 epoll() 在 select/poll 的基础上有了大幅改进： 它使用红黑树来存储所有需要查询的事件，事件的添加和删除对应红黑树的插入和删除，复杂度从 O(N) 降为了 O(logN)。 它使用双向链表来保存就绪的事件。所有添加到红黑树上的事件都会与设备 (网卡) 驱动程序建立回调关系，当相应的事件发生时会调用这个回调方法，回调方法会把事件放入双向链表中。 返回时返回的是就绪事件（双向链表）而不是所有事件，既减少了内核到用户空间的拷贝数据量，又省了用户程序筛选就绪事件的时间。 相比 select/poll 的水平触发模式，epoll 也支持边沿触发模式。即用户可以选择到底是接受所有就绪的事件（水平触发），还是只接受上次检查以后新就绪的事件（边沿触发）。 Java 中的 NIO 模型Java 中的 NIO 模型选用了事件驱动的 I/O 复用模型。事实上，在 Linux 上 Java 的 NIO 就是基于 select,poll,epoll 来实现的（Linux 2.6 之前是 select、poll，2.6 之后是 epoll）。 在 Java 的 NIO 中，有 4 类事件：读就绪（OP_READ），写就绪（OP_WRITE），收到请求（仅服务端有效，OP_ACCEPT），发出请求（仅客户端有效，OP_CONNECT）。我们需要注册当这几个事件到来的时候所对应的处理器。然后在合适的时机告诉事件选择器：我对这个事件感兴趣。对于写操作，就是写不出去的时候对写事件感兴趣；对于读操作，就是完成连接和系统没有办法承载新读入的数据的时；对于 accept，一般是服务器刚启动的时候；而对于 connect，一般是 connect 失败需要重连或者直接异步调用 connect 的时候。新事件到来的时候，会在 selector 上注册标记位，标示可读、可写或者有连接到来。编程范式（伪代码）一般如下： 12345678910111213141516171819202122232425262728 //处理器抽象接口interface ChannelHandler&#123; void channelReadable(Channel channel); void channelWritable(Channel channel);&#125;class Channel&#123; Socket socket; Event event;//读，写或者连接&#125;Map&lt;Channel，ChannelHandler&gt; handlerMap;//所有 channel 的对应事件处理器//IO 线程主循环:class IoThread extends Thread&#123; public void run()&#123; Channel channel; while(channel=Selector.select())&#123;//选择就绪的事件和对应的连接 if(channel.event==accept)&#123; registerNewChannelHandler(channel);//如果是新连接，则注册一个新的读写处理器 &#125; if(channel.event==write)&#123; getChannelHandler(channel).channelWritable(channel);//如果可以写，则执行写事件 &#125; if(channel.event==read)&#123; getChannelHandler(channel).channelReadable(channel);//如果可以读，则执行读事件 &#125; &#125; &#125;&#125; Buffer 的选择Java 中的 NIO 还有一个特点是面向缓冲区的。这一特性其实在传统 IO 中就有用到，这里不再赘述。但是 Buffer 的选择也是一个值得注意的地方。 通常情况下，操作系统的一次写操作分为两步： 1. 将数据从用户空间拷贝到系统空间。 2. 从系统空间往网卡写。同理，读操作也分为两步： ① 将数据从网卡拷贝到系统空间； ② 将数据从系统空间拷贝到用户空间。 对于 NIO 来说，缓存的使用可以使用 DirectByteBuffer 和 HeapByteBuffer。如果使用了 DirectByteBuffer，一般来说可以减少一次系统空间到用户空间的拷贝。但 Buffer 创建和销毁的成本更高，更不宜维护，通常会用内存池来提高性能。如果数据量比较小的中小应用情况下，可以考虑使用 heapBuffer；反之可以用 directBuffer。 使用 NIO != 高性能，当连接数 &lt;1000，并发程度不高或者局域网环境下 NIO 并没有显著的性能优势。 NIO 并没有完全屏蔽平台差异，它仍然是基于各个操作系统的 I/O 系统实现的，差异仍然存在。使用 NIO 做网络编程构建事件驱动模型并不容易，陷阱重重。 推荐大家使用成熟的 NIO 框架，如 Netty，MINA 等。解决了很多 NIO 的陷阱，并屏蔽了操作系统的差异，有较好的性能和编程模型。 总结最后总结一下 Java 中的 NIO 为我们带来了什么： 非阻塞 I/O，I/O 读写不再阻塞，而是返回 0 避免多线程，单个线程可以处理多个任务 事件驱动模型 基于 block 的传输，通常比基于流的传输更高效 IO 多路复用大大提高了 Java 网络应用的可伸缩性和实用性","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"Java 线程池浅析","slug":"Java-线程池浅析","date":"2018-09-15T05:58:26.000Z","updated":"2019-03-08T14:10:07.280Z","comments":true,"path":"2018/09/15/Java-线程池浅析/","link":"","permalink":"http://yoursite.com/2018/09/15/Java-线程池浅析/","excerpt":"","text":"Java 中的线程池是运用场景最多的并发框架，几乎所有需要异步或并发执行任务的程序都可以使用线程池。在开发过程中，合理地使用线程池能够带来 3 个好处。 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。 线程池的继承关系如下图： 线程池最顶层是Executor，这是只有一个 execute 方法的接口，也是整个 Executor 框架的顶层接口，所有 Executor 框架的组件都要实现这个接口。 ExecutorService继承了 Executor，在此基础上增加了 submit(Runnable) 和 submit(Callable)，表示任务的提交，Runnable 和 Callable 的区别在于 Callable 的 call() 方法有返回值，而 Runnable 的 run 没有。 ThreadPoolExecutor是线程池的核心实现类，大部分线程池的功能都在这个类中被定义，它有多个参数和构造函数，根据不同的构造参数可以实现不同功能的线程池。线程池的参数会在下文详细介绍。 Executors是 ThreadPoolExecutor 的工厂类，封装了一些常用的线程池，具体类型也会在下文详细介绍。 线程池基本概念创建一个线程池我们可以通过 ThreadPoolExecutor 来创建一个线程池： 1new ThreadPoolExecutor(corePoolSize,maximumPoolSize,keepAliveTime,unit,workQueue,ThreadFactory,RejectedExecutionHandler) 线程池的构造函数中需要接收 7 个参数，它们分别是： corePoolSize 核心线程数，指保留的线程池大小（不超过 maximumPoolSize 值时，线程池中最多有 corePoolSize 个线程工作） maximumPoolSize 指的是线程池的最大大小（线程池中最大有 maximumPoolSize 个线程可运行） keepAliveTime 指的是空闲线程结束的超时时间（当一个线程不工作时，过 keepAliveTime 长时间将停止该线程） unit 是一个枚举，表示 keepAliveTime 的单位（有 NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS，7 个可选值） workQueue 表示存放任务的队列（存放需要被线程池执行的线程队列）。它的类型是 BlockingQueue 就是阻塞队列，有关阻塞队列的内容可以参考这篇《阻塞队列源码阅读》 threadFactory 是一个线程工厂，负责线程的创建，一般会使用默认的 Executors.defaultThreadFactory()。 handler 拒绝策略（添加任务失败后如何处理该任务） 线程池的运行策略线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。我们可以使用execute()方法提交任务到线程池： 123456executorService.execute(new Runnable() &#123; @Override public void run() &#123; //TODO &#125;&#125;); 也可以使用submit()方法提交任务到线程池： 123456Future future = executorService.submit(new Runnable() &#123; @Override public void run() &#123; //TODO &#125;&#125;); 区别在于 submit() 会返回一个 Future 对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get() 方法来获取返回值。另外继承了 ExecutorService 接口的 ScheduledExecutorService 还可以使用schedule()方法来提交一个定时任务： 123456scheduledExecutorService.schedule(new Runnable() &#123; @Override public void run() &#123; //TODO &#125;&#125;, 1, TimeUnit.SECONDS); 上面代码就会在 1 秒后执行我们的定时任务。无论是 submit() 还是 schedule()，其底层最后都会调用 execute() 来提交执行任务。不过，就算队列里面有任务，线程池也不会马上执行它们。 当添加一个任务时，线程池会做如下判断： 如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务； 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列； 如果这时候队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么还是要创建线程运行这个任务； 如果队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会调用 reject()，这个方法会调用 handler.rejectedExecution() 方法，根据不同的 handler 策略会有不同的处理方式。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。 线程池的拒绝策略上面提到任务添加失败后，线程池会调用 reject() 方法，这个方法会调用 handler.rejectedExecution() 方法，根据不同的 handler 策略会有不同的处理方式。线程池中预设有以下几种处理方式： AbortPolicy：为 Java 线程池默认的阻塞策略，不执行此任务，而且直接抛出一个运行时异常，切记 ThreadPoolExecutor.execute 需要 try catch，否则程序会直接退出。 DiscardPolicy：直接抛弃，任务不执行，空方法。 DiscardOldestPolicy：从队列里面抛弃 head 的一个任务，并再次 execute 此 task。 CallerRunsPolicy：还给原线程自己执行，会阻塞入口。 用户自定义拒绝策略：实现 RejectedExecutionHandler，并自己定义策略模式。 关闭线程池Java 线程池提供了两个方法用于关闭一个线程池，一个是 shutdownNow()，另一个是 shutdown()。我们可以看一下这两个方法的声明： 12void shutdown();List&lt;Runnable&gt; shutdownNow(); 这两个方法的区别在于： shutdown()：当线程池调用该方法时，线程池的状态则立刻变成 SHUTDOWN 状态。我们不能再往线程池中添加任何任务，否则将会抛出 RejectedExecutionException 异常；但是，此时线程池不会立刻退出，直到添加到线程池中的任务都已经处理完成后才会退出。 shutdownNow()：当执行该方法，线程池的状态立刻变成 STOP 状态，并试图停止所有正在执行的线程，不再处理还在池队列中等待的任务，并以返回值的形式返回那些未执行的任务。此方法会通过调用 Thread.interrupt() 方法来试图停止正在运行的 Worker 线程，但是这种方法的作用有限，如果线程中没有 sleep 、wait、Condition、定时锁等操作时，interrupt() 方法是无法中断当前的线程的。所以，shutdownNow() 并不代表线程池就一定立即就能退出，可能必须要等待所有正在执行的任务都执行完成了才能退出。 Executors 提供的线程池ThreadPoolExecutor 提供的线程创建方式参数太多，对开发人员并不友好。因此 Java 在 Executors 类中封装了几种常用的线程池，它们分别是： Executors.newCachedThreadPool 这是一个会根据需要创建线程的线程池，它的 corePoolSize 被设置为 0，maximumPoolSize 被设置为 Integer.MAX_VALUE，KeepAliveTime 被设置为 60s，使用没有容量的 SynchronousQueue 作为线程池的工作队列。这就意味着，线程池中没有固定的线程数量，任何一个任务被提交时，线程池都会为它创建或者分配一个线程；而任何一个线程空闲时间超过 60s，都会关闭它。使用该线程池时要注意主线程提交任务的速度和线程池处理任务的速度，若提交速度大于处理速度，CachedThreadPool 会因为创建过多线程而耗尽 CPU 和内存资源。该线程池的吞吐量在几种预设线程池中是最大的。 Executors.newFixedThreadPool 这是被称为可重用固定线程数的线程池，它的 corePoolSize 等于 maximumPoolSize，KeepAliveTime 被设置为 0，使用最大长度的有界队列 LinkedBlockingQueue（队列容量为 Integer.MAX_VALUE）作为工作队列，这也意味着 FixedThreadPool 运行稳定后线程数量是不变的，且所有任务都会进入工作队列，不会拒绝任务。 Executors.newSingleThreadExecutor 这是一个只有一个工作线程的线程池，它的 corePoolSize 和 maximumPoolSize 都被设置为 1，其它参数与 FixedThreadPool 相同，可以把它理解为 newFixedThreadPool(1)，线程执行完任务后会无限反复从 LinkedBlockingQueue 获取任务来执行。 Executors.newScheduledThreadPool 这是一个可以定时执行的线程池，它的 maximumPoolSize 被设置为 Integer.MAX_VALUE，KeepAliveTime 被设置为 10ms，使用 DelayedWorkQueue 作为阻塞队列，这是一个类似于 DelayedQueue 和 PriorityBlockingQueue 的阻塞队列，每次会取出队列中执行时间最早的任务，如果没有到执行之间，则 await 两者的差值，以此来达到定时执行的目的。同时 newScheduledThreadPool 的 scheduleAtFixedRate 和 scheduleWithFixedRate 方法还可以实现周期执行的功能。两者都是依靠给 ScheduledFutureTask（newScheduledThreadPool 中被执行的任务）设置下一次执行时间来实现的。区别在于 scheduleAtFixedRate 中下次执行时间=本次开始时间+间隔时间，而 scheduleWithFixedRate 中下次执行时间=本次结束时间+间隔时间。 线程池代码分析线程池的属性字段在开始深入了解 ThreadPoolExecutor 代码之前, 我们先来简单地看一下 ThreadPoolExecutor 类中到底有哪些重要的字段。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class ThreadPoolExecutor extends AbstractExecutorService &#123; // 这个是一个复用字段, 它复用地表示了当前线程池的状态, 当前线程数信息. private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); // 用于存放提交到线程池中, 但是还未执行的那些任务. private final BlockingQueue&lt;Runnable&gt; workQueue; // 线程池内部锁, 对线程池内部操作加锁, 防止竞态条件 private final ReentrantLock mainLock = new ReentrantLock(); // 一个 Set 结构, 包含了当前线程池中的所有工作线程. // 对 workers 字段的操作前, 需要获取到这个锁. private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); // 条件变量, 用于支持 awaitTermination 操作 private final Condition termination = mainLock.newCondition(); // 记录线程池中曾经到达过的最大的线程数. // 这个字段在获取 mainLock 锁的前提下才能操作. private int largestPoolSize; // 记录已经完成的任务数. 仅仅当工作线程结束时才更新此字段. // 这个字段在获取 mainLock 锁的前提下才能操作. private long completedTaskCount; // 线程工厂. 当需要一个新的线程时, 这里生成. private volatile ThreadFactory threadFactory; // 任务提交失败后的处理 handler private volatile RejectedExecutionHandler handler; // 空闲线程的等待任务时间, 以纳秒为单位. // 当当前线程池中的线程数大于 corePoolSize 时, // 或者 allowCoreThreadTimeOut 为真时, 线程才有 idle 等待超时时间, // 如果超时则此线程会停止.; // 反之线程会一直等待新任务到来. private volatile long keepAliveTime; // 默认为 false. // 当为 false 时, keepAliveTime 不起作用, 线程池中的 core 线程会一直存活, // 即使这些线程是 idle 状态. // 当为 true 时, core 线程使用 keepAliveTime 作为 idle 超时 // 时间来等待新的任务. private volatile boolean allowCoreThreadTimeOut; // 核心线程数. private volatile int corePoolSize; // 最大线程数. private volatile int maximumPoolSize;&#125; ThreadPoolExecutor 中, 使用到 ctl 这个字段来维护线程池中当前线程数和线程池的状态。ctl 是一个 AtomicInteger 类型, 它的低 29 位用于存放当前的线程数，因此一个线程池在理论上最大的线程数是 536870911；高 3 位是用于表示当前线程池的状态，其中高三位的值和状态对应如下： 111: RUNNING 此时能够接收新任务，以及对已添加的任务进行处理。状态切换：线程池初始化时就是 RUNNING 状态。 000: SHUTDOWN 此时不接收新任务，但能处理已添加的任务。状态切换：调用线程池的 shutdown() 接口时，线程池由 RUNNING -&gt; SHUTDOWN。 001: STOP 此时不接收新任务，不处理已添加的任务，并且会中断正在处理的任务。状态切换：调用线程池的 shutdownNow() 接口时，线程池由 (RUNNING or SHUTDOWN ) -&gt; STOP。 010: TIDYING 当所有的任务已终止，ctl 记录的”任务数量”为 0，线程池会变为 TIDYING 状态。当线程池变为 TIDYING 状态时，会执行钩子函数 terminated()。terminated() 在 ThreadPoolExecutor 类中是空的，若用户想在线程池变为 TIDYING 时，进行相应的处理；可以通过重载 terminated() 函数来实现。状态切换：当线程池在 SHUTDOWN 状态下，阻塞队列为空并且线程池中执行的任务也为空时，就会由 SHUTDOWN -&gt; TIDYING。当线程池在 STOP 状态下，线程池中执行的任务为空时，就会由 STOP -&gt; TIDYING。 011: TERMINATED 线程池彻底终止，就变成 TERMINATED 状态。状态切换：线程池处在 TIDYING 状态时，执行完 terminated() 之后，就会由 TIDYING -&gt; TERMINATED。 提交任务到线程池1234567891011121314151617181920public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command);&#125; 上面的代码有三个步骤，首先第一步是检查当前线程池的线程数是否小于 corePoolSize，如果小于，那么由我们前面提到的规则，线程池会创建一个新的线程来执行此任务，因此在第一个 if 语句中，会调用 addWorker(command, true) 来创建一个新 Worker 线程，并执行此任务。 如果当前线程池的线程数不小于 corePoolSize，那么会尝试将此任务插入到工作队列中，即 workQueue.offer(command)。当插入到 workQueue 成功后，我们还需要再次检查一下此时线程池是否还是 RUNNING 状态，如果不是的话就会将原来插入队列中的那个任务删除，然后调用 reject 方法拒绝此任务的提交；接着考虑到在我们插入任务到 workQueue 中的同时，如果此时线程池中的线程都执行完毕并终止了，在这样的情况下刚刚插入到 workQueue 中的任务就永远不会得到执行了。为了避免这样的情况，因此我们要再次检查一下线程池中的线程数，如果为零，则调用 addWorker(null, false) 来添加一个线程。 最后如果任务插入到工作队列失败了，就会直接调用 addWorker(command, false) 来新开一个线程。如果失败了，那么我们就知道线程池已经关闭或者饱和了，就拒绝这次添加。 关于 addWorker 方法前面我们大体分析了一下 execute 提交任务的流程，不过省略了一个关键步骤，即 addWorker 方法。现在我们来看看这个方法里究竟发生了什么。 首先看一下 addWorker 方法的签名：1private boolean addWorker(Runnable firstTask, boolean core) 这个方法接收两个参数，第一个是一个 Runnable 类型的参数，一般来说是我们调用 execute 方法所传输的参数，不过也有可能是 null 值，这样的情况我们在前面一小节中也见到过。那么第二个参数是做什么的呢？第二个参数是一个 boolean 类型的变量，它的作用是标识是否使用 corePoolSize 属性。我们知道，ThreadPoolExecutor 中，有一个 corePoolSize 属性，用于规定线程池中的核心线程数。那么当 core 这个参数是 true 时，则表示在添加新任务时，需要考虑到 corePoolSzie 的影响（例如如果此时线程数已经大于 corePoolSize 了，那么就不能再添加新线程了）；当 core 为 false 时，就不考虑 corePoolSize 的影响，而是以 maximumPoolSize 代替 corePoolSize 来做判断条件。 然后是 addWorker 的源码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (int c = ctl.get();;) &#123; // Check if queue empty only if necessary. if (runStateAtLeast(c, SHUTDOWN) &amp;&amp; (runStateAtLeast(c, STOP) || firstTask != null || workQueue.isEmpty())) return false; for (;;) &#123; // 当 core 为真, 那么就判断当前线程是否大于 corePoolSize // 当 core 为假, 那么就判断当前线程数是否大于 maximumPoolSize // 这里的 for 循环是一个自旋 CAS(CompareAndSwap) 操作, 用于确保多线程环境下的正确性 if (workerCountOf(c) &gt;= ((core ? corePoolSize : maximumPoolSize) &amp; COUNT_MASK)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateAtLeast(c, SHUTDOWN)) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int c = ctl.get(); if (isRunning(c) || (runStateLessThan(c, STOP) &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 首先在 addWorker 的一开始，有一个 for 循环，用于判断当前是否可以添加新的 Worker 线程。它的逻辑如下： 如果传入的 core 为真，那么判断当前的线程数是否大于 corePoolSize，如果大于，则不能新建 Worker 线程，返回 false。 如果传入的 core 为假，那么判断当前的线程数是否大于 maximumPoolSize，如果大于，则不能新建 Worker 线程，返回 false。 如果条件符合，那么在 for 循环内，又有一个自旋 CAS 更新逻辑，用于递增当前的线程数，即 compareAndIncrementWorkerCount(c)，这个方法会原子地更新 ctl 的值，将当前线程数的值+1。addWorker 接下来有一个 try…finally 语句块，这里就是实际上的创建线程、启动线程、添加线程到线程池中的工作了。首先可以看到 w = new Worker(firstTask)；这里是实例化一个 Worker 对象，这个类其实就是 ThreadPoolExecutor 中对工作线程的封装。Worker 类继承于 AbstractQueuedSynchronizer 并实现了 Runnable 接口，我们来看一下它的构造器： 12345Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this);&#125; 它会把我们提交的任务（firstTask）设置为自己的内部属性 firstTask，然后使用 ThreadPoolExecutor 中的 threadFactory 来创建一个新的线程，并保存在 thread 字段中，而且注意到，创建线程时，我们传递给新线程的 Runnable 其实是 Worker 对象本身（this），因此当这个线程启动时，实际上运行的是 Worker.run() 中的代码。 回过头来再看一下 addWorker 方法。当创建好 Worker 线程后，就会将这个 worker 线程存放在 workers 这个 HashSet 类型的字段中。而且注意到，正如我们在前面所提到的，mainLock 是 ThreadPoolExecutor 的内部锁，我们对 ThreadPoolExecutor 中的字段进行操作时，为了保证线程安全，需要在获取到 mainLock 的前提下才能操作。 最后，我们可以看到，在 addWorker 方法的最后，调用了 t.start()；来真正启动这个新建的线程。 任务的分配与调度线程池在执行完 firstTask 后并不会立即销毁，而是可以根据情况复用。线程的复用就涉及到任务的分配与调度。Java 线程池的调度方式很简单，就是执行完之后从 workQueue 中拿出下一个任务，如果获取到了任务，那就再次执行。 前一小节中，我们看到 addWorker 中会新建一个 Worker 对象来代表一个 worker 线程，接着会调用线程的 start() 来启动这个线程，我们也提到了当启动这个线程后，会运行到 Worker 中的 run 方法，我们来看一下 Worker.run 具体的实现：123public void run() &#123; runWorker(this);&#125; Worker.run 方法很简单，只是调用了 ThreadPoolExecutor.runWorker 方法而已。runWorker 方法比较关键，它是整个线程池任务分配的核心：12345678910111213141516171819202122232425262728293031323334353637383940414243final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125; runWorker 方法是整个工作线程的核心循环，在这个循环中，工作线程会不断的从 workerQuque 中获取新的 task，然后执行它。我们注意到在 runWorker 一开始，有一个 w.unlock()，咦, 这是为什么呢? 其实这是 Worker 类玩的一个小把戏。回想一下，Worker 类继承于 AQS 并实现了 Runnable 接口，它的构造器如下：12345Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this);&#125; setState(-1) 方法是 AQS 提供的，初始化 Worker 时，会先设置 state 为 -1，根据注释，这样做的原因是为了抑制工作线程的 interrupt 信号，直到此工作线程开始执行 task。那么在 addWorker 中的 w.unlock() 就是允许 Worker 的 interrupt 信号。 接着在 addWorker 中会进入一个 while 循环，在这里此工作线程会不断地从 workQueue 中取出一个任务，然后调用 task.run() 来执行这个任务，因此就执行到了用户所提交的 Runnable 中的 run() 方法了。 工作线程的 idle 超时处理工作线程的 idle 超出处理在底层依赖于 BlockingQueue 带超时的 poll 方法，即工作线程会不断地从 workQueue 这个 BlockingQueue 中获取任务，如果 allowCoreThreadTimeOut 字段为 true，或者当前的工作线程数大于 corePoolSize，那么线程的 idle 超时机制就生效了，此时工作线程会以带超时的 poll 方式从 workQueue 中获取任务。当超时了还没有获取到任务，那么我们就知道此线程已经到达 idle 超时时间了，就终止此工作线程。具体源码如下：12345678910111213141516171819202122232425262728293031323334353637private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; 从源码中就可以看到，一开始会判断当前的线程池状态，如果不是 SHUTDOWN 或 STOP 之类的状态，那么接着获取当前的工作线程数，然后判断工作线程数量是否已经大于了 corePoolSize。当 allowCoreThreadTimeOut 字段为 true，或者当前的工作线程数大于 corePoolSize，那么线程的 idle 超时机制就生效，此时工作线程会以带超时的 workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) 方式从 workQueue 中获取任务；反之会以 workQueue.take() 方式阻塞等待任务，直到获取一个新的任务。当从 workQueue 获取新任务超时时，会调用 compareAndDecrementWorkerCount 将当前的工作线程数-1，并返回 null。getTask 方法返回 null 后， runWorker 中的 while 循环自然也就结束了，因此也导致了 runWorker 方法的返回，最后自然整个工作线程的 run() 方法执行完毕，工作线程自然就终止了。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"Java 阻塞队列浅析","slug":"Java-阻塞队列浅析","date":"2018-09-03T13:58:36.000Z","updated":"2019-03-08T14:11:28.678Z","comments":true,"path":"2018/09/03/Java-阻塞队列浅析/","link":"","permalink":"http://yoursite.com/2018/09/03/Java-阻塞队列浅析/","excerpt":"","text":"阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 一、Java 中的阻塞队列JDK7 中提供了 7 个阻塞队列，分别是： ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。 LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。 PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。 DelayQueue：一个使用优先级队列实现的无界阻塞队列。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 ArrayBlockingQueueArrayBlockingQueue 是一个用数组实现的有界阻塞队列。此队列按照先进先出（FIFO）的原则对元素进行排序。默认情况下不保证访问者公平的访问队列，所谓公平访问队列是指阻塞的所有生产者线程或消费者线程，当队列可用时，可以按照阻塞的先后顺序访问队列，即先阻塞的生产者线程，可以先往队列里插入元素，先阻塞的消费者线程，可以先从队列里获取元素。ArrayBlockingQueue 的公平性是由 ReentrantLock 实现的，公平模式下的 ReentrantLock 在有一个线程申请锁时，会检测 AQS 队头是否有等待的线程，如果有则执行队头的线程，而非公平模式下则不会进行检测。通常情况下为了保证公平性会降低吞吐量。 12345678public ArrayBlockingQueue(int capacity, boolean fair) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); // ReentrantLock 实现了公平策略 notEmpty = lock.newCondition(); notFull = lock.newCondition();&#125; LinkedBlockingQueueLinkedBlockingQueue 是一个用链表实现的有界阻塞队列。此队列的默认最大长度为 Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序。 PriorityBlockingQueuePriorityBlockingQueue 是一个支持优先级的无界队列，底层由数组实现，数组最大长度为 Integer.MAX_VALUE - 8（所以并不是真的无界），默认数组大小为 11，如果元素超过数组大小，数组会进行扩容，具体扩容的大小，在数组长度小于 64 时+2，大于 64 时变为 150%，当数组扩容后大小超过 Integer.MAX_VALUE - 8 时，不会再用 150%扩容，而是使用+1 的方式扩容。最后若+1 的方式扩容的数组大小也超过了 Integer.MAX_VALUE - 8，则报 OutOfMemoryError()。默认情况下元素采取自然顺序，排列采用二叉树最小堆实现（所以有序不是数据存储的有序，而是取出数据的有序），也可以通过比较器 comparator 来指定元素的排序规则。元素按照升序排列。 DelayQueueDelayQueue 是一个支持延时获取元素的无界阻塞队列。队列使用 PriorityQueue，PriorityQueue 和 PriorityBlockingQueue 一样都是使用小顶堆实现的，唯一的区别在于 PriorityQueue 是非线程安全的，而 PriorityBlockingQueue 是线程安全的。队列中的元素必须实现 Delayed 接口（该接口有两个方法getDelay()和compareTo()），在创建元素时可以指定多久才能从队列中获取当前元素。只有在延迟期满时才能从队列中提取元素。我们可以将 DelayQueue 运用在以下应用场景： 缓存系统的设计：可以用 DelayQueue 保存缓存元素的有效期，使用一个线程循环查询 DelayQueue，一旦能从 DelayQueue 中获取元素时，表示缓存有效期到了。 定时任务调度。使用 DelayQueue 保存当天将会执行的任务和执行时间，一旦从 DelayQueue 中获取到任务就开始执行，从比如 TimerQueue 就是使用 DelayQueue 实现的。 compareTo() 接口实现示例队列中的 Delayed 必须实现 compareTo 来指定元素的顺序。比如让延时时间最长的放在队列的末尾。实现代码如下：123456789101112131415161718public int compareTo(Delayed other) &#123; if (other == this) // compare zero ONLY if same object return 0; if (other instanceof ScheduledFutureTask) &#123; ScheduledFutureTask x = (ScheduledFutureTask)other; long diff = time - x.time; if (diff &lt; 0) return -1; else if (diff &gt; 0) return 1; &#125;else if (sequenceNumber &lt; x.sequenceNumber)&#123; return -1; &#125;else&#123; return 1; &#125; long d = (getDelay(TimeUnit.NANOSECONDS) - other.getDelay(TimeUnit.NANOSECONDS)); return (d == 0) ? 0 : ((d &lt; 0) ? -1 : 1);&#125; getDelay() 接口实现示例getDelay() 可以查询当前元素还需要延时多久，需要在对象创建时传入一个开始时间。以 ScheduledThreadPoolExecutor 里 ScheduledFutureTask 类为例。这个类实现了 Delayed 接口。123456ScheduledFutureTask(Runnable r, V result, long ns, long period) &#123; super(r, result); this.time = ns; // 这个 time 就是开始时间 this.period = period; this.sequenceNumber = sequencer.getAndIncrement();&#125; 然后使用 getDelay 可以查询当前元素还需要延时多久，代码如下：123public long getDelay(TimeUnit unit) &#123; return unit.convert(time - now(), TimeUnit.NANOSECONDS);&#125; 最后是延时队列的使用，当消费者从队列里获取元素时，如果元素没有达到延时时间，就阻塞当前线程。12345long delay = first.getDelay(TimeUnit.NANOSECONDS);if (delay &lt;= 0) return q.poll();else if (leader != null) available.await(); SynchronousQueueSynchronousQueue 是一个不存储元素的阻塞队列。每一个 put 操作必须等待一个 take 操作，否则不能继续添加元素。SynchronousQueue 可以看成是一个传球手，负责把生产者线程处理的数据直接传递给消费者线程。队列本身并不存储任何元素，非常适合于传递性场景,比如在一个线程中使用的数据，传递给另外一个线程使用，SynchronousQueue 的吞吐量高于 LinkedBlockingQueue 和 ArrayBlockingQueue。 LinkedTransferQueueLinkedTransferQueue 是一个由链表结构组成的无界阻塞 TransferQueue 队列。TransferQueue 队列继承了 BlockingQueue，在 BlockingQueue 的基础上多了 tryTransfer 和 transfer 方法。transfer 会在元素进入阻塞队列前，先判断有没有消费者线程在等待获取，若有，则直接移交；否则将元素插入到队列尾部。tryTransfer 相比 transfer 少了插入的步骤，多了返回值 boolean，即若当前没有消费者线程空闲，则直接返回 false，不会插入到阻塞队列。 LinkedBlockingDequeLinkedBlockingDeque 是一个由链表结构组成的双向阻塞队列。所谓双向队列指的你可以从队列的两端插入和移出元素。双端队列因为多了一个操作队列的入口，在多线程同时入队时，也就减少了一半的竞争。相比其他的阻塞队列，LinkedBlockingDeque 多了 addFirst，addLast，offerFirst，offerLast，peekFirst，peekLast 等方法，以 First 单词结尾的方法，表示插入，获取（peek）或移除双端队列的第一个元素。以 Last 单词结尾的方法，表示插入，获取或移除双端队列的最后一个元素。另外插入方法 add 等同于 addLast，移除方法 remove 等效于 removeFirst。但是 take 方法却等同于 takeFirst，不知道是不是 Jdk 的 bug，使用时还是用带有 First 和 Last 后缀的方法更清楚。在初始化 LinkedBlockingDeque 时可以初始化队列的容量，用来防止其再扩容时过渡膨胀。另外双向阻塞队列可以运用在“工作窃取”模式中。 二、阻塞队列的实现原理阻塞队列的实现需要满足这样两个要求：如果队列为空，消费者会一直等待；如果队列为满，生产者会一直等待。因此，在实现阻塞队列的方式上，消费者和生产者的通信是必不可少的。JDK 使用 Condition 实现了线程间通信，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889final ReentrantLock lock;private final Condition notFull; // 非满条件，插入元素的时候需要满足这个条件private final Condition notEmpty; // 非空条件，取出元素的时候需要满足这个条件public ArrayBlockingQueue(int capacity, boolean fair) &#123; // 初始容量必须大于 0 if (capacity &lt;= 0) throw new IllegalArgumentException(); // 初始化数组 this.items = new Object[capacity]; // 初始化可重入锁 lock = new ReentrantLock(fair); // 初始化等待条件 notEmpty = lock.newCondition(); notFull = lock.newCondition();&#125;public void put(E e) throws InterruptedException &#123; checkNotNull(e); // 获取可重入锁 final ReentrantLock lock = this.lock; // 如果当前线程未被中断，则获取锁 lock.lockInterruptibly(); try &#123; while (count == items.length) // 判断元素是否已满 // 若满，则等待 notFull.await(); // 入队列 enqueue(e); &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125;public E take() throws InterruptedException &#123; // 可重入锁 final ReentrantLock lock = this.lock; // 如果当前线程未被中断，则获取锁，中断会抛出异常 lock.lockInterruptibly(); try &#123; while (count == 0) // 元素数量为 0，即 Object 数组为空 // 则等待 notEmpty 条件 notEmpty.await(); // 出队列 return dequeue(); &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125;private void enqueue(E x) &#123; // assert lock.getHoldCount() == 1; // assert items[putIndex] == null; // 获取数组 final Object[] items = this.items; // 将元素放入 items[putIndex] = x; if (++putIndex == items.length) // 放入后存元素的索引等于数组长度（表示已满） // 重置存索引为 0 putIndex = 0; // 元素数量加 1 count++; // 唤醒在 notEmpty 条件上等待的线程 notEmpty.signal();&#125;private E dequeue() &#123; // assert lock.getHoldCount() == 1; // assert items[takeIndex] != null; final Object[] items = this.items; @SuppressWarnings(\"unchecked\") // 取元素 E x = (E) items[takeIndex]; // 该索引的值赋值为 null items[takeIndex] = null; // 取值索引等于数组长度 if (++takeIndex == items.length) // 重新赋值取值索引 takeIndex = 0; // 元素个数减 1 count--; if (itrs != null) itrs.elementDequeued(); // 唤醒在 notFull 条件上等待的线程 notFull.signal(); return x;&#125; 当我们往队列里插入一个元素时或者取出一个元素时，如果队列不可用，则会调用 Condition 的 await() 方法来阻塞当前线程，该方法主要通过是 LockSupport.park(this);来实现的12345678910111213141516171819202122public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 加入等待队列 Node node = addConditionWaiter(); // 释放同步状态（锁） int savedState = fullyRelease(node); int interruptMode = 0; // 判断节点是否在同步队列中 while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); // 核心部分，阻塞（和 Object.wait(0) 一样进入无限等待状态） if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 退出 while 循环说明节点被 signal() 调入同步队列中，调用 acquireQueued() 加入同步状态竞争，竞争到锁后从 await() 方法返回 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 函数的核心部分是第 8 行的 LockSupport.park() 函数，该函数表示阻塞当前线程，它的源码如下：123456public static void park(Object blocker) &#123; Thread t = Thread.currentThread(); setBlocker(t, blocker); unsafe.park(false, 0L); // 核心部分 setBlocker(t, null);&#125; 函数的核心部分是第 4 行 unsafe.park() 函数，这是一个 native 方法，park 这个方法会阻塞当前线程，只有以下四种情况中的一种发生时，该方法才会返回。 与 park 对应的 unpark 执行或已经执行时。注意：已经执行是指 unpark 先执行，然后再执行的 park。 线程被中断时。 如果参数中的 time 不是零，等待了指定的毫秒数时。 发生异常现象时。这些异常事先无法确定。 JVM 中 park 在不同的操作系统使用不同的方式实现，在 linux 下是使用的是系统方法 pthread_cond_wait 实现。12345678910111213141516171819202122232425262728293031void os::PlatformEvent::park() &#123; int v ; for (;;) &#123; v = _Event ; if (Atomic::cmpxchg (v-1, &amp;_Event, v) == v) break ; &#125; guarantee (v &gt;= 0, \"invariant\") ; if (v == 0) &#123; // Do this the hard way by blocking ... int status = pthread_mutex_lock(_mutex); assert_status(status == 0, status, \"mutex_lock\"); guarantee (_nParked == 0, \"invariant\") ; ++ _nParked ; while (_Event &lt; 0) &#123; status = pthread_cond_wait(_cond, _mutex); // 核心部分 // for some reason, under 2.7 lwp_cond_wait() may return ETIME ... // Treat this the same as if the wait was interrupted if (status == ETIME) &#123; status = EINTR; &#125; assert_status(status == 0 || status == EINTR, status, \"cond_wait\"); &#125; -- _nParked ; // In theory we could move the ST of 0 into _Event past the unlock(), // but then we'd need a MEMBAR after the ST. _Event = 0 ; status = pthread_mutex_unlock(_mutex); assert_status(status == 0, status, \"mutex_unlock\"); &#125; guarantee (_Event &gt;= 0, \"invariant\") ; &#125; &#125; pthread_cond_wait 是一个多线程的条件变量函数，cond 是 condition 的缩写，字面意思可以理解为线程在等待一个条件发生，这个条件是一个全局变量。这个方法接收两个参数，一个共享变量_cond，一个互斥量_mutex。而 unpark 方法在 linux 下是使用 pthread_cond_signal 实现的。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"CAS 原理和缺陷","slug":"CAS 原理和缺陷","date":"2018-09-03T12:21:30.000Z","updated":"2019-03-08T14:00:58.684Z","comments":true,"path":"2018/09/03/CAS 原理和缺陷/","link":"","permalink":"http://yoursite.com/2018/09/03/CAS 原理和缺陷/","excerpt":"","text":"JDK1.6 以后 JVM 对 synchronize 锁机制作了不少优化，加入了偏向锁和自旋锁，在锁的底层实现中或多或少的都借助了 CAS 操作，其实 Java 中 java.util.concurrent 包的实现也是差不多建立在 CAS 之上，可见 CAS 在 Java 同步领域的重要性。 CAS 是 Compare and Swap 的简写形式，可翻译为：比较并交换。用于在硬件层面上提供原子性操作。其实现方式是基于硬件平台的汇编指令，就是说 CAS 是靠硬件实现的，JVM 只是封装了汇编调用。比较是否和给定的数值一致，如果一致则修改，不一致则不修改。 CAS 案例分析AtomicInteger 的原子特性就是 CAS 机制的典型使用场景。 其相关的源码片段如下：123456789101112131415161718private volatile int value; public final int get() &#123; return value; &#125; public final int incrementAndGet() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; &#125; &#125; public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125; AtomicInteger 在没有锁的机制下借助 volatile 原语，保证了线程间的数据是可见的（共享的）。其 get() 方法可以获取最新的内存中的值。 在 incrementAndGet() 的操作中，使用了 CAS 操作，每次从内存中读取最新的数据然后将此数据+1，最终写入内存时，先比较内存中最新的值，同累加之前读出来的值是否一致，不一致则写失败，循环重试直到成功为止。 compareAndSet 的具体实现调用了 unsafe 类的 compareAndSwapInt 方法，它其实是一个 Java Native Interface（简称 JNI）java 本地方法，会根据不同的 JDK 环境调用不同平台的对应 C 实现，下面以 windows 操作系统，X86 处理器的实现为例，这个本地方法在 openjdk 中依次调用的 c++代码为：unsafe.cpp，atomic.cpp 和 atomic_windows_x86.inline.hpp，它的实现代码存在于：openjdk7\\hotspot\\src\\os_cpu\\windows_x86\\vm\\atomic_windows_x86.inline.hpp，下面是相关的代码片段：1234567891011121314151617181920// Adding a lock prefix to an instruction on MP machine // VC++ doesn't like the lock prefix to be on a single line // so we can't insert a label after the lock prefix. // By emitting a lock prefix, we can define a label after it. #define LOCK_IF_MP(mp) __asm cmp mp, 0 \\ __asm je L0 \\ __asm _emit 0xF0 \\ __asm L0: inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) &#123; // alternative for InterlockedCompareExchange int mp = os::is_MP(); __asm &#123; mov edx, dest mov ecx, exchange_value mov eax, compare_value LOCK_IF_MP(mp) cmpxchg dword ptr [edx], ecx &#125; &#125; 由上面源代码可见在该平台的处理器上 CAS 通过指令 cmpxchg（就是 x86 的比较并交换指令）实现，并且程序会根据当前处理器是否是多处理器 (is_MP) 来决定是否为 cmpxchg 指令添加 lock 前缀 (LOCK_IF_MP)，如果是单核处理器则省略 lock 前缀 (单处理器自身会维护单处理器内的顺序一致性，不需要 lock 前缀提供的内存屏障效果 (而在 JDK9 中，已经忽略了这种判断都会直接添加 lock 前缀，这或许是因为现代单核处理器几乎已经消亡)。关于 Lock 前缀指令： Lock 前缀指令可以通过对总线或者处理器内部缓存加锁，使得其他处理器无法读写该指令要访问的内存区域，因此能保存指令执行的原子性。 Lock 前缀指令将禁止该指令与之前和之后的读和写指令重排序。 Lock 前缀指令将会把写缓冲区中的所有数据立即刷新到主内存中。 上面的第 1 点保证了 CAS 操作是一个原子操作，第 2 点和第 3 点所具有的内存屏障效果，保证了 CAS 同时具有 volatile 读和 volatile 写的内存语义（不过一般还是认为 CAS 只具有原子性而不具有可见性，因为底层的处理器平台可能不同）。 关于总线锁定和缓存锁定 1、早期的处理器只支持通过总线锁保证原子性。所谓总线锁就是使用处理器提供的一个 LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。很显然，这会带来昂贵的开销。2、缓存锁定是改进后的方案。在同一时刻我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把 CPU 和内存之间通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，最近的处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。缓存锁定是指当两个 CPU 的缓存行同时指向一片内存区域时，如果 A CPU 希望对该内存区域进行修改并使用了缓存锁定，那么 B CPU 将无法访问自己缓存中相应的缓存行，自然也没法访问对应的内存区域，这样就 A CPU 就实现了独享内存。 但是有两种情况下处理器不会使用缓存锁定。第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行（cache line），则处理器会调用总线锁定。第二种情况是：有些处理器不支持缓存锁定。对于 Inter486 和奔腾处理器，就算锁定的内存区域在处理器的缓存行中也会调用总线锁定。 关于同样使用 Lock 前缀的 volatile 却无法保证原子性 volatile 和 cas 都是基于 lock 前缀实现，但 volatile 却无法保证原子性这是因为：Lock 前缀只能保证缓存一致性，但不能保证寄存器中数据的一致性，如果指令在 lock 的缓存刷新生效之前把数据写入了寄存器，那么寄存器中的数据不会因此失效而是继续被使用，就好像数据库中的事务执行失败却没有回滚，原子性就被破坏了。以被 volatile 修饰的 i 作 i++为例，实际上分为 4 个步骤：mov 0xc(%r10),%r8d ; 把 i 的值赋给寄存器inc %r8d ; 寄存器的值+1mov %r8d,0xc(%r10) ; 把寄存器的值写回lock addl $0x0,(%rsp) ; 内存屏障，禁止指令重排序，并同步所有缓存 如果两个线程 AB 同时把 i 读进自己的寄存器，此时 B 线程等待，A 线程继续工作，把 i++后放回内存。按照原子性的性质，此时 B 应该回滚，重新从内存中读取 i，但因为此时 i 已经拷贝到寄存器中，所以 B 线程会继续运行，原子性被破坏。 而 cas 没有这个问题，因为 cas 操作对应指令只有一个：lock cmpxchg dword ptr [edx], ecx ; 该指令确保了直接从内存拿数据（ptr [edx]），然后放回内存这一系列操作都在 lock 状态下，所以是原子性的。 总结：volatile 之所以不是原子性的原因是 jvm 对 volatile 语义的实现只是在 volatile 写后面加一个内存屏障，而内存屏障前的操作不在 lock 状态下，这些操作可能会把数据放入寄存器从而导致无法有效同步；cas 能保证原子性是因为 cas 指令只有一个，这个指令从头到尾都是在 lock 状态下而且从内存到内存，所以它是原子性的。 CAS 缺陷1、ABA 问题。因为 CAS 需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是 A，变成了 B，又变成了 A，那么使用 CAS 进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA 问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么 A－B－A 就会变成 1A - 2B－3A。 从 Java1.5 开始 JDK 的 atomic 包里提供了一个类 AtomicStampedReference 来解决 ABA 问题。这个类的 compareAndSet 方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2、循环时间长开销大。自旋 CAS 如果长时间不成功，会给 CPU 带来非常大的执行开销。如果 JVM 能支持处理器提供的 pause 指令那么效率会有一定的提升，pause 指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline），使 CPU 不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起 CPU 流水线被清空（CPU pipeline flush），从而提高 CPU 的执行效率。 3、只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量 i＝2，j=a，合并一下 ij=2a，然后用 CAS 来操作 ij。从 Java1.5 开始 JDK 提供了 AtomicReference 类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作。 4、总线风暴带来的本地延迟。在多处理架构中，所有处理器会共享一条总线，靠此总线连接主存，每个处理器核心都有自己的高速缓存，各核相对于 BUS 对称分布，这种结构称为“对称多处理器”即 SMP。当主存中的数据同时存在于多个处理器高速缓存的时候，某一个处理器的高速缓存中相应的数据更新之后，会通过总线使其它处理器的高速缓存中相应的数据失效，从而使其重新通过总线从主存中加载最新的数据，大家通过总线的来回通信称为“Cache 一致性流量”，因为总线被设计为固定的“通信能力”，如果 Cache 一致性流量过大，总线将成为瓶颈。而 CAS 恰好会导致 Cache 一致性流量，如果有很多线程都共享同一个对象，当某个核心 CAS 成功时必然会引起总线风暴，这就是所谓的本地延迟。而偏向锁就是为了消除 CAS，降低 Cache 一致性流量。 关于偏向锁如何消除 CAS 试想这样一种情况：线程 A：申请锁 - 执行临界区代码 - 释放锁 - 申请锁 - 执行临界区代码 - 释放锁。锁的申请和释放都会执行 CAS，一共执行 4 次 CAS。而在偏向锁中，线程 A：申请锁 - 执行临界区代码 - 比较对象头 - 执行临界区代码。只执行了 1 次 CAS。 关于总线风暴 其实也不是所有的 CAS 都会导致总线风暴，这跟 Cache 一致性协议有关，具体参考：http://blogs.oracle.com/dave/entry/biased_locking_in_hotspot另外与 SMP 对应还有非对称多处理器架构，现在主要应用在一些高端处理器上，主要特点是没有总线，没有公用主存，每个 Core 有自己的内存。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"《高性能 MySQL》读书笔记——查询优化","slug":"《高性能 MySQL》读书笔记——查询优化","date":"2018-08-28T10:34:21.000Z","updated":"2019-03-08T13:57:05.156Z","comments":true,"path":"2018/08/28/《高性能 MySQL》读书笔记——查询优化/","link":"","permalink":"http://yoursite.com/2018/08/28/《高性能 MySQL》读书笔记——查询优化/","excerpt":"","text":"在设计了最优的库表结构、如何建立最好的索引，这些对于高性能来说必不可少。但这些还不够，还需要合理地设计查询。 一、查询执行的基础当 MySQL 执行一个查询语句时，会经历以下几个步骤： 客户端发送一条查询给服务器。 服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。 服务器端进行 SQL 解析、预处理，再由优化器生成对应的执行计划。 MySQL 根据优化器生成的执行计划，调用存储引擎的 API 来执行查询。 将结果返回给客户端。 二、MySQL 查询优化器MySQL 查询优化器会做大量的工作，这些工作包括但不限于： 1. 重新定义关联表的顺序MySQL 使用一种叫 “嵌套循环关联” 的方式来执行关联查询。顾名思义，这是一种嵌套式的查询。在正常情况下，最左边的表会嵌套在最外层，然后根据表中的每一行数据去遍历内层表，找到所有符合条件的行。如果外层表行数为 m，内层表行数为 n，则总共要遍历 m*n 行数据。 但如果内层表使用了索引，而关联字段恰好就被索引覆盖的话，就只需要几次查询就可以定位内层表的数据行。总行数从 m*n 变为 m*i(i 一般小于 3)。这无疑大大加快了关联查询的速度。MySQL 查询优化器会调整关联表查询的顺序来尽可能使用多的索引查询。 2. 将外连接转化成内连接并不是所有的 OUTER JOIN 语句都必须以外连接的方式执行。诸多原因，例如 WHERE 条件、库表结构都可能让一个外连接等价于一个内连接。MySQL 能够识别这点并重写查询，让其可以调整关联顺序（外连接分左右，所以无法调整顺序）。 3. 使用等价变换规则MySQL 可以合并和减少一些比较，例如（5=5 AND a&gt;5）将被改写成（a&gt;5）。 4. 优化 COUNT()、MIN() 和 MAX()索引和是否可为空可以帮助 MySQL 优化这类表达式。例如要找某一列的最小值，只需查询对应 B-tree 索引最左端的记录，而最大值只需查询对应 B-tree 索引最右端。另外，没有任何 WHERE 条件的 COUNT(*) 查询在 MyISAM 中也是 O(1)，因为 MyISAM 维护了一个变量来存放数据表的行数（不过 innodb 没有，innodb 中执行 COUNT(*) 会做全表查询）。 5. 预估并转化为常数表达式当 MySQL 检测到一个表达式或者一个子查询可以转化为常数的时候，就会一直把该表达式作为常数进行优化处理。 6. 覆盖索引扫描当索引中的列覆盖所有查询中需要使用的列时，MySQL 将直接使用索引返回所需数据而不做回表查询。 7. 提前终止查询在发现已经满足查询要求的时候，MySQL 总是能够立刻终止查询。比如 LIMIT。 8. 子查询优化MySQL 5.6 中处理子查询的思路是，基于查询重写技术的规则，尽可能将子查询转换为连接，并配合基于代价估算的 Materialize、exists 等优化策略让子查询执行更优。 9. 等值传播如果两个列的值通过等式关联，那么 MySQL 能够把其中一个列的 WHERE 语句条件传递到另一列上。例如： SELECT film.film_id FROM film INNER JOIN film_actor USING(film_id) WHERE film.film_id &gt; 500; 优化器会把它优化为： SELECT film.film_id FROM film INNER JOIN film_actor USING(film_id) WHERE film.film_id &gt; 500 AND film_actor.film_id &gt; 500; 10. 列表 IN() 的比较MySQL 会将 IN() 列表中的数据先进行排序，然后通过二分查找的形式来确定取出的值是否在列表中。 11. 索引合并当 WHERE 子句中包含多个复杂条件涉及到多个索引时，MySQL 会先根据不同索引取出多组数据，再将这些数据合并。 三、MySQL 查询优化器的局限1. 关联子查询上一节有提到 MySQL 查询优化器会把 IN 子查询变为 EXISTS 子查询的形式，大部分情况下这种优化会带来性能提升，但某些情况下，会让查询更慢。比如： SELECT * FROM film WHERE film_id IN(SELECT film_id FROM film_actor WHERE actor_id = 1); 假设 film 和 film_actor 表在 film_id 上都有索引，那么这条语句会走 film_actor 和 film 的索引，速度非常快。但查询优化器会把它 “优化” 为： SELECT * FROM film WHERE EXISTS(SELECT 1 FROM film_actor WHERE actor_id = 1 AND film.film_id = actor.film_id); 此时 MySQL 只会走 film_actor 的索引而会对 film 做全表扫描，效率大大下降。 解决的方法就是使用左外连接改造： SELECT film.* FROM film LEFT OUTER JOIN film_actor USING(film_id) WHERE film_actor.actor_id = 1; PS：5.6 及以后版本的 MySQL 对关联子查询做了大量优化，现在的思路是，基于查询重写技术的规则，尽可能将子查询转换为连接，并配合基于代价估算的 Materialize、exists 等优化策略让子查询执行更优。因此 5.6 以后将不存在这个问题。 2.UNION 的限制MySQL 无法将限制条件从外层 “下推” 到内层，其中一个典型就是 UNION： (SELECT first_name FROM actor) UNION ALL (SELECT first_name FROM customer) LIMIT 20; MySQL 会把 actor 和 customer 中的所有记录放在同一张临时表中，然后从临时表中取出前 20 条。可以通过把 LIMIT 放入内部来解决这个问题： (SELECT first_name FROM actor LIMIT 20) UNION ALL (SELECT first_name FROM customer LIMIT 20) LIMIT 20; 这样临时表的规模就缩小到 40 了。 3. 最大值和最小值优化MySQL 优化器会对不加条件的 MAX 和 MIN 做优化，但并没有对加条件的这两个函数做优化。比如： SELECT MIN(actor_id) FROM actor WHERE first_name = “PENELOPE”; actor_id 是主键，严格按照大小排序，那么其实在找到第一个满足条件的记录就可以返回了。而 MySQL 会继续遍历整张表。修改的方式是使用 LIMIT： SELECT actor_id FROM actor WHERE first_name = “PENELOPE” LIMIT 1; 此时会触发提前终止机制，返回最小的 actor_id。 四、优化特定类型的查询1. 优化 COUNT() 查询很多时候一些业务场景并不要求完全精确的 COUNT 值，可以使用 EXPLAIN SELECT * FROM film; 得到的近似值来代替。 innodb 下，如果表中单行数据量很大，且没有二级索引的话，可以对表上较短的且不为空的字段加索引，再执行 count(*），此时优化器会自动选择走二级索引，由于二级索引是短字段，单页存储的数据行数就多，减少了取页的次数，查询时间也就更短了。 2. 优化关联查询确保 ON 或者 USING 子句中的列上有索引，这样只需要全表扫描第一个表，第二个表可以走索引。 确保任何的 GROUP BY 和 ORDER BY 中的表达式只涉及到一个表中的列，这样 MySQL 才可能使用索引来优化这个过程。 3. 优化 LIMIT 分页LIMIT 分页在系统偏移量非常大的时候效果会很差。比如 LIMIT 1000,20 这样的查询，这时 MySQL 需要查询 10020 条记录然后只返回最后 20 条。 优化此类分页查询的一个最简单的办法就是尽可能利用索引覆盖扫描。考虑下面的查询： SELECT film_id, description FROM film ORDER BY title LIMIT 50,5; 这个语句可以被改写成： SELECT film.film_id,film,description FROM film INNER JOIN(SELECT film_id FROM film ORDER BY title LIMIT 50,5)AS lim USING(film_id); 这里的 “延迟关联” 操作将大大提升查询效率，它会利用覆盖索引直接定位到分页所需数据所在的位置，而不需要从头遍历。 记录上一次取数据的位置也是一个不错的主意。比如在频繁使用 “下一页” 这个功能的时候，记录下上次取数据最后的位置，然后可以把查询语句写成： SELECT film_id, description FROM film WHERE film_id&gt;16030 ORDER BY film_id LIMIT 5;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"《高性能 MySQL》读书笔记——索引优化","slug":"《高性能 MySQL》读书笔记——索引优化","date":"2018-08-25T09:34:28.000Z","updated":"2019-03-08T14:00:09.624Z","comments":true,"path":"2018/08/25/《高性能 MySQL》读书笔记——索引优化/","link":"","permalink":"http://yoursite.com/2018/08/25/《高性能 MySQL》读书笔记——索引优化/","excerpt":"","text":"索引是数据库得以高效的关键，以最常见的 B+Tree 索引为例，至少有以下三个优点：1）索引大大减少了服务器需要扫描的数据量；2）索引可以帮助服务器避免排序和临时表；3）索引可以将随机 I/O 变为顺序 I/O。因此，如何使用索引也成了高效 MySQL 重要的一部分。 一、MySQL 中的索引1、B-Tree 索引B-Tree 和其变体 B+Tree 是绝大部分数据库引擎默认使用的数据结构，我们在谈到索引时若无特殊指代一般是指 B+Tree（B-Tree 和 B+Tree 的区别及选择这种数据结构的原因可以看这里 ） 优点B-Tree 索引适用于全键值、键值范围或键前缀查找。其中键前缀查找只适用于根据最左前缀的查找。以下表为例 family_name first_name birthday 张 三 1960-1-1 李 四 1962-3-2 王 五 1966-4-5 依次在 family_name、first_name 和 birthday 上建立 B-Tree 索引（暂不考虑主键的影响），则上述的索引对如下的索引有效： 全值匹配：全值匹配指的是和索引中的所有列进行匹配，如查找 family_name = 张，first_name = 三，birthday = 1960-1-1 的人。 匹配最左前缀：查找所有 family_name= 张 的人。 匹配列前缀：查找所有 family_name = 张，first_name = 三，birthday = 196X 的人。列前缀的语法有很多，比如 where birthday(3) = 196 或者 where birthday like “196%” 都是合法的匹配列前缀的语法，但 birthday like “%96%” 不是合法的列前缀匹配，不会使用索引查找。 匹配范围值：查找所有 family_name = 张，first_name = 三，birthday &gt; 1960-1-1 and birthday &lt; 1969-12-31 的人。 只访问索引的查询：若查询的内容在索引中便可全部找到，则无需回表查询，可以节省大量磁盘 IO，这种索引有个专有名词叫“覆盖索引”。 缺点B-Tree 索引的功能强大，但也有局限，仍以上述的索引为例： 如果不是按照索引的最左列开始查找，则无法使用索引。例如我们无法用索引查找 first_name = 四 的人。 不能跳过索引中的列。例如我们无法用索引查找 family_name = 张，birthday = 1960-1-1 的人。 查询中可以使用范围查询，但只能使用一次，且它右边所有的列都无法使用索引优化查找。如查找 family_name = 张，first_name &gt; 三 and first_name &lt; 五（字典序），birthday = 1960-1-1 的人，只会用到姓和名两个索引列，无法使用第三个索引列。（这个涉及到联合索引底层的数据结构，如下图） B-Tree 在建立联合索引的时候只会对第一个字段建立 B-Tree 索引，其它字段会在对应的叶子节点的 data 域上按给定字段的顺序作为优先级排序后储存。如上图，对 id、family_name、first_name 三个列建立索引。则底层存储时会先按 id 构造 B-Tree，再在 B-Tree 的叶子节点上按 family_name、first_name 的优先级排序后存储对应的地址。对于叶子节点上数据的查找，会采用二分查找的方式。而一旦确定了前一个字段使用范围查找后，得到的一组数据对于后一个字段而言是无序的，无法继续使用二分查找，只能遍历，此时索引失效。除了范围查询，整个 B-Tree 的最左匹配原则的原因也是和这个数据存储的方式息息相关的，理解了这个数据结构也就理解了 B-Tree 的最左匹配原则。 2、哈希索引哈希索引基于哈希表实现，使用链表法解决哈希冲突，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码，哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 在 MySQL 中，只有 Memory 引擎显式支持哈希索引，这也是 Memory 引擎表的默认索引类型。 优点 索引的结构十分紧凑，查找的速度非常快。 缺点 哈希索引只包含哈希值和指针，而不存储字段值，所以不能使用覆盖查询。 哈希索引数据并不是按照索引值顺序存储的，所以也就无法用于排序。 哈希索引也不支持部分索引列匹配查找，因为哈希值的计算是使用索引列的全部内容计算的。 哈希索引只支持等值比较，不支持任何范围查询 哈希索引的缺点也决定了哈希索引只适用于某些特定的场合，但一旦适合哈希索引，则它带来的性能提升将非常显著。 3、全文索引在标准的 MySQL 中只有 MyISAM 引擎支持全文索引，同时 innodb 也开始实验性质地支持全文索引。 MyISAM 的全文索引作用对象是一个“全文集合”，这可能是某个数据表的一列，也可能使多个列。具体的，对数据表的某一条记录，MySQL 会将需要索引的列全部拼接成一个字符串，然后进行索引。 MyISAM 的全文索引是一类特殊的 B-Tree 索引，共有两层。第一层是所有关键字，然后对于每一个关键字的第二层，包含的是一组相关的“文档指针”。 二、innodb 中的索引1、聚簇索引聚簇索引不是一种单独的索引类型，而是一种数据存储方式。innodb 的聚簇索引是在 B-tree 的叶子节点上存放了数据行。 innodb 所有表中的数据都会以这种形式保存在磁盘，这也意味着 innodb 每张表中至少要有一个主键。如果没有显式地定义主键，innodb 会选择一个唯一的非空索引代替；如果没有这样的索引，innodb 会隐式定义一个主键作为聚簇索引。 聚簇索引中，每个叶子节点称为一个数据页，相邻的数据页之间有双向指针相连，范围查找可以直接按顺序读出，速度非常快。 2、非聚簇索引（辅助索引）innodb 中每张表有且仅有一个聚簇索引，剩下的都是非聚簇索引。对于非聚簇索引，叶子节点并不会包含行记录的全部数据，而是保存指向聚簇索引中某一条记录的指针。比如 user 表中使用 user_id 作为主键，那么在它的非聚簇索引的叶子节点中，保存的就是 user_id。对于一次使用了非聚簇索引的查找，数据库引擎会先在非聚簇索引上找到 user_id，再根据 user_id 在聚簇索引上找到对应的数据行，这也就是 innodb 中的二次查询。 3、自适应哈希索引自适应哈希索引是 innodb 上的一种优化措施。InnoDB 存储引擎会监控对表上各索引页的查询。如果观察到建立哈希索引可以带来速度提升，则建立哈希索引，称之为自适应哈希索引 (Adaptive Hash Index, AHI)。AHI 是通过缓冲池的 B+树页构造而来，因此建立的速度很快，而且不需要对整张表构建哈希索引。InnoDB 存储引擎会自动根据访问的频率和模式来自动地为某些热点页建立哈希索引。 AHI 有一个要求，对这个页的连续访问模式必须是一样的。例如对于 (a,b) 这样的联合索引页，其访问模式可以是下面情况： where a=xxx where a =xxx and b=xxx 访问模式一样是指查询的条件是一样的，若交替进行上述两种查询，那么 InnoDB 存储引擎不会对该页构造 AHI。AHI 还有下面几个要求： 以该模式连续访问了 100 次 以该模式连续访问了 页中记录总数/16 次 必须同时满足上述所有要求才会建立 AHI。 InnoDB 存储引擎官方文档显示，启用 AHI 后,读取和写入速度可以提高 2 倍，辅助索引的连接操作性能可以提高 5 倍。 三、常见索引失效场景1、查询条件包含 or SELECT * FROM order WHERE order_id = 1 OR pay_method=’123’; 当 or 左右查询字段只有一个是索引，该索引失效；只有当 or 左右查询字段均为索引时，才会生效。 2、索引列上有计算、函数等操作 SELECT * FROM order WHERE order_id +1 = 2; 3、使用负向查询（!=、&lt;&gt;、not in、not exists、not like 等） SELECT * FROM order WHERE order_id &lt;&gt; 2; 4、5.7 之前的 is null 和 is not null SELECT * FROM order WHERE order_id is not null; 5.7 之后 is null 和 is not null 也会走索引，但对于使用了声明了 NOT NULL 的索引行不会。 5、不符合最左前缀原则的组合索引当查询涉及到联合索引时，查询的条件必须是联合索引的一个前缀。比如对于联合索引 A/B/C/D，查询的条件可以是 A，也可以是 A/B/C，但不能是 B/C。另外对于范围查询，只能有一个条件是范围查询且必须是最后一个。比如查询 A/B/C，只有 C 可以是范围查询。另外在 MySQL 中，IN 被定义为范围查询，但却是当作多个条件等于来处理，因此 IN 语句放在中间，也会走索引。 6、like 以通配符开头 SELECT * FROM order WHERE pay_method LIKE ‘%23’; 7、字符串不加单引号 SELECT * FROM order WHERE pay_method = 123; 8、当全表扫描速度比索引速度快时，mysql 会使用全表扫描，此时索引失效一个有意思的例子是 IN 的索引失效。MySQL 优化器对开销代价的估算方法有两种：index dive 和 index statistics。前者统计速度慢但是能得到精准的值，后者统计速度快但是数据未必精准。老版本的 MySQL 默认使用 index dive 这种统计方式，但在 IN() 组合条件过多的时候会发生很多问题。查询优化可能需要花很多时间，并消耗大量内存。因此新版本 MySQL 在组合数超过一定的数量（eq_range_index_dive_limit）就会使用 index statistics 统计。而 index statistics 统计的结果不精确，因此可能会出现 IN 不走索引的情况。此时可以尝试通过增加 eq_range_index_dive_limit 的值（5.6 中默认是 10，5.7 中默认是 200）让 IN 语句走索引。 四、高性能索引策略1、使用前缀索引在对一个比较长的字符串建立索引的时候，把字符串所有字符放入索引是比较低效的做法。前文对字符串做哈希是一种方式。也可以使用字符串的前缀做索引。比如 CREATE INDEX index_name ON table_name (column_name(10)); 表示将列的前 10 个字符做索引，这样做的好处是减少索引字段的大小，可以在一页内放入更多的数据，从而降低 B-tree 的高度，同时更短的索引字段带来更短的匹配时间，提高了查找效率。 2、使用覆盖索引覆盖索引是一种索引包含了查询所需所有数据的情况，在这种情况下，MySQL 可以使用索引来直接获取列的数据，这样就不需要再读取数据行。覆盖索引是非常有用的工具，能够极大地提升性能： 索引条目远小于数据行大小，所以如果只需要读取索引，MySQL 就会极大地减少数据访问量。这对缓存的负载非常重要，因为这种情况下响应时间大部分花费在数据拷贝上。 因为索引是按值顺序存储的（至少在单个页内如此），对于 I/O 密集型的范围查询会比随机从磁盘读取每一行数据的 I/O 要少得多。 对于 innodb 的聚簇索引，覆盖索引特别有用。innodb 的二级索引在叶子节点中保存了行的主键值，所以如果二级主键能够覆盖索引，则可以避免对主键索引的二次查询。 3、延迟关联覆盖索引可以极大地提升查找的效率，但很多时候我们会遇到 select * 这样的需求，这时使用覆盖索引就不可能了。不过我们可以使用延迟关联的方式利用覆盖索引。 比如对于语句： select from t_portal_user where create_time &gt; ‘2012-10:10’ and create_time&lt;’2017:10:10’ LIMIT 5000,10; 可以改写成： SELECT from t_portal_user INNER JOIN (select id from t_portal_user where create_time &gt; ‘2012-10:10’ and create_time&lt;’2017:10:10’ LIMIT 5000,10) as a USING(id); 对于子查询： select id from t_portal_user where create_time &gt; ‘2012-10:10’ and create_time&lt;’2017:10:10’ LIMIT 5000,10; 如果在 create_time 上做了索引（innodb 中主键会被默认添加进索引中），则可以利用覆盖索引找到符合条件的 id，再根据 id 做普通查询。 4、利用索引来做排序MySQL 支持二种方式的排序，文件排序和索引，后者效率高，它指 MySQL 扫描索引本身完成排序。文件排序方式效率较低。ORDER BY 满足以下情况，会使用 Index 方式排序: 使用覆盖索引，即通过扫描索引本身就可完成排序 ORDER BY 语句 或者 WHERE , JOIN 子句和 ORDER BY 语句组成的条件组合满足最左前缀（一个例外是 IN，IN 在没有排序的最左匹配中被视为等值查询，对排序来说是一种范围查询） ORDER BY 语句中条件的排序顺序是一样（都为正序或者都为倒序） 5、自定义哈希索引如果存储引擎不支持哈希索引，则可以在 B-tree 基础上创建一个伪哈希索引。这和真正的哈希索引不是一回事，因为还是使用 B-tree 进行查找，但它使用的是哈希值而不是键本身进行查找。如 SELECT id FROM user WHERE address = “zhejiang ningbo”; 若删除原来 URL 列上的索引，而新增一个被索引的 address_crc 列，使用 CRC32 做哈希，就可以使用下面的方式查询： SELECT id FROM user WHERE address=”zhejiang ningbo” AND url_crc=CRC32(“zhejiang ningbo”); 这样做的性能会非常高，因为 MySQL 优化器会使用这个选择性很高而体积很小的基于 url_crc 列的索引来完成查找。即使有多个记录有相同的索引值，查找仍然很快，只需要根据哈希值做快速的整数比较就能找到索引条目，然后一一比较返回对应的行。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"《高性能 MySQL》读书笔记——库表结构优化","slug":"《高性能 MySQL》读书笔记——库表结构优化","date":"2018-08-18T10:08:36.000Z","updated":"2019-03-08T13:58:07.624Z","comments":true,"path":"2018/08/18/《高性能 MySQL》读书笔记——库表结构优化/","link":"","permalink":"http://yoursite.com/2018/08/18/《高性能 MySQL》读书笔记——库表结构优化/","excerpt":"","text":"本文介绍了 MySQL 中的常用数据类型及其适用场景，以及数据库范式和反范式化的设计。 1、MySQL 常用数据类型MySQL 常用数据类型分为：整数、实数、字符串、日期和时间、位数据几种。 整数类型整数类型可以分为：TINYINT，SMALLINT，MEDIUMINT，INT，BIGINT，分别使用：8，16，24，32，64 位的存储空间。 整数类型有可选的 UNSIGNED 属性，表示不允许负值，可以把正数的上限提高一倍。 整数可以指定宽度，比如 INT(11)，这表示在一些交互工具中 INT 会显示 11 个数字，对于存储和计算来说没有意义，INT(1) 和 INT(20) 都是 32 位。 实数类型实数类型可以是带有小数部分的数字，也可以不是。比如可以使用 DECIMAL 存储比 BIGINT 还大的整数。 实数类型分为：FLOAT，DOUBLE 和 DECIMAL，其中 FLOAT 和 DOUBLE 用于存储不精确的小数类型，分别使用 32 和 64 位的存储空间；DECIMAL 用于存储精确的小数类型，存储空间的大小根据小数的长度决定。5.0 版本以后 DECIMAL 最多允许 65 个数字。 尽量使用 FLOAT 和 DOUBLE 存储小数类型。只有需要精确计算的场合才使用 DECIMAL，但在数据量比较大的时候，可以考虑使用 BIGINT 代替 DECIMAL，将需要存储的数字乘以相应的倍数即可。 字符串类型MySQL 中的字符串类型有 CHAR，VARCHAR，TEXT 和 BLOB 四种，其中 TEXT 和 BLOB 两种类型比较特别，MySQL 把每个 BLOB 和 TEXT 值当作一个独立的对象处理，存储引擎在处理时也通常会做相应的处理。比如当 TEXT 或 BLOB 的值较大时，innodb 会使用专门的“外部”存储区域来进行存储，此时每个值在行内需要 1~4 个字节存储一个指针。 TEXT 和 BLOB 之间仅有的不同是 BLOB 类型存储的是二进制数据，没有排序规则或字符集，而 TEXT 类型有字符集和排序规则。 MySQL 对 BLOB 和 TEXT 列进行排序与其他类型是不同的：它只对每个列的最前 max_sort_length 字节而不是整个字符串做排序。 VARCHAR 是变长字符串，而 CHAR 是定长字符串。VARCHAR 比 CHAR 更省空间，因为它仅使用必要的空间。VARCHAR 需要使用 1 或 2 个额外字节记录字符串的长度：如果列的最大长度小于或等于 255 字节，则只使用 1 个字节表示，否则使用 2 个。 VARCHAR 节省了存储空间，但由于行是变长的，在 UPDATE 时可能使行变得比原来更长，这就导致需要做额外的工作。如果一个行占用空间增长，超出页的大小，innodb 会使用裂页来使行可以放进页内。对于过长的 VARCHAR，innodb 会将其存储为 BLOB。 CHAR 值在存储时，MySQL 会删除所有的末尾空格。 使用枚举（ENUM）代替字符串类型如果预计字符串可取的值范围确定且数量不大，可以使用枚举的方式替代字符串。比如存储水果，预计种类只有苹果、香蕉、梨。可以把水果种类定义为 ENUM(“apple”,”banana”,”pear”)，实际存储时 MySQL 只会在列表中保存数字，并在.frm 文件中保存一个“数字-字符串”的映射关系，可以大大减少存储的空间。 日期和时间类型MySQL 能存储的最小时间粒度为秒，但也可以通过使用 BIGINT 类型存储微秒级别的时间戳等方式绕开这一限制。MySQL 中存储时间的数据类型有两种：DATETIME 和 TIMESTAMP。两种类型的区别如下： DATETIME 占用 8 个字节 允许为空值，可以自定义值，系统不会自动修改其值。 实际格式储存，格式为 YYYYMMDDHHMMSS 的整数 与时区无关 不可以设定默认值，所以在不允许为空值的情况下，必须手动指定 datetime 字段的值才可以成功插入数据。 可以在指定 datetime 字段的值的时候使用 now() 变量来自动插入系统的当前时间。 结论：datetime 类型适合用来记录数据的原始的创建时间，因为无论你怎么更改记录中其他字段的值，datetime 字段的值都不会改变，除非你手动更改它。 TIMESTAMP 占用 4 个字节，默认为 NOT NULL TIMESTAMP 值不能早于 1970 或晚于 2037。这说明一个日期，例如 ‘1968-01-01’，虽然对于 DATETIME 或 DATE 值是有效的，但对于 TIMESTAMP 值却无效，如果分配给这样一个对象将被转换为 0。 值以 UTC 格式保存，为从 1970 年 1 月 1 日（格林尼治时间）午夜以来的秒数。 时区转化 ，存储时对当前的时区进行转换，检索时再转换回当前的时区。 默认情况下，如果插入或更新时没有指定第一个 TIMESTAMP 的值，MySQL 会设置这个列的值为当前时间。 结论：timestamp 类型适合用来记录数据的最后修改时间，因为只要你更改了记录中其他字段的值，timestamp 字段的值都会被自动更新。 位数据类型BIT 和 SET 是 MySQL 中典型的位数据类型，位数据的本质是一个二进制字符串，使用位数据类型可以在一列中存储多个”true/false”值。 2、范式和反范式数据库中的范式满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多规范要求的称为第二范式（2NF），其余范式以次类推。一般说来，数据库只需满足第三范式 (3NF）就行了。 范式的包含关系。一个数据库设计如果符合第二范式，一定也符合第一范式。如果符合第三范式，一定也符合第二范式。 1NF：属性不可分 2NF：属性完全依赖于主键 [消除部分子函数依赖 ] 3NF：属性不依赖于其它非主属性 [消除传递依赖 ] 第一范式 (1NF)符合 1NF 的关系中的每个属性都不可再分。反例： 第二范式 (2NF)2NF 在 1NF 的基础之上，消除了非主属性对于码（主键）的部分函数依赖 可以通过分解来满足。 分解前 学号 姓名 系名 系主任 课名 分数 1022211101 李小明 经济系 王强 高等数学 95 1022211101 李小明 经济系 王强 大学英语 87 1022211101 李小明 经济系 王强 普通化学 76 1022211102 张莉莉 经济系 王强 高等数学 72 1022211102 张莉莉 经济系 王强 大学英语 98 1022211102 张莉莉 经济系 王强 计算机基础 88 1022511101 高芳芳 法律系 刘玲 高等数学 82 1022511101 高芳芳 法律系 刘玲 法律基础 82 以上学生课程关系中，{学号, 课名} 为键码（主键），有如下函数依赖： （学号，课名） -&gt; 分数 学号 -&gt; 姓名 学号 -&gt; 系名 -&gt; 系主任 分数完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。 姓名、系名和系主任都部分依赖于键码，我们需要把部分依赖变成完全依赖。 分解后 关系-1 学号 姓名 系名 系主任 1022211101 李小明 经济系 王强 1022211102 张莉莉 经济系 王强 1022211101 高芳芳 法律系 刘玲 有以下函数依赖： Sno -&gt; Sname, Sdept Sdept -&gt; Mname 关系-2 学号 课名 分数 1022211101 高等数学 95 1022211101 大学英语 87 1022211101 普通化学 76 1022211102 高等数学 72 1022211102 大学英语 98 1022211102 计算机基础 88 1022511101 高等数学 82 1022511101 法学基础 82 有以下函数依赖： Sno, Cname -&gt; Grade 第三范式 (3NF)3NF 在 2NF 的基础之上，消除了非主属性对于码（主键）的传递函数依赖 上面的 关系-1 中存在以下传递函数依赖： Sno -&gt; Sdept -&gt; Mname 可以进行以下分解： 关系-11 学号 姓名 系名 1022211101 李小明 经济系 1022211102 张莉莉 经济系 1022211101 高芳芳 法律系 关系-12 系名 系主任 经济系 王强 法律系 刘玲 范式的优缺点优点： 当数据较好地范式化时，就只有很少或者没有重复数据，所以更新时只需要修改更少的数据。 范式化的表通常更小，可以更好地放在内存里，所以执行操作很更快。 很少有多余的数据意味着检索列表数据时更少需要 DISTINCT 或者 GROUP BY 语句。比如前面的例子：在非范式化的结构中必须使用 DISTINCT 或者 GROUP BY 才能获得唯一的一张系名列表，但如果使用范式，只需要单独查询系名-系主任表就可以了。 缺点： 范式化的设计通常需要关联。稍微复杂一点的查询语句在符合范式的 schema 上都有可能需要至少一次关联，这不但代价昂贵，也可能使一些索引无效。例如，范式化可能将列存放在不同的表中，而这些列如果在一个表中本可以属于同一个索引。 常见反范式化设计范式化不一定适合所有场合，很多时候，一些冗余数据有助于我们提升性能。下面列举几个常见的反范式化操作。 缓存表缓存表可以存储那些可以从其他表获取但每次获取的速度比较慢的数据。比如有时可能会需要很多不同的索引组合来加速各种类型的查询。这些矛盾的需求有时需要创建一张只包含主表中部分列的缓存表。有时候我们可能需要不同存储引擎提供的不同特性。例如，如果主表使用 innodb，用 MyISAM 作为缓存表的引擎将会得到更小的索引空间，并且可以做全文索引。 汇总表汇总表保存的是 GROUP BY 语句聚合数据的表。相比缓存表，汇总表的数据不是逻辑上冗余的，但可以通过其它表计算得到。例如，计算某网站之前 24 小时内发送的消息数。我们可以通过 COUNT() 得到，但这样需要检索全表。作为替代方案，可以每小时生成一张汇总表。这样也许一条简单的查询就可以做到，并且比实时维护计数器要高效得多。缺点是计数并不是 100% 精确。 某网站之前 24 小时内发送的消息数的汇总表： CREATE TABLE msg_per_hr (hr DATETIME NOT NULL,cnt INT UNSIGNED NOT NULL,PRIMARY KEY(hr)); 计数器表计数器在应用中很常见。比如网站的点击数，文件下载次数等。 如果其它数据保存在一起，很可能碰到并发问题。创建一张独立的表是个比较好的办法，这样可使计数器表小且快。而且使用独立的表可以帮助避免查询缓存失效。 下面是一张简单的计数器表，只有一行数据，记录网站的点击次数： mysql&gt; CREATE TABLE hit_counter( -&gt; cnt int unsigned not null -&gt; ) ENGINE=InnoDB; 网站的每次点击都会导致对计数器进行更新： mysql&gt; UPDATE hit_counter SET cnt = cnt + 1; 问题在于，对于任何想要更新这一行的事务来说，这条记录上都有一个全局的互斥锁。这会使得这些事务只能串行进行。要获得更高的并发性，可以将计数器保存在多个行中，每次随机选择一行更新： mysql&gt; CREATE TABLE hit_counter( -&gt; slot tinyint unsigned not null primary key, -&gt; cnt int unsigned not null -&gt; ) ENGINE=InnoDB; 然后预先在这张表增加 100 行数据。现在选择一个随机的槽进行更新： mysql&gt; UPDATE hit_counter SET cnt = cnt + 1 WHERE slot = RAND() * 100; 要获得统计结果，需要使用下面这样的聚合查询： mysql&gt; CREATE TABLE daily_hit_counter( -&gt; day date not null, -&gt; slot tinyint unsigned not null, -&gt; cnt int unsigned not null, -&gt; primary key(day, slot) -&gt; ) ENGINE=InnoDB; 一个常见的需求是每隔一段时间开始一个新的计算器（例如，每天一个）。再作进一步修改： mysql&gt; CREATE TABLE daily_hit_counter( -&gt; day date not null, -&gt; slot tinyint unsigned not null, -&gt; cnt int unsigned not null, -&gt; primary key(day, slot) -&gt; ) ENGINE=InnoDB; 在这个场景中可以不用预告生成行 ，而用 ON DUPLICATE KEY UPDATE（对唯一索引或主键字段的值会检查是否已存在，存在则更新，不存在则插入）代替： mysql&gt; INSERT INTO daily_hit_counter(day, slot, cnt) -&gt; VALUES(CURRENT_DATE, RAND()*100, 1) -&gt; ON DUPLICATE KEY UPDATE cnt = cnt + 1; 如果希望减少表的行数，可以写一个周期执行的任务，合并所有结果到 0 号槽，并且删除所有其他的槽： UPDATE daily_hit_counter as c -&gt; INNER JOIN ( -&gt; SELECT day, SUM(cnt) AS cnt, MIN(slot) AS mslot -&gt; FROM daily_hit_counter -&gt; GROUP BY day -&gt; ) AS X USING(day) -&gt; SET c.cnt = IF(c.slot = x.mslot, x.cnt, 0), -&gt; c.slot = IF(c.slot = x.mslot, 0, c.slot);mysql&gt; DELETE FROM daily_hit_counter WHERE slot &lt;&gt; 0 AND cnt = 0;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"GET 与 POST 的区别","slug":"GET 与 POST 的区别","date":"2018-05-15T06:55:30.000Z","updated":"2019-03-08T14:04:53.741Z","comments":true,"path":"2018/05/15/GET 与 POST 的区别/","link":"","permalink":"http://yoursite.com/2018/05/15/GET 与 POST 的区别/","excerpt":"","text":"HTTP 定义了一组请求方法, 以表明要对给定资源执行的操作。而硕果仅存的只剩两个半（笑）。实际开发中我们常用到的一般是 POST 和 GET，极少数情况下会用到 PUT。RFC7231 规范了 GET 方法用于请求一个指定资源的表示形式（transfer a current representation of the target resource），而 POST 方法用于将实体提交到指定的资源（Perform resource-specific processing on the request payload）。但仅仅了解规范是不够的，很多工具比如 chrome,nginx 有它自己履行规范的方式，从开发角度看，或许这些更有价值。 参数GET 传递的参数只能带 URL 后面，文本格式 QueryString，长度受限于浏览器发送长度和服务器接收长度。各家标准不一，作为开发人员宜选取其中最短一个（2083 字节）作为开发标准，以避免不必要的兼容问题。 IE(Browser) 2,083 Bytes Firefox(Browser) 65,536 Bytes Safari(Browser) 80,000 Bytes Opera(Browser) 190,000 Bytes Google (chrome) 8,182 Bytes Apache (Server) 8,182 Bytes Microsoft Internet Information Server(IIS) 16,384 Bytes POST 的参数就比较灵活，可以传递 application/x-www-form-urlencoded 的类似 QueryString、multipart/form-data 的二进制报文格式（支持文件信息嵌入报文传输）、纯文本或二进制的 body 参数。很多时候我们把参数写在 body 里，这时参数没有长度上的限制。 幂等幂等是一个计算机术语，表示重复执行某一操作得到的结果总是相同的。在 HTTP 中，如果我们说一个 HTTP 方法是幂等的，指的是同样的请求被执行一次与连续执行多次的效果是一样的，服务器的状态也是一样的。 GET 是幂等的，我们使用 GET 获取服务器上同一份数据，拿到的数据应该都是相同的。每次请求后服务器的状态应该也是相同的（统计数据除外）。 POST 不是幂等的，多次 POST 返回的结果不一定相同，每次请求后服务器的状态也是不一样的。 缓存GET 时默认可以复用前面的请求数据作为缓存结果返回，此时以完整的 URL 作为缓存数据的 KEY。所以有时候为了强制每次请求都是新数据，我们可以在 URL 后面加上一个随机参数或时间戳或版本号来避免从缓存中读取，也可以直接设置 Cache-Control 禁用缓存。 POST 则一般不会被这些缓存因素影响。 安全性服务器的日志比如像 nginx 的 access log 会自动记录 GET 和 POST 的 URL，包括其中带的参数，但不会记录请求的报文。对于一些敏感数据，POST 更安全一些。 自动化性能测试基于上面提到的 nginx 日志，可以使用 grep GET+日期，awk 格式化，然后 sort -u 去重，从而提取到某天的所有 GET 请求 URL，使用程序模拟登陆，然后请求所有 URL 即可获取简单的性能测试数据，每个请求是否正确，响应时间多少等等。 但是对于 POST 请求，因为不知道报文，无法这样简单处理。可以通过 nginx-lua 获取报文输出到 log，这样格式化会麻烦很多，但不失为一个办法。 TCP 包的数量GET 请求稳定只发送一个包，而 POST 请求在某些浏览器里会发送两个，具体的原因还在探究。 IE 6 – 2 packets IE 7 – 2 packets IE 8 – 2 packets Firefox 3.0.13 – 1 packet Firefox 3.5.2 – 1 packet Opera 9.27 – 2 packets Safari 4.0.3 – 2 packets Chrome 2.0.172.43 – 2 packets 参考资料1、URL 最大长度问题 2、xmlhttprequest-xhr-uses-multiple-packets-for-http-post3、comparing-get-and-post","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/tags/计算机网络/"}],"keywords":[]},{"title":"idea 常用快捷键备忘","slug":"idea 常用快捷键备忘","date":"2018-05-08T07:00:32.000Z","updated":"2019-03-08T14:06:14.836Z","comments":true,"path":"2018/05/08/idea 常用快捷键备忘/","link":"","permalink":"http://yoursite.com/2018/05/08/idea 常用快捷键备忘/","excerpt":"","text":"记录一些开发中常用但不容易记住的快捷键。 CtrlCtrl + Ｙ 删除行Ctrl + Ｒ 替换Ctrl + Ｆ 当前代码中查找Ctrl + Ｗ 选中光标所在的单词，连按范围扩大Ctrl + －／＝ 折叠／展开当前光标所在代码和注释Ctrl + Ｉ 接口方法补全 Ctrl + AltCtrl + Alt + Ｌ 代码格式化Ctrl + Alt + Ｔ 选中的地方代码环绕提示Ctrl + Alt + 空格 类名或接口名提示Ctrl + Alt + 方向左／右 回退／前进到上一个操作的地方Ctrl + Alt + Ｂ 查看一个方法的实现Ctrl + Alt + Ｏ 清除多余包 Ctrl + ShiftCtrl + Shift + Ｕ 大／小写转换Ctrl + Shift + －／＝ 折叠／展开当前文件下所有代码和注释 AltAlt + 回车 智能提示Alt + 方向上／下 上／下一方法Alt + Insert 类方法补全 Shift双击 Shift 在项目的所有目录查找特定内容Shift ＋ F6 重命名类，方法，变量","categories":[],"tags":[{"name":"备忘录","slug":"备忘录","permalink":"http://yoursite.com/tags/备忘录/"}],"keywords":[]},{"title":"Java9 新特性概述","slug":"Java9 新特性概述","date":"2018-04-11T02:51:48.000Z","updated":"2019-03-08T14:07:49.031Z","comments":true,"path":"2018/04/11/Java9 新特性概述/","link":"","permalink":"http://yoursite.com/2018/04/11/Java9 新特性概述/","excerpt":"","text":"Java9 正式发布于 2017 年 9 月 21 日。作为 Java8 之后 3 年半才发布的新版本，Java9 带来了很多重大的变化。其中最重要的改动是 Java 平台模块系统的引入。除此之外，还有一些新的特性。本文对 Java9 中包含的新特性做了概括性的介绍，可以帮助你快速了解 Java9。 Java 平台模块系统Java 平台模块系统，也就是 Project Jigsaw，把模块化开发实践引入到了 Java 平台中。在引入了模块系统之后，JDK 被重新组织成 94 个模块。Java 应用可以通过新增的 jlink 工具，创建出只包含所依赖的 JDK 模块的自定义运行时镜像。这样可以极大的减少 Java 运行时环境的大小。这对于目前流行的不可变基础设施的实践来说，镜像的大小的减少可以节省很多存储空间和带宽资源。 模块化开发的实践在软件开发领域并不是一个新的概念。Java 开发社区已经使用这样的模块化实践有相当长的一段时间。主流的构建工具，包括 Apache Maven 和 Gradle 都支持把一个大的项目划分成若干个子项目。子项目之间通过不同的依赖关系组织在一起。每个子项目在构建之后都会产生对应的 JAR 文件。在 Java9 中，已有的这些项目可以很容易的升级转换为 Java9 模块，并保持原有的组织结构不变。 Java9 模块的重要特征是在其工件（artifact）的根目录中包含了一个描述模块的 module-info.class 文件。工件的格式可以是传统的 JAR 文件或是 Java9 新增的 JMOD 文件。这个文件由根目录中的源代码文件 module-info.java 编译而来。该模块声明文件可以描述模块的不同特征。模块声明文件中可以包含的内容如下： 模块导出的包：使用exports可以声明模块对其他模块所导出的包。包中的 public 和 protected 类型，以及这些类型的 public 和 protected 成员可以被其他模块所访问。没有声明为导出的包相当于模块中的私有成员，不能被其他模块使用。 模块的依赖关系：使用requires可以声明模块对其他模块的依赖关系。使用requires transitive可以把一个模块依赖声明为传递的。传递的模块依赖可以被依赖当前模块的其他模块所读取。如果一个模块所导出的类型的型构中包含了来自它所依赖的模块的类型，那么对该模块的依赖应该声明为传递的。 服务的提供和使用：如果一个模块中包含了可以被 ServiceLocator 发现的服务接口的实现，需要使用provides with语句来声明具体的实现类；如果一个模块需要使用服务接口，可以使用uses语句来声明。 代码清单 1 中给出了一个模块声明文件的示例。在该声明文件中，模块 com.mycompany.sample 导出了 Java 包 com.mycompany.sample。该模块依赖于模块 com.mycompany.sample。该模块也提供了服务接口 com.mycompany.common.DemoService 的实现类 com.mycompany.sample.DemoServiceImpl。 清单 1.模块声明示例：123456module com.mycompany.sample &#123; exports com.mycompany.sample; requires com.mycompany.common; provides com.mycompany.common.DemoService with com.mycompany.sample.DemoServiceImpl; &#125; 模块系统中增加了模块路径的概念。模块系统在解析模块时，会从模块路径中进行查找。为了保持与之前 Java 版本的兼容性，CLASSPATH 依然被保留。所有的类型在运行时都属于某个特定的模块。对于从 CLASSPATH 中加载的类型，它们属于加载它们的类加载器对应的未命名模块。可以通过 Class 的 getModule() 方法来获取到表示其所在模块的 Module 对象。 在 JVM 启动时，会从应用的根模块开始，根据依赖关系递归的进行解析，直到得到一个表示依赖关系的图。如果解析过程中出现找不到模块的情况，或是在模块路径的同一个地方找到了名称相同的模块，模块解析过程会终止，JVM 也会退出。Java 也提供了相应的 API 与模块系统进行交互。 Jshelljshell 是 Java9 新增的一个实用工具。jshell 为 Java 增加了类似 NodeJS 和 Python 中的读取-求值-打印循环（ Read-Evaluation-Print Loop ）。在 jshell 中可以直接输入表达式并查看其执行结果。当需要测试一个方法的运行效果，或是快速的对表达式进行求值时，jshell 都非常实用。只需要通过 jshell 命令启动 jshell，然后直接输入表达式即可。每个表达式的结果会被自动保存下来，以数字编号作为引用，类似$1 和$2 这样的名称。可以在后续的表达式中引用之前语句的运行结果。在 jshell 中，除了表达式之外，还可以创建 Java 类和方法。jshell 也有基本的代码完成功能。 在代码清单 2 中，我们直接创建了一个方法 add。 清单 2.在 jshell 中添加方法：1234jshell&gt; int add(int x, int y) &#123; ...&gt; return x + y; ...&gt; &#125; | created method add(int,int) 接着就可以在 jshell 中直接使用这个方法，如代码清单 3 所示。 清单 3.在 jshell 中使用创建的方法：12jshell&gt; add(1, 2) $19 ==&gt; 3 集合、Stream 和 Optional在集合上，Java9 增加了 List.of()、Set.of()、Map.of() 和 Map.ofEntries() 等工厂方法来创建不可变集合，如代码清单 4 所示。 清单 4.创建不可变集合：12345678List.of(); List.of(\"Hello\", \"World\"); List.of(1, 2, 3);Set.of(); Set.of(\"Hello\", \"World\"); Set.of(1, 2, 3);Map.of();Map.of(\"Hello\", 1, \"World\", 2); Stream 中增加了新的方法 ofNullable、dropWhile、takeWhile 和 iterate。在代码清单 5 中，流中包含了从 1 到 5 的元素。断言检查元素是否为奇数。第一个元素 1 被删除，结果流中包含 4 个元素。 清单 5.Stream 中的 dropWhile 方法示例：1234567@Test public void testDropWhile() throws Exception &#123; final long count = Stream.of(1, 2, 3, 4, 5) .dropWhile(i -&gt; i % 2 != 0) .count(); assertEquals(4, count); &#125; Collectors 中增加了新的方法 filtering 和 flatMapping。在代码清单 6 中，对于输入的 String 流，先通过 flatMapping 把 String 映射成 Integer 流，再把所有的 Integer 收集到一个集合中。 清单 6.Collectors 的 flatMapping 方法示例：1234567@Test public void testFlatMapping() throws Exception &#123; final Set&lt;Integer&gt; result = Stream.of(\"a\", \"ab\", \"abc\") .collect(Collectors.flatMapping(v -&gt; v.chars().boxed(), Collectors.toSet())); assertEquals(3, result.size()); &#125; Optional 类中新增了 ifPresentOrElse、or 和 stream 等方法。在代码清单 7 中，Optional 流中包含 3 个元素，其中只有 2 个有值。在使用 flatMap 之后，结果流中包含了 2 个值。 清单 7.Optional 的 stream 方法示例：12345678910@Test public void testStream() throws Exception &#123; final long count = Stream.of( Optional.of(1), Optional.empty(), Optional.of(2) ).flatMap(Optional::stream) .count(); assertEquals(2, count); &#125; 进程 APIJava9 增加了 ProcessHandle 接口，可以对原生进程进行管理，尤其适合于管理长时间运行的进程。在使用 P rocessBuilder 来启动一个进程之后，可以通过 Process.toHandle() 方法来得到一个 ProcessHandle 对象的实例。通过 ProcessHandle 可以获取到由 ProcessHandle.Info 表示的进程的基本信息，如命令行参数、可执行文件路径和启动时间等。ProcessHandle 的 onExit() 方法返回一个 CompletableFuture 对象，可以在进程结束时执行自定义的动作。代码清单 8 中给出了进程 API 的使用示例。 清单 8.进程 API 示例：123456789final ProcessBuilder processBuilder = new ProcessBuilder(\"top\").inheritIO(); final ProcessHandle processHandle = processBuilder.start().toHandle(); processHandle.onExit().whenCompleteAsync((handle, throwable) -&gt; &#123; if (throwable == null) &#123; System.out.println(handle.pid()); &#125; else &#123; throwable.printStackTrace(); &#125; &#125;); 平台日志 API 和服务Java9 允许为 JDK 和应用配置同样的日志实现。新增的 System.LoggerFinder 用来管理 JDK 使用的日志记录器实现。JVM 在运行时只有一个系统范围的 LoggerFinder 实例。LoggerFinder 通过服务查找机制来加载日志记录器实现。默认情况下，JDK 使用 java.logging 模块中的 java.util.logging 实现。通过 LoggerFinder 的 getLogger() 方法就可以获取到表示日志记录器的 System.Logger 实现。应用同样可以使用 System.Logger 来记录日志。这样就保证了 JDK 和应用使用同样的日志实现。我们也可以通过添加自己的 System.LoggerFinder 实现来让 JDK 和应用使用 SLF4J 等其他日志记录框架。代码清单 9 中给出了平台日志 API 的使用示例。 清单 9.使用平台日志 API：123456public class Main &#123; private static final System.Logger LOGGER = System.getLogger(\"Main\"); public static void main(final String[] args) &#123; LOGGER.log(Level.INFO, \"Run!\"); &#125; &#125; 反应式流（Reactive Streams）反应式编程的思想最近得到了广泛的流行。在 Java 平台上有流行的反应式库 RxJava 和 Reactor。反应式流规范的出发点是提供一个带非阻塞负压（non-blocking backpressure）的异步流处理规范。反应式流规范的核心接口已经添加到了 Java9 中的 java.util.concurrent.Flow 类中。 Flow 中包含了 Flow.Publisher、Flow.Subscriber、Flow.Subscription 和 Flow.Processor 等 4 个核心接口。Java9 还提供了 SubmissionPublisher 作为 Flow.Publisher 的一个实现。RxJava2 和 Reactor 都可以很方便的与 Flow 类的核心接口进行互操作。 变量句柄变量句柄是一个变量或一组变量的引用，包括静态域，非静态域，数组元素和堆外数据结构中的组成部分等。变量句柄的含义类似于已有的方法句柄。变量句柄由 Java 类 java.lang.invoke.VarHandle 来表示。可以使用类 java.lang.invoke.MethodHandles.Lookup 中的静态工厂方法来创建 VarHandle 对象。通过变量句柄，可以在变量上进行各种操作。这些操作称为访问模式。不同的访问模式尤其在内存排序上的不同语义。目前一共有 31 种访问模式，而每种访问模式都在 VarHandle 中有对应的方法。这些方法可以对变量进行读取、写入、原子更新、数值原子更新和比特位原子操作等。VarHandle 还可以用来访问数组中的单个元素，以及把 byte[] 数组和 ByteBuffer 当成是不同原始类型的数组来访问。 在代码清单 10 中，我们创建了访问 HandleTarget 类中的域 count 的变量句柄，并在其上进行读取操作。 清单 10.变量句柄使用示例： 123456789101112131415161718192021public class HandleTarget &#123; public int count = 1; &#125; public class VarHandleTest &#123; private HandleTarget handleTarget = new HandleTarget(); private VarHandle varHandle; @Before public void setUp() throws Exception &#123; this.handleTarget = new HandleTarget(); this.varHandle = MethodHandles .lookup() .findVarHandle(HandleTarget.class, \"count\", int.class); &#125; @Test public void testGet() throws Exception &#123; assertEquals(1, this.varHandle.get(this.handleTarget)); assertEquals(1, this.varHandle.getVolatile(this.handleTarget)); assertEquals(1, this.varHandle.getOpaque(this.handleTarget)); assertEquals(1, this.varHandle.getAcquire(this.handleTarget)); &#125; &#125; 改进方法句柄（Method Handle）类 java.lang.invoke.MethodHandles 增加了更多的静态方法来创建不同类型的方法句柄。 arrayConstructor：创建指定类型的数组。 arrayLength：获取指定类型的数组的大小。 varHandleInvoker 和 varHandleExactInvoker：调用 VarHandle 中的访问模式方法。 zero：返回一个类型的默认值。 empty：返回 MethodType 的返回值类型的默认值。 loop、countedLoop、iteratedLoop、whileLoop 和 doWhileLoop：创建不同类型的循环，包括 for 循环、while 循环和 do-while 循环。 tryFinally：把对方法句柄的调用封装在 try-finally 语句中。在代码清单 11 中，我们使用 iteratedLoop 来创建一个遍历 String 类型迭代器的方法句柄，并计算所有字符串的长度的总和。 清单 11. 循环方法句柄使用示例：123456789101112131415161718192021222324public class IteratedLoopTest &#123; static int body(final int sum, final String value) &#123; return sum + value.length(); &#125; @Test public void testIteratedLoop() throws Throwable &#123; final MethodHandle iterator = MethodHandles.constant( Iterator.class, List.of(\"a\", \"bc\", \"def\").iterator()); final MethodHandle init = MethodHandles.zero(int.class); final MethodHandle body = MethodHandles .lookup() .findStatic( IteratedLoopTest.class, \"body\", MethodType.methodType( int.class, int.class, String.class)); final MethodHandle iteratedLoop = MethodHandles .iteratedLoop(iterator, init, body); assertEquals(6, iteratedLoop.invoke()); &#125; &#125; 并发在并发方面，类 CompletableFuture 中增加了几个新的方法。completeAsync 使用一个异步任务来获取结果并完成该 CompletableFuture。orTimeout 在 CompletableFuture 没有在给定的超时时间之前完成，使用 TimeoutException 异常来完成 CompletableFuture。completeOnTimeout 与 orTimeout 类似，只不过它在超时时使用给定的值来完成 CompletableFuture。新的 Thread.onSpinWait 方法在当前线程需要使用忙循环来等待时，可以提高等待的效率。 NashornNashorn 是 Java8 中引入的新的 JavaScript 引擎。Java9 中的 Nashorn 已经实现了一些 ECMAScript6 规范中的新特性，包括模板字符串、二进制和八进制字面量、迭代器和 for..of 循环和箭头函数等。Nashorn 还提供了 API 把 ECMAScript 源代码解析成抽象语法树（Abstract Syntax Tree，AST），可以用来对 ECMAScript 源代码进行分析。 I/O 流新特性类 java.io.InputStream 中增加了新的方法来读取和复制 InputStream 中包含的数据。 readAllBytes：读取 InputStream 中的所有剩余字节。readNBytes：从 InputStream 中读取指定数量的字节到数组中。transferTo：读取 InputStream 中的全部字节并写入到指定的 OutputStream 中。代码清单 12 中给出了这些新方法的使用示例。 清单 12.InputStream 中的新方法使用示例：1234567891011121314151617181920212223242526public class TestInputStream &#123; private InputStream inputStream; private static final String CONTENT = \"Hello World\"; @Before public void setUp() throws Exception &#123; this.inputStream = TestInputStream.class.getResourceAsStream(\"/input.txt\"); &#125; @Test public void testReadAllBytes() throws Exception &#123; final String content = new String(this.inputStream.readAllBytes()); assertEquals(CONTENT, content); &#125; @Test public void testReadNBytes() throws Exception &#123; final byte[] data = new byte[5]; this.inputStream.readNBytes(data, 0, 5); assertEquals(\"Hello\", new String(data)); &#125; @Test public void testTransferTo() throws Exception &#123; final ByteArrayOutputStream outputStream = new ByteArrayOutputStream(); this.inputStream.transferTo(outputStream); assertEquals(CONTENT, outputStream.toString()); &#125; &#125; ObjectInputFilter 可以对 ObjectInputStream 中包含的内容进行检查，来确保其中包含的数据是合法的。可以使用 ObjectInputStream 的方法 setObjectInputFilter 来设置。ObjectInputFilter 在进行检查时，可以检查如对象图的最大深度、对象引用的最大数量、输入流中的最大字节数和数组的最大长度等限制，也可以对包含的类的名称进行限制。 改进应用安全性能Java9 新增了 4 个 SHA-3 哈希算法，SHA3-224、SHA3-256、SHA3-384 和 SHA3-512。另外也增加了通过 java.security.SecureRandom 生成使用 DRBG 算法的强随机数。代码清单 13 中给出了 SHA-3 哈希算法的使用示例。 清单 13.SHA-3 哈希算法使用示例：import org.apache.commons.codec.binary.Hex;public class SHA3 { public static void main(final String[] args) throws NoSuchAlgorithmException { final MessageDigest instance = MessageDigest.getInstance(“SHA3-224”); final byte[] digest = instance.digest(“”.getBytes()); System.out.println(Hex.encodeHexString(digest)); }} 用户界面类 java.awt.Desktop 增加了新的与桌面进行互动的能力。可以使用 addAppEventListener 方法来添加不同应用事件的监听器，包括应用变为前台应用、应用隐藏或显示、屏幕和系统进入休眠与唤醒、以及用户会话的开始和终止等。还可以在显示关于窗口和配置窗口时，添加自定义的逻辑。在用户要求退出应用时，可以通过自定义处理器来接受或拒绝退出请求。在 AWT 图像支持方面，可以在应用中使用多分辨率图像。 统一 JVM 日志Java9 中，JVM 有了统一的日志记录系统，可以使用新的命令行选项-Xlog 来控制 JVM 上所有组件的日志记录。该日志记录系统可以设置输出的日志消息的标签、级别、修饰符和输出目标等。Java9 移除了在 Java8 中被废弃的垃圾回收器配置组合，同时把 G1 设为默认的垃圾回收器实现。另外，CMS 垃圾回收器已经被声明为废弃。Java9 也增加了很多可以通过 jcmd 调用的诊断命令。 其他改动方面在 Java 语言本身，Java9 允许在接口中使用私有方法。在 try-with-resources 语句中可以使用 effectively-final 变量。类 java.lang.StackWalker 可以对线程的堆栈进行遍历，并且支持过滤和延迟访问。Java9 把对 Unicode 的支持升级到了 8.0。ResourceBundle 加载属性文件的默认编码从 ISO-8859-1 改成了 UTF-8，不再需要使用 native2ascii 命令来对属性文件进行额外处理。注解@Deprecated 也得到了增强，增加了 since 和 forRemoval 两个属性，可以分别指定一个程序元素被废弃的版本，以及是否会在今后的版本中被删除。 在代码清单 14 中，buildMessage 是接口 SayHi 中的私有方法，在默认方法 sayHi 中被使用。 清单 14.接口中私有方法的示例：123456789public interface SayHi &#123; private String buildMessage() &#123; return \"Hello\"; &#125; void sayHi(final String message); default void sayHi() &#123; sayHi(buildMessage()); &#125; &#125; 小结作为 Java 平台最新的一个重大更新，Java9 中的很多新特性，尤其模块系统，对于 Java 应用的开发会产生深远的影响。本文对 Java9 中的新特性做了概括的介绍，可以作为了解 Java9 的基础。这些新特性的相关内容，可以通过官方文档来进一步的了解。 参考资源 (resources) 参考Java9 官方文档 ，了解 Java9 的更多内容。 参考Java9 官方 Java 文档 ，了解 Java API 的细节。 了解反应式流规范 的更多内容。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"收藏","slug":"收藏","permalink":"http://yoursite.com/tags/收藏/"}],"keywords":[]},{"title":"Centos7 下使用 systemd 管理服务","slug":"Centos7 下使用 systemd 管理服务","date":"2018-03-30T13:09:37.000Z","updated":"2019-03-08T14:03:07.867Z","comments":true,"path":"2018/03/30/Centos7 下使用 systemd 管理服务/","link":"","permalink":"http://yoursite.com/2018/03/30/Centos7 下使用 systemd 管理服务/","excerpt":"","text":"Centos7 新增了 systemd 用于为系统的启动和管理提供一套完整的解决方案，以替代原先的系统管理器 system V init（SysVInit）。相比于 SysVInit，systemd 支持服务并行启动，从而使效率大大提高；同时它还具有日志管理、快速备份与恢复、挂载点管理等多种实用功能，是一套更完善的系统管理方案。服务器后端会经常有将 mysql、redis、nginx 等部件开机启动的需求，现在可以统一交给 Systemd 管理，方便许多。 Systemd 概述在 Linux 系统中，我们经常会遇到结尾为 ‘d’ 的进程，比如 initd，mysqld。根据 Linux 惯例，字母‘d’是守护进程（daemon）的缩写。systemd 这个名字的含义，就是它是整个系统的守护进程。在 Centos7 中，它是系统的第一个进程（PID 等于 1），创建于系统启动的过程中，其他进程都是它的子进程。 在 Centos7 中，系统的启动可以汇整成如下几个过程： ①、打开计算机电源，载入 BIOS 的硬件信息与进行自我测试，并依据设置取得第一个可开机的设备；②、读取并执行第一个开机设备内 MBR 的 boot loader（亦即是 grub2，spfdisk 等程序）；③、依据 boot loader 的设置载入 Kernel，Kernel 会开始侦测硬件与载入驱动程序；④、在硬件驱动成功后，Kernel 会主动调用 systemd 程序，并以 default.target 流程开机； 1) systemd 执行 sysinit.target 初始化系统及 basic.target 准备操作系统； 2) systemd 启动 multi-user.target 下的本机与服务器服务； 3) systemd 执行 multi-user.target 下的/etc/rc.d/rc.local 文件； 4) systemd 执行 multi-user.target 下的 getty.target 及登录服务； 5) systemd 执行 graphical 需要的服务（图形化版本特有） 大概就是上面这样子了。你会发现 systemd 出现的频率很高，这是因为 systemd 负责了开机时所有的资源（unit）调度任务，操作系统只需要启动 systemd，剩下的 systemd 都会帮它处理。不仅如此，事实上，systemd 扮演的就是一个资源管理者的角色，它最主要的功能就是准备软件执行的环境，包括系统的主机名称、网络设置、语系处理、文件系统格式及其他服务的启动等。 Systemd 的基本概念和操作一、UnitSystemd 可以管理所有系统资源。不同的资源统称为 Unit（单元）。systemd 将资源归纳为以下一些不同的类型。然而，systemd 正在快速发展，新功能不断增加。所以资源类型可能在不久的将来继续增加。 Service unit：系统服务（最常见）Target unit：多个 Unit 构成的一个组Device Unit：硬件设备Mount Unit：文件系统的挂载点Automount Unit：自动挂载点Path Unit：文件或路径Scope Unit：不是由 Systemd 启动的外部进程Slice Unit：进程组Snapshot Unit：Systemd 快照，可以切回某个快照Socket Unit：进程间通信的 socketSwap Unit：swap 文件Timer Unit：定时器 Unit 配置文件每一个 unit 都有一个配置文件，配置文件一般存放在目录/usr/lib/systemd/system/，它会告诉 systemd 怎么启动这个 unit。配置文件的后缀名，就是该 unit 的种类，比如 sshd.socket。如果省略，systemd 默认后缀名为.service，所以 sshd 会被理解成 sshd.service。 上图为我系统中 redis.service 配置文件的内容。它大致包含了这些信息：1）对这个资源的描述；2）所需的前置资源；3）实际执行此 service 的指令或脚本程序；4）实际停止此 service 的指令或脚本程序；5）该资源所在的组（target）。 unit 配置文件中常用的字段整理如下： Unit 字段 参数意义说明 Description 就是当我们使用systemctl list-units时，会输出给管理员看的简易说明。使用systemctl status输出的此服务的说明，也是这个字段。 After 说明此 unit 是在哪个 daemon 启动之后才启动的意思。基本上仅是说明服务启动的顺序而已，并没有强制要求里头的服务一定要启动后此 unit 才能启动。 Before 与 After 的意义相反，是在什么服务启动前最好启动这个服务的意思。不过这仅是规范服务启动的顺序，并非强制要求的意思。 Requires 明确地定义此 unit 需要在哪个 daemon 启动后才能够启动。如果在此项设置的前导服务没有启动，那么此 unit 就不会被启动。 Wants 表示这个 unit 之后最好还要启动什么服务比较好的意思，不过并没有明确的规范。主要的目的是希望创建让使用者比较好操作的环境。因此，这个 Wants 后面接的服务如果没有启动，其实不会影响到这个 unit 本身。 Conflicts 代表冲突的服务。表示这个项目后面接的服务如果有启动，那么我们这个 unit 本身就不能启动！我们 unit 有启动，则此项目后的服务就不能启动！是一种冲突性的检查。 Service 字段（service 特有） 参数意义说明 Type Type 字段定义启动类型。它可以设置的值如下：simple（默认值）：ExecStart 字段启动的进程为主进程。forking：ExecStart 字段将以 fork() 方式启动，此时父进程将会退出，子进程将成为主进程。oneshot：类似于 simple，但只执行一次，systemd 会等它执行完，才启动其他服务。dbus：类似于 simple，但会等待 D-Bus 信号后启动。notify：类似于 simple，启动结束后会发出通知信号，然后 systemd 再启动其他服务。idle：类似于 simple，但是要等到其他任务都执行完，才会启动该服务。一种使用场合是为让该服务的输出，不与其他服务的输出相混合。 EnvironmentFile 可以指定启动脚本的环境配置文件！例如 sshd.service 的配置文件写入到 /etc/sysconfig/sshd 当中！你也可以使用 Environment=后面接多个不同的 Shell 变量来给予设置！ ExecStart 就是实际执行此 daemon 的指令或脚本程序。你也可以使用 ExecStartPre（之前）以及 ExecStartPost（之后）两个设置项目来在实际启动服务前，进行额外的指令行为。但是你得要特别注意的是，指令串仅接受“指令 参数 参数…”的格式，不能接受 &lt;,&gt;,&gt;&gt;,&amp;等特殊字符，很多的 bash 语法也不支持喔！所以，要使用这些特殊的字符时，最好直接写入到指令脚本里面去！不过，上述的语法也不是完全不能用，亦即，若要支持比较完整的 bash 语法，那你得要使用 Type=oneshot 才行喔！其他的 Type 才不能支持这些字符。 ExecStop 与systemctl stop的执行有关，关闭此服务时所进行的指令。 ExecReload 与systemctl reload有关的指令行为。 Restart 当设置 Restart=1 时，则当此 daemon 服务终止后，会再次的启动此服务。 RemainAfterExit 当设置为 RemainAfterExit=1 时，则当这个 daemon 所属的所有程序都终止之后，此服务会再尝试启动。这对于 Type=oneshot 的服务很有帮助！ TimeoutSec 若这个服务在启动或者是关闭时，因为某些缘故导致无法顺利“正常启动或正常结束”的情况下，则我们要等多久才进入“强制结束”的状态！ KillMode 可以是process,control-group,none的其中一种，如果是process则 daemon 终止时，只会终止主要的程序（ExecStart 接的后面那串指令），如果是control-group时，则由此 daemon 所产生的其他 control-group 的程序，也都会被关闭。如果是none的话，则没有程序会被关闭。 RestartSec 与 Restart 有点相关性，如果这个服务被关闭，然后需要重新启动时，大概要 sleep 多少时间再重新启动的意思。默认是 100ms（毫秒）。 Install 字段 参数意义说明 WantedBy 这个设置后面接的大部分是*.target unit！意思是，这个 unit 本身是附挂在哪一个 target unit 下面的！一般来说，大多的服务性质的 unit 都是附挂在 multi-user.target 下面！ Also 当目前这个 unit 本身被 enable 时，Also 后面接的 unit 也请 enable 的意思！也就是具有相依性的服务可以写在这里呢！ Alias 进行一个链接的别名的意思！当 systemctl enable 相关的服务时，则此服务会进行链接文件的创建/usr/lib/systemd/system/multi-user.target。 配置 unit 开机启动开机时，systemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。systemctl enable命令用于在上面两个目录之间，建立符号链接关系，相当于激活开机启动。与之对应的，systemctl disable命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。 启动和停止 unit执行systemctl start命令启动软件，执行systemctl status命令查看该服务的状态 上面的输出结果含义如下： Loaded 行：配置文件的位置，是否设为开机启动Drop-In 行：符号链接地址Active 行：表示正在运行Main PID 行：主进程 IDCGroup 块：应用的所有子进程 当不需要服务继续运行时，可以执行systemctl stop命令终止正在运行的服务。有时候，该命令可能没有响应，服务停不下来，这时候可以执行systemctl kill命令强制终止。另外，需要重启服务时可以执行systemctl restart命令。 二、Target启动计算机的时候，需要启动大量的 unit。如果每一次启动，都要一一写明本次启动需要哪些 unit，显然非常不方便。Systemd 的解决方案就是 target。 简单说，target 就是一个 unit 组，包含许多相关的 unit 。启动某个 target 的时候，systemd 就会启动里面所有的 unit。从这个意义上说，target 这个概念类似于”状态点”，启动某个 target 就好比启动到某种状态。 传统的 init 启动模式里面，有 runlevel 的概念，跟 target 的作用很类似。不同的是，runlevel 是互斥的，不可能多个 runlevel 同时启动，但是多个 target 可以同时启动。 查看 target我们可以执行指令systemctl list-unit-files --type=target查看当前系统的所有 target。也可以执行指令systemctl list-dependencies multi-user.target查看一个 target 包含的所有 unit。 系统启动时 systemctl 会根据/etc/systemd/system/default.target 规划的策略进行启动，我们可以通过执行指令systemctl get-default查看启动时默认的 target（一般是 multi-user.target）。指令systemctl set-default可以设置启动时的默认 target。切换 target 时，默认不关闭前一个 target 启动的进程，我们可以通过systemctl isolate关闭前一个 target 里面所有不属于后一个 target 的进程。 三、日志管理Systemd 统一管理所有 unit 的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。 查看日志我们可以通过指令journalctl查看所有日志（默认情况下 ，只保存本次启动的日志）。journalctl -k可以查看内核日志，journalctl -b和journalctl -b -0可以查看系统本次启动的日志，journalctl -b -1可以查看上一次启动的日志，journalctl _PID=X查看指定进程的日志，journalctl /usr/bin/bash查看某个路径的脚本的日志，journalctl -u redis.service和journalctl -u redis.service --since today查看某个 unit 的日志。 Systemd 的特性（对比 SysVInit）为了保证运行在先前 Linux 版本上的应用程序运行稳定，systemd 兼容了原先的 SysVInit 以及 LSB initscripts，但也引入了新的特性。这使得系统中已经存在的服务和进程无需修改，降低了系统向 systemd 迁移的成本。但我们也应该了解 systemd 所做的改变，以更好的适应当前的版本。大体而言，systemd 相比 SysVInit 更改了以下几个方面： 一、支持并行启动系统启动时，需要启动很多启动项目，在 SysVInit 中，每一个启动项目都由一个独立的脚本负责，它们由 SysVinit 顺序地，串行地调用。因此总的启动时间是各脚本运行时间之和。而 systemd 通过 socket/D-Bus activation 等技术，能够将启动项目同时并行启动，大大提高了系统的启动速度。 二、提供按需启动能力当 sysvinit 系统初始化的时候，它会将所有可能用到的后台服务进程全部启动运行。并且系统必须等待所有的服务都启动就绪之后，才允许用户登录。这种做法有两个缺点：首先是启动时间过长；其次是系统资源浪费。 某些服务很可能在很长一段时间内，甚至整个服务器运行期间都没有被使用过。比如 CUPS，打印服务在多数服务器上很少被真正使用到。您可能没有想到，在很多服务器上 SSHD 也是很少被真正访问到的。花费在启动这些服务上的时间是不必要的；同样，花费在这些服务上的系统资源也是一种浪费。 Systemd 可以提供按需启动的能力，只有在某个服务被真正请求的时候才启动它。当该服务结束，systemd 可以关闭它，等待下次需要时再次启动它。 三、采用 Linux 的 Cgroup 特性跟踪和管理进程的生命周期Init 系统的一个重要职责就是负责跟踪和管理服务进程的生命周期。它不仅可以启动一个服务，也必须也能够停止服务。这看上去没有什么特别的，然而在真正用代码实现的时候，我们会发现有时候停止服务比一开始想的要困难。 服务进程一般都会作为守护进程（daemon）在后台运行，为此服务程序有时候会派生 (fork) 两次。在 SysVInit 中，需要在配置文件中正确地配置 expect 小节。这样 SysVInit 通过对 fork 系统调用进行计数，从而获知真正的守护进程的 PID 号。 还有更加特殊的情况。比如，一个 CGI 程序会派生两次，从而脱离了和 Apache 的父子关系。当 Apache 进程被停止后，该 CGI 程序还在继续运行。而我们希望服务停止后，所有由它所启动的相关进程也被停止。 为了处理这类问题，SysVInit 通过 strace 来跟踪 fork、exit 等系统调用，但是这种方法很笨拙，且缺乏可扩展性。Systemd 则利用了 Linux 内核的特性即 CGroup 来完成跟踪的任务。当停止服务时，通过查询 CGroup，Systemd 可以确保找到所有的相关进程，从而干净地停止服务。 CGroup 已经出现了很久，它主要用来实现系统资源配额管理。CGroup 提供了类似文件系统的接口，使用方便。当进程创建子进程时，子进程会继承父进程的 CGroup。因此无论服务如何启动新的子进程，所有的这些相关进程都会属于同一个 CGroup，systemd 只需要简单地遍历指定的 CGroup 即可正确地找到所有的相关进程，将它们一一停止即可。 四、启动挂载点和自动挂载的管理传统的 Linux 系统中，用户可以用/etc/fstab 文件来维护固定的文件系统挂载点。这些挂载点在系统启动过程中被自动挂载，一旦启动过程结束，这些挂载点就会确保存在。这些挂载点都是对系统运行至关重要的文件系统，比如 HOME 目录。和 SysVInit 一样，Systemd 会管理这些挂载点，以便能够在系统启动时自动挂载它们。Systemd 兼容了/etc/fstab 文件，我们可以继续使用该文件管理挂载点。 有时候用户还需要动态挂载点，比如打算访问 DVD 内容时，才临时执行挂载以便访问其中的内容，而不访问光盘时该挂载点被取消 (umount)，以便节约资源。传统地，人们依赖 autofs 服务来实现这种功能。 Systemd 内建了自动挂载服务，无需另外安装 autofs 服务，可以直接使用 systemd 提供的自动挂载管理能力来实现 autofs 的功能。 五、实现事务性依赖关系管理系统启动过程是由很多的独立工作共同组成的，这些工作之间可能存在依赖关系，比如挂载一个 NFS 文件系统必须依赖网络能够正常工作。Systemd 虽然能够最大限度地并发执行很多有依赖关系的工作，但是类似”挂载 NFS”和”启动网络”这样的工作还是存在天生的先后依赖关系，无法并发执行。对于这些任务，systemd 维护一个”事务一致性”的概念，保证所有相关的服务都可以正常启动而不会出现互相依赖，以至于死锁的情况。 六、能够对系统进行快照和恢复Systemd 支持按需启动，因此系统的运行状态是动态变化的，人们无法准确地知道系统当前运行了哪些服务。Systemd 快照提供了一种将当前系统运行状态保存并恢复的能力。 比如系统当前正运行服务 A 和 B，可以用 systemd 命令行对当前系统运行状况创建快照。然后将进程 A 停止，或者做其他的任意的对系统的改变，比如启动新的进程 C。在这些改变之后，运行 systemd 的快照恢复命令，就可立即将系统恢复到快照时刻的状态，即只有服务 A，B 在运行。一个可能的应用场景是调试：比如服务器出现一些异常，为了调试用户将当前状态保存为快照，然后可以进行任意的操作，比如停止服务等等。等调试结束，恢复快照即可。 这个快照功能目前在 systemd 中并不完善，似乎开发人员也没有特别关注它，因此有报告指出它还存在一些使用上的问题，使用时尚需慎重。 七、日志服务systemd 自带日志服务 journald，该日志服务的设计初衷是克服现有的 syslog 服务的缺点。比如： syslog 不安全，消息的内容无法验证。每一个本地进程都可以声称自己是 Apache PID 4711，而 syslog 也就相信并保存到磁盘上。 数据没有严格的格式，非常随意。自动化的日志分析器需要分析人类语言字符串来识别消息。一方面此类分析困难低效；此外日志格式的变化会导致分析代码需要更新甚至重写。 Systemd Journal 用二进制格式保存所有日志信息，用户使用 journalctl 命令来查看日志信息。无需自己编写复杂脆弱的字符串分析处理程序。 Systemd Journal 的优点如下： 简单性：代码少，依赖少，抽象开销最小。 零维护：日志是除错和监控系统的核心功能，因此它自己不能再产生问题。举例说，自动管理磁盘空间，避免由于日志的不断产生而将磁盘空间耗尽。 移植性：日志文件应该在所有类型的 Linux 系统上可用，无论它使用的何种 CPU 或者字节序。 性能：添加和浏览日志非常快。 最小资源占用：日志数据文件需要较小。 统一化：各种不同的日志存储技术应该统一起来，将所有的可记录事件保存在同一个数据存储中。所以日志内容的全局上下文都会被保存并且可供日后查询。例如一条固件记录后通常会跟随一条内核记录，最终还会有一条用户态记录。重要的是当保存到硬盘上时这三者之间的关系不会丢失。Syslog 将不同的信息保存到不同的文件中，分析的时候很难确定哪些条目是相关的。 扩展性：日志的适用范围很广，从嵌入式设备到超级计算机集群都可以满足需求。 安全性：日志文件是可以验证的，让无法检测的修改不再可能。 总结Systemd 作为 Centos7 最新采用的系统管理进程，相比前任有相当多的改变。它的优点是功能强大，使用方便，缺点是过于复杂，与操作系统的其他部分强耦合，可能在某种程度上违背了 Linux 原本”keep simple, keep stupid”设计哲学。但从一个系统使用者的角度，它的确在很多方面做得都要比它的前任更好。作为一个后端，我们需要对这些改变有所了解，才能将这个系统用得更好。 参考资料1、鸟哥的 Linux 私房菜：基础学习篇 第四版 第十九章 2、Systemd 入门教程 - 阮一峰的网络日志 3、CentOS / RHEL 7 : How to set default target (default runlevel)4、IBM developerWorks Systemed","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}],"keywords":[]},{"title":"简单的密码学生成唯一邀请码","slug":"简单的密码学生成唯一邀请码","date":"2018-03-23T09:10:23.000Z","updated":"2019-03-08T14:17:32.922Z","comments":true,"path":"2018/03/23/简单的密码学生成唯一邀请码/","link":"","permalink":"http://yoursite.com/2018/03/23/简单的密码学生成唯一邀请码/","excerpt":"","text":"最近项目需要生成用户邀请码，网上找了几个算法都不太满意，就自己写了一个。里面借鉴了一些密码学里的思路，最后的算法效果还不错。想把思路记录下来，可以用在类似对加密强度要求不高的场合下。 需求分析从业务需求和市面上其它产品邀请码的使用体验上来看，邀请码有以下几个强制性的要求： 不可重复 唯一确定 这两点要求首先就排除了 hash code 的可能，因为 hash code 是可以发生碰撞的。然后在强制性要求的基础之上，我们还有一些进一步的需求： 长度不能太长，6-10 位是合适的区间 不容易被推测出 资源消耗尽可能小 在这些需求的约束下，我们先来看看常见的通用的序列码生成算法。 通用方案通用方案的解决思路可以分为两种：一种是生成一串不重复随机数，然后将其保存到数据库里。使用邀请码时从数据库里查询就可以得到邀请人；另一种是对身份信息作加密，通常是用户 id，将加密后的密文作为邀请码，使用时可以不查询数据库，直接解密得到。理论上说，第二种方式稍好一点，可以少进行一次数据库查询。但是考虑到安全性，我们还是会把解密后的 id 拿到数据库中查询，防止有人输错或者伪造邀请码产生 NPE。因此在选择算法的时候，这两种思路我都有考虑到。 1、UUID谈到不重复的随机数，最先想到的自然是 UUID。UUID 是一种软件构建的标准，也是开放软件基金会组织（OSF）在分布式计算环境领域的一部分。按照 OSF 制定的标准计算，它用到了以太网卡地址、纳秒级时间、芯片 ID 码和许多可能的数字，保证对在同一时空中的所有机器都是唯一的。Java 的工具类 java.util.UUID 是 Java 提供的一整套 UUID 生成方案，对于开发者来说可以很方便的调用。然而 UUID 并不适合用在这里，因为 UUID 的位数是固定的 32 位，这个对于我们的邀请码来说显然是太长了（想象一下用户面对面分享邀请码的时候居然需要报一串 32 位的数字+字母）。 网上也有用 UUID 的一部分当随机数的，但 UUID 只能保证完整的 32 位是不会重复的，不能保证其中的某一段不重复，因此这个方案也行不通。 2、系统当前时间系统当前时间也是一种常见的随机数生成方案。它的做法是先获取到系统当前时间，再用它和某个时间点对比，将这两段时间的间隔以毫秒或者纳秒为单位存到内存中去。最后我们程序获取到的是一串数字。Java 提供了两个系统函数用于实现这个功能：System.currentTimeMillis() 和 System.nanoTime()。然而这两个系统函数在这个业务里都有各自的问题。 System.currentTimeMillis() 返回的是从 1970.1.1 UTC 零点开始到现在的时间，精确到毫秒。它的问题在于不能支持高并发的邀请码生成。在这套方案中，只要我们的系统在某 1 秒内生成的邀请码超过 32 个，那么出现相同邀请码的概率就超过 50%（详见生日攻击 ）。显然，这个规模的并发量是不能接受的。 System.nanoTime() 返回的是从某个固定但随意的时间点开始的时间，精确到纳秒。这个固定但随意的时间，在不同 JVM 中是不一样的。这也就是说不同计算机计算出来的 nanoTime() 是有可能重合的。甚至同一台计算机重启 JVM 后生成的 nanoTime() 也是可能重合的。这违背了我们的第一个要求。 3、RC4 算法RC4 对于学过密码的同学来说肯定不会陌生。它是大名鼎鼎的 RSA 三人组中的头号人物 Ronald Rivest 在 1987 年设计的一种轻量级对称加密算法。它的特点是按字节流加密，也就是说明文多长，密文就多长。这一特点很好避免了 UUID 只能生成 32 位字符串的尴尬。而且 RC4 是一个轻量级加密算法，运行速度快，占用资源少，很好地满足了我们的第 5 点要求。乍一看 RC4 似乎是种理想的方案，然而实际一跑就出现了问题： 出现了乱码！这是因为字符的取值在 0~255，而我们熟悉的英文和数字只占了其中的 62 位，其它符号是我们不熟悉的，当然也不能作为邀请码。解决方法也很简单，把字符串转成 16 进制即可： 由于把 8 位的字符串转成了 4 位的 16 进制，字符串的长度增加了一倍，但长度尚在可接受范围之内。不太满意的一点是加密后的密文都是连续性的，高位的数字基本不变。这也意味着如果被邀请的同学输错了后几位数字，后台大概率检测不到他的这次操作失误，因为他输入的错误邀请码能在数据库里被找到。而且连续的密文容易被找出规律，安全性较低。因此这种方式也不建议。 4、用户身份标志+随机数这种方法是我在网上找到的已经被用于实际业务中的方法，它的大致思路是这样： 获取用户身份的唯一标志，比如用户 ID。 将用户 ID 补全，补全的位数取决于你希望得到的邀请码长度，如：106 可以补全为 00106. 随机生成一串大写字母串，长度和补全后的用户 ID 相同，如：SZUDF。 将随机数隔位插入用户 ID，得到邀请码：S0Z0U1D0F6。 这种方式得到的邀请码基本能满足我们的要求：由用户 ID 的唯一性保证了邀请码的唯一性；随机生成的字母串又能保证不容易被找到规律，同时又提高了用户操作的容错率；长度也在可接受范围内。因此第一版的邀请码生成算法我们采用了这种方式。 但是它仍然有改进的空间。①、字母和数字的位置是固定的，有一定的容易被察觉的规律，且对于数字来说，仍然具有连续性；②、用户 ID 直接暴露在密文中，存在风险；③、没有校验位，邀请码的校验依赖于数据库，无法对恶意伪造大量错误邀请码的攻击进行有效防御。 因此我在这种算法上作了改进，克服了以上的缺点。 我的方案为了让字母和数字的位置不再固定，我将用户 ID 作了 36 进制转换，即把用户 ID 映射为一串字母+数字的组合，高位用 0 补全。 123456int[] b = new int[CODE_LENGTH];b[0] = id;for (int i = 0; i &lt; 5; ++i) &#123; b[i + 1] = b[i] / 36; b[i] = (b[i]) % 36;&#125; 同时把随机数生成的范围扩大到字母和数字，这样密文中的每一位都可能是数字和字母，规律性就不易察觉得多。 然后是用户 ID 暴露在密文中的问题。这个问题的解决办法是我们可以加一点盐 。盐的取值最好不要太小，太小缺乏隐蔽性；也不能太大，太大会占用过多用户 ID 的取值空间。具体的取值取决于业务需求。 当然，最后是校验位的问题。这个问题我思考之后决定在随机数上作文章。目前的算法，会生成和补全后用户 ID 长度相等的随机数。这有两点问题：一是邀请码长度稍显过长，6 位用户 ID 就会产生 12 位的邀请码；二是随机数没有提供额外的信息，这对密文来说是一种资源浪费。鉴于此，我改变了随机数的生成方式，让它不再随机生成，而是承担起对密文其它部分的校验功能。同时改变了它的长度，把它固定在 2 位。当然，缩短后的校验码就没有办法隔位插入，我就把它放在了密文尾部。用这一套校验方式，理论上能保证 99.9%的误操作可以被后台检测出来而不需要查询数据库。 生成的邀请码如上，相比第一版，可以看到一些很明显的改进。而且理论上可以容纳 1000 万的用户量，比第一版的 10 万位有了很大提升。 但是这一版的算法仍有问题，细心的同学会发现 6 个验证码的 2~5 位是一样的。这是因为低位的变化不足以影响到高位，导致高位的字符没有发生变化。这样的算法在安全性上是比较薄弱的，攻击人可以利用这一规律大大降低猜测的区间。而且密文和密钥（超参数，本文中就是 salt 和 prime1）之间的关系比较直接，没有进行进一步的处理。现代密码学认为，密码的安全性应该由密钥来保障而不是加密算法，如果密钥和密文之间的联系过于直接，密码的安全性便会削弱。当然，密码学上对这些问题有解决方法，那就是扩散和混淆。 扩散和混淆扩散 (diffusion) 和混淆 (confusion) 是 C.E.Shannon 提出的设计密码体制的两种基本方法，其目的是为了抵抗对手对密码体制的统计分析。在分组密码的设计中，充分利用扩散和混淆，可以有效地抵抗对手从密文的统计特性推测明文或密钥。扩散和混淆是现代分组密码的设计基础。 所谓扩散就是让明文中的每一位影响密文中的许多位，或者说让密文中的每一位受明文中的许多位的影响。这样可以隐蔽明文的统计特性。当然，理想的情况是让明文中的每一位影响密文中的所有位，或者说让密文中的每一位受明文中所有位的影响。 所谓混淆就是将密文与密钥之间的统计关系变得尽可能复杂，使得对手即使获取了关于密文的一些统计特性，也无法推测密钥。使用复杂的非线性代替变换可以达到比较好的混淆效果，而简单的线性代替变换得到的混淆效果则不理想。可以用”揉面团”来形象地比喻扩散和混淆。当然，这个”揉面团”的过程应该是可逆的。乘积和迭代有助于实现扩散和混淆。选择某些较简单的受密钥控制的密码变换，通过乘积和迭代可以取得比较好的扩散和混淆的效果。 改进后的算法我用扩散和混淆的方式对算法进行了改进。 扩散的方式很简单，只需要将个位和其它每一位作和后取余，即可把变化传导到每一位。为了隐蔽，我还把变化进行了放大：1id = id * PRIME1; PRIME1 可以为任意随机数，最好和 36 以及 10^n（n 为用户 id 位数）互质。这是因为根据循环群 的性质：若 m 和 p 互质，则 ( id * m ) % p 的结果遍历[0, p) 的所有整数。保证了放大后结果的分布和原数据的分布同样均匀。为了使结果看起来更随机，我还给每一位分配了不同系数： 12345678910id = id * PRIME1;id = id + SALT;int[] b = new int[CODE_LENGTH];b[0] = id;for (int i = 0; i &lt; 5; ++i) &#123; b[i + 1] = b[i] / ARY; b[i] = (b[i] + b[0] * i) % ARY;&#125;b[5] = (b[0] + b[1] + b[2]) * PRIME1 % ARY;b[6] = (b[3] + b[4] + b[5]) * PRIME1 % ARY; ARY 表示进制，这里是 36，也可以设置成其它的数，比如 62（字母区分大小写）。代码的第 7、9、10 行中我分别对每一位设置了不同的系数，使得每一次的增量显得更不固定。 然后是混淆。混淆我用了P-box 的方式，其实就是将数字洗牌。比如把 1234567 洗成 5237641。这样处理之后可以隐藏密钥和密文之间的关系。洗牌的方式也很简单，选择一个和 CODE_LENGTH（本文中为 7）互质的数 PRIME2，和数组角标相乘取余即可（原理同 PRIME1）。最终的代码如下： 1234567891011121314151617public static String inviCodeGenerator(int id) &#123; id = id * PRIME1; id = id + SALT; int[] b = new int[CODE_LENGTH]; b[0] = id; for (int i = 0; i &lt; 5; ++i) &#123; b[i + 1] = b[i] / ARY; b[i] = (b[i] + b[0] * i) % ARY; &#125; b[5] = (b[0] + b[1] + b[2]) * PRIME1 % ARY; b[6] = (b[3] + b[4] + b[5]) * PRIME1 % ARY; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; sb.append(HEX_36_Array.charAt(b[(i * PRIME2) % CODE_LENGTH])); &#125; return sb.toString();&#125; 测试结果如下： 完美符合我们的需求^_^ 邀请码和用户 ID 的转换也很简单，因为加密的过程都是可逆的，所以只需将加密过程作逆变换即可。这里要提一点就是我们是设置了校验位的，所以可以在解密的过程中对邀请码进行校验，如果是用户的误输入或者有人企图构造邀请码恶意攻击，我们在业务层就可以检测出来，不需要拿到数据层去做校验。具体的解密代码如下： 1234567891011121314151617181920212223242526272829303132public static int inviDecoding(String inviCode) &#123; if (inviCode.length() != CODE_LENGTH) &#123; return -1; &#125; int res = 0; int a[] = new int[CODE_LENGTH]; int b[] = new int[CODE_LENGTH]; char[] c = new char[CODE_LENGTH]; for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; a[(i * PRIME2) % CODE_LENGTH] = i; &#125; for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; c[i] = inviCode.charAt(a[i]); &#125; for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; a[i] = HEX_36_Array.indexOf(c[i]); &#125; b[5] = (a[0] + a[1] + a[2]) * PRIME1 % ARY; b[6] = (a[3] + a[4] + a[5]) * PRIME1 % ARY; if (a[5] != b[5] || a[6] != b[6]) &#123; return -1; &#125; for (int i = 4; i &gt;= 0; --i) &#123; b[i] = (a[i] - a[0] * i + ARY * i) % ARY; &#125; for (int i = 4; i &gt; 0; --i) &#123; res += b[i]; res *= ARY; &#125; res = ((res + b[0]) - SALT) / PRIME1; return res;&#125; 代码 18~22 行就是在作校验。 总结不同的业务有不同的需求，市面上通用的方案可能只能满足大部分共性的需求，但对于某些特定的需求，市面上找不到完善的解决方案，这时候就需要我们独立解决问题的能力。本科的时候觉得密码学没用，没想到在这用上了。越来越觉得世上没有无用的知识，多积累一些总是好的^_^","categories":[],"tags":[{"name":"密码学","slug":"密码学","permalink":"http://yoursite.com/tags/密码学/"}],"keywords":[]},{"title":"分布式缓存的一致性 Hash 算法","slug":"分布式缓存的一致性 Hash 算法","date":"2018-03-16T03:58:32.000Z","updated":"2019-03-08T14:17:05.394Z","comments":true,"path":"2018/03/16/分布式缓存的一致性 Hash 算法/","link":"","permalink":"http://yoursite.com/2018/03/16/分布式缓存的一致性 Hash 算法/","excerpt":"","text":"分布式有利于提高网站的可用性、伸缩性和安全性。分布式缓存，顾名思义，就是将缓存服务器作分布式配置，提高集群性能和可伸缩性。然而对分布式缓存集群而言，不能像应用服务器一样使用简单的负载均衡手段来实现，因为分布式缓存服务器集群中不同服务器中缓存的数据各不相同，缓存访问请求不可以在缓存服务器集群中的任意一台处理，必须先找到缓存有需要数据的服务器，然后才能访问。这个特点会严重制约分布式缓存集群的伸缩性设计，因为新上线的缓存服务器没有缓存任何数据，而已下线的缓存服务器还缓存着网站的许多热点数据。 分布式缓存算法的主要设计目标，就是在保证负载均衡的同时，尽可能让新上线的缓存服务器对整个分布式缓存集群影响最小，也就是说新加入缓存服务器后应使整个缓存服务器集群中已经缓存的数据尽可能还被访问到，同时新增服务器对原有服务器的影响要尽可能均衡。 余数 hash 算法分布式缓存的算法要保证对于一个确定数据，它所在的服务器也必须是确定的。比如对于键值对 &lt;’BEIJING’,DATA&gt;，每一次查找 ‘BEIJING’ 这个关键词，系统总是访问相同的服务器去读取数据。这样，只要服务器还缓存着该数据，就能保证命中。 余数 hash 是一个不错的办法:用服务器数目除缓存数据 KEY 的 hash 值，余数为服务器列表下标编号。假设我们有三台服务器，要存键值对 &lt;’BEIJING’,DATA&gt; 到缓存。则先计算 ‘BEIJING’的 hash 值是 490806430（Java 中的 HashCode() 返回值），用服务器数目 3 除该值，得到余数 1，将其保存到 NODE1 上，以后想要读取数据 ‘BEIJING’ 的时候，只要服务器数量不变，一定会定位 NODE1 上。同时，由于 HashCode 具有随机性，使用余数 hash 算法可保证缓存数据在整个缓存服务器集群中比较均匀地分布。 对余数 Hash 算法稍加改进，还能满足对不同硬件性能的服务器集群作负载均衡的需求。比如 3 台服务器中，第 2 台服务器的性能是另外 2 台的 2 倍，这时我们可以调整算法，把除数设置为 4，当余数为 1、2 的时候，将数据存入 NODE2，实现加权负载均衡。 事实上，如果不需要考虑缓存服务器集群伸缩性，余数 hash 几乎可以满足绝大多数的缓存路由需求。 余数 hash 算法的不足然而，当考虑到缓存服务器集群伸缩性的时候，余数 hash 算法的不足就暴露出来了。假设由于业务发展，网站需要将 3 台缓存服务器扩容至 4 台。这时用户再次访问 ‘BEIJING’ 这个数据的时候，除数变成了 4，用 4 除‘BEIJING’的 Hash 值 490806430，余数为 2，对应 NODE2。由于数据 &lt;’BEIJING’,DATA&gt; 缓存在 NODE1，对 NODE2 的读缓存操作失败，缓存没有命中。 很容易就可以计算出，3 台服务器扩容至 4 台的时候，大约只能命中 25%的缓存（3/4），随着服务器集群规模的增大，这个比例线性上升。当 100 台服务器的集群加入一台新服务器，不能命中的概率是 99%（N/(N+1)）。 这个结果显然是无法接受的，因此我们需要改进这种算法，提高增加新机器后的缓存命中率。 一致性 hash 算法一致性 hash 算法通过一个叫作一致性 hash 环的数据结构提高了新增机器后的缓存命中率，如下图： 具体算法过程为：先构造一个长度为 2^32 的整数环（这个环被称作一致性 hash 环），根据节点名称的 hash 值（其分布范围为[0,2^32-1]）将缓存服务器节点放置在这个 hash 环上。然后根据需要缓存的数据的 KEY 值计算得到其 hash 值（其分布范围也同样为[0,2^32-1]），然后在 hash 环上顺时针查找距离这个 KEY 的 hash 值最近的缓存服务器节点，完成 KEY 到服务器的 hash 映射查找。 在上图中，假设 NODE1 的 hash 值为 3594963423，NODE2 的 hash 值为 1845328979，而 KEY0 的 hash 值为 2534256785，那么 KEY0 在环上顺时针查找，找到的最近的节点就是 NODE1。 当缓存服务器集群需要扩容的时候，只需要将新加入的节点名称（NODE3）的 hash 值放入一致性 hash 环中，由于 KEY 是顺时针查找距离其最近的节点，因此新加入的节点只影响整个环中的一小段，如下图中加粗的一段： 假设 NODE3 的 hash 值是 2790324235，那么加入 NODE3 后，KEY0（hash 值 2534256785）顺时针查找得到的节点就是 NODE3。 如上图所示，加入新节点 NODE3 后，原来的 KEY 大部分还能继续计算到原来的节点，只有 KEY3、KEY0 从原来的 NODE1 重新计算到 NODE3。这样就能保证大部分被缓存的数据还可以继续命中。3 台服务器扩容至 4 台，命中率可以达到 75%，远高于余数 hash 的 25%，而且随着集群规模增大，继续命中原有缓存数据的概率也逐渐增大，100 台服务器扩容增加 1 台，继续命中的概率是 99%。虽然仍有小部分数据缓存在服务器中不能被读到，但这个比例在可接受范围之内。 一致性 hash 算法的不足虽然上述的算法使缓存服务器集群在增加新服务器后的命中率有了大幅提高，但还存在一个小小的问题。 新加入的节点 NODE3 只影响了原来的节点 NODE1，也就是说一部分原来需要访问 NODE1 的缓存数据现在需要访问 NODE3（概率上是 50%）。但是原来的节点 NODE0 和 NODE2 不受影响，这也就意味着，新引入的 NODE3 这个节点只减轻了 NODE1 的压力，假设原先三个节点的压力是一样大的，那么在引入 NODE3 这个节点后，NODE0 和 NODE2 的缓存数据量和负载压力是 NODE1 与 NODE3 的两倍。这个是有违我们负载均衡的初衷的。 怎么办？ 改！ 改进后的一致性 hash 算法我们可以通过增加一层虚拟层的方式解决这个问题：将每台物理缓存服务器虚拟为一组虚拟缓存服务器，将虚拟缓存服务器的 hash 值放置在 hash 环上，KEY 在环上先找到虚拟服务器节点，再得到物理服务器的信息。 这样新加入物理服务器节点时，是将一组虚拟节点加入环中，如果虚拟节点的数目足够多，这组虚拟节点将会影响同样多数目的已经在环上存在的虚拟节点，这些已经存在的虚拟节点又对应不同的物理节点。最终的结果是：新加入一台缓存服务器，将会较为均匀地影响原来集群中已经存在的所有服务器，也就是说分摊原有缓存服务器集群中所有服务器的一小部分负载，其总的影响范围和上面讨论过的相同。如下图所示： 显然每个物理节点对应的虚拟节点越多，各个物理节点之间的负载越均衡，新加入物理服务器对原有的物理服务器的影响越保持一致（这就是一致性 hash 这个名称的由来）。那么在实践中，一台物理服务器虚拟为多少个虚拟服务器节点合适呢？太多会影响性能，太少又会导致负载不均衡，一般说来，经验值是 150，当然根据集群规模和负载均衡的精度需求，这个值应该根据具体情况具体对待。","categories":[],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/tags/读书笔记/"}],"keywords":[]},{"title":"MySQL 索引原理","slug":"MySQL 索引原理","date":"2018-03-13T02:48:28.000Z","updated":"2019-03-08T14:11:51.863Z","comments":true,"path":"2018/03/13/MySQL 索引原理/","link":"","permalink":"http://yoursite.com/2018/03/13/MySQL 索引原理/","excerpt":"","text":"数据库索引是面试中的常考项，也是日常开发中提高程序可用性的实用技巧。我在美团技术点评团队 中找到了这篇文章《MySQL 索引原理及慢查询优化》，摘录了其中专讲索引原理的部分，以供随时复习。 索引目的索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到 m 字母，然后从下往下找到 y 字母，再找到剩下的 sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到 m 开头的单词呢？或者 ze 开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？ 索引原理除了词典，生活中随处可见索引的例子，如火车站的车次表、图书的目录等。它们的原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询 (&gt;、&lt;、between、in)、模糊查询 (like)、并集查询 (or) 等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果 1000 条数据，1 到 100 分成第一段，101 到 200 分成第二段，201 到 300 分成第三段……这样查第 250 条数据，只要找第三段就可以了，一下子去除了 90%的无效数据。但如果是 1 千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是 lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。 磁盘 IO 与预读前面提到了访问磁盘，那么这里先简单介绍一下磁盘 IO 和预读，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分，寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在 5ms 以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘 7200 转，表示每分钟能转 7200 次，也就是说 1 秒钟能转 120 次，旋转延迟就是 1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘 IO 的时间约等于 5+4.17 = 9ms 左右，听起来还挺不错的，但要知道一台 500 -MIPS 的机器每秒可以执行 5 亿条指令，因为指令依靠的是电的性质，换句话说执行一次 IO 的时间可以执行 40 万条指令，数据库动辄十万百万乃至千万级数据，每次 9 毫秒的时间，显然是个灾难。下图是计算机硬件延迟的对比图，供大家参考：考虑到磁盘 IO 是非常高昂的操作，计算机操作系统做了一些优化，当一次 IO 时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次 IO 读取的数据我们称之为一页 (page)。具体一页有多大数据跟操作系统有关，一般为 4k 或 8k，也就是我们读取一页内的数据时候，实际上才发生了一次 IO，这个理论对于索引的数据结构设计非常有帮助。 索引的数据结构前面讲了生活中索引的例子，索引的基本原理，数据库的复杂性，又讲了操作系统的相关知识，目的就是让大家了解，任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘 IO 次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。 详解 b+树如上图，是一颗 b+树，关于 b+树的定义可以参见 B+树，这里只说一些重点，浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块 1 包含数据项 17 和 35，包含指针 P1、P2、P3，P1 表示小于 17 的磁盘块，P2 表示在 17 和 35 之间的磁盘块，P3 表示大于 35 的磁盘块。真实的数据存在于叶子节点即 3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如 17、35 并不真实存在于数据表中。 b+树的查找过程如图所示，如果要查找数据项 29，那么首先会把磁盘块 1 由磁盘加载到内存，此时发生一次 IO，在内存中用二分查找确定 29 在 17 和 35 之间，锁定磁盘块 1 的 P2 指针，内存时间因为非常短（相比磁盘的 IO）可以忽略不计，通过磁盘块 1 的 P2 指针的磁盘地址把磁盘块 3 由磁盘加载到内存，发生第二次 IO，29 在 26 和 30 之间，锁定磁盘块 3 的 P2 指针，通过指针加载磁盘块 8 到内存，发生第三次 IO，同时内存中做二分查找找到 29，结束查询，总计三次 IO。真实的情况是，3 层的 b+树可以表示上百万的数据，如果上百万的数据查找只需要三次 IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次 IO，那么总共需要百万次的 IO，显然成本非常非常高。 b+树性质1.通过上面的分析，我们知道 IO 次数取决于 b+数的高度 h，假设当前数据表的数据为 N，每个磁盘块的数据项的数量是 m，则有 h=㏒(m+1)N，当数据量 N 一定的情况下，m 越大，h 越小；而 m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如 int 占 4 字节，要比 bigint8 字节少一半。这也是为什么 b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于 1 时将会退化成线性表。2.当 b+树的数据项是复合的数据结构，比如 (name,age,sex) 的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当 (张三,20,F) 这样的数据来检索的时候，b+树会优先比较 name 来确定下一步的所搜方向，如果 name 相同再依次比较 age 和 sex，最后得到检索的数据；但当 (20,F) 这样的没有 name 的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候 name 就是第一个比较因子，必须要先根据 name 来搜索才能知道下一步去哪里查询。比如当 (张三,F) 这样的数据来检索时，b+树可以用 name 来指定搜索方向，但下一个字段 age 的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是 F 的数据了， 这个是非常重要的性质，即索引的最左匹配特性。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"},{"name":"收藏","slug":"收藏","permalink":"http://yoursite.com/tags/收藏/"}],"keywords":[]},{"title":"MySQL 作图建库","slug":"MySQL 作图建库","date":"2018-02-03T03:59:36.000Z","updated":"2019-03-08T14:12:28.868Z","comments":true,"path":"2018/02/03/MySQL 作图建库/","link":"","permalink":"http://yoursite.com/2018/02/03/MySQL 作图建库/","excerpt":"","text":"workbench 是 MySQL 自带的数据库可视化工具，相比另一款常用的数据库可视化工具 Navicat 最大的优势就是它是免费的。两者的操作逻辑没有太大区别，不过 workbench 英文的界面对初学者不是太友好。我花了一点时间学习了如何作图，希望能为以后节省时间。 E-R 图是描述数据库关系最常用的方式。优点是直观、详尽。workbench 当然也提供了这种图表的绘制方式，不过在里面叫 EER 图。经过我的一番考证，这个 EER 图就是我们常说的 E-R 图。 一、已有数据库，自动生成 EER 图：①、首先在 mysql workbench 里选中 Database——&gt; reverse engineering ②、然后选择你建立的连接（也就是数据库） ③、接下来一路 next，直到最后选择导出的数据库 ④、自动生成的 E-R 图大概长相如图： 二、先画 EER 图，然后自动生成数据库：①、启动软件过后，注意不需要连接数据库（我第一次就是直接连接数据库了所以找不到设计 ER 模型的地方） ②、点击”+” ,进入模型设计界面 ③、双击 Add Diagram 进入如下设计界面 ④、点击工具栏表格，并在设计区域点击，就会出现一个 table1 并双击它 ⑤、最后 执行 “File”-&gt;”Export” 按钮，选择 Forward Engineer SQL CREATE Script (ctrl+shift+G). 这样就可以把模型导出为 SQL 脚本文件。现在执行这个 SQL 文件就 OK 了","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"Java 中的异常","slug":"Java 中的异常","date":"2017-12-29T11:43:59.000Z","updated":"2019-03-08T14:10:34.550Z","comments":true,"path":"2017/12/29/Java 中的异常/","link":"","permalink":"http://yoursite.com/2017/12/29/Java 中的异常/","excerpt":"","text":"异常是 Java 中非常基础但重要的一块内容，异常的概念理解起来简单，但系统讲起来却并不容易。专门抽了一个下午将它整理出来，以备所需。 一、异常的分类Java 中的异常是指程序在编译或者运行中遇到的问题。 Java 中的异常都继承自 Throwable 类。Throwable 是 java 语言中所有错误和异常的超类，它表示可抛（万物皆可抛）。它有两个子类：Error 和 Exception。 Error：Error 为错误，是程序无法处理的。如 OutOfMemoryError、ThreadDeath 等，出现这种情况只能听之任之，交由 JVM 处理。一般情况下 JVM 也没法子，只好终止线程。 Exception：Exception 是程序可以处理的异常。它有很多子类，比如 IOException,RuntimeException,SQLException 等等。其中 RuntimeException 比较特殊，它表示程序运行中 发生的异常，在编译时可以不接受检查。而其它异常编译时就要接受检查，对于抛出异常的部分，要么 throw 给子类，要么用 try…catch 处理。 常见异常继承关系： 二、常见的异常记住常见的异常可以让我们更高效地调试程序代码，有助于提高开发效率。 runtimeException 子类 ArrayIndexOutOfBoundsException：数组索引越界异常。当对数组的索引值为负数或大于等于数组大小时抛出 ArithmeticException：算术条件异常。譬如：整数除零等 NullPointerException：空指针异常。当应用试图在要求使用对象的地方使用了 null 时，抛出该异常。譬如：调用 null 对象的实例方法、访问 null 对象的属性、计算 null 对象的长度、使用 throw 语句抛出 null 等等 ClassNotFoundException：找不到类异常。当应用试图根据字符串形式的类名构造类，而在遍历 CLASSPAH 之后找不到对应名称的 class 文件时，抛出该异常 NegativeArraySizeException：数组长度为负异常 IOException 子类 IOException：操作输入流和输出流时可能出现的异常 EOFException：文件已结束异常 FileNotFoundException：文件未找到异常 其它 Exception 子类 ClassCastException：类型转换异常类 ArrayStoreException：数组中包含不兼容的值抛出的异常 SQLException：操作数据库异常类 三、异常处理的机制Java 中异常抛出后有两种处理方式：try…catch…finally 机制和 throws 继续抛出机制。 try…catch…finally 机制try…catch…finally 是 java 中的关键字，使用方式如下：1234567try&#123; 抛出异常的代码&#125;catch(异常类)&#123; 处理语句&#125;finally&#123; 处理完后执行语句&#125; 在使用 try…catch…finally 时，若 try 中某一语句抛出了异常，则 try 后面的代码会被屏蔽，直接进行 catch 中的语句。catch 中语句执行到 return(返回语句) 时，会先看看有没有 finally 块，若有，则优先执行 finally 块中语句。如果 catch 块和 finally 块都有 return 语句，则执行 finally 块中的。 抛出异常的代码可能抛出多种异常，处理异常的 catch 也可以有多个 catch，分别处理不同的异常。在执行时，会依次查找 catch 语句，直到找到第一个能 catch 某异常的代码块。 throw 和 throws 机制throws 用在方法声明中，表示这个方法将会抛出某一异常。使用该方法的时候必须对该异常进行处理（try…catch 或 throw）throw 用在语句中，表示抛出一个异常。抛出异常后方法会出栈，方法中后面的代码将不会执行。 四、注意事项 throws 只是再次抛出了某个异常，并没有真正处理异常。在使用中，需要有代码去真正处理抛出的异常 如果子类重写了父类的方法，则子类能够抛出的异常只能是父类的子集（父类所有异常类及它们的子类集合） 对于 runtimeException 异常及其子类，程序可以选择显式处理也可以不处理，交给程序调用者去处理；对于其它 exception（编译时异常），程序必须要显式处理（try…catch）或抛出（throws），交给调用者处理 五、异常处理规约摘自阿里 JAVA 开发手册，个人觉得有助于良好的代码风格形成。 1.【强制】不要捕获 Java 类库中定义的继承自 RuntimeException 的运行时异常类，如：IndexOutOfBoundsException/NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。正例：if(obj != null) {…}反例：try { obj.method() } catch(NullPointerException e){…} 2.【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。 3.【强制】对大段代码进行 try-catch，这是不负责任的表现。catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。 4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。 5.【强制】有 try 块放到了事务代码中，catch 异常后，如果需要回滚事务，一定要注意手动回滚事务。 6.【强制】finally 块必须对资源对象、流对象进行关闭，有异常也要做 try-catch。说明：如果 JDK7，可以使用 try-with-resources 方式。 7.【强制】不能在 finally 块中使用 return，finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。 8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。 9.【推荐】方法的返回值可以为 null，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回 null 值。调用方需要进行 null 判断防止 NPE 问题。说明：本规约明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败，运行时异常等场景返回 null 的情况。 10.【推荐】防止 NPE，是程序员的基本修养，注意 NPE 产生的场景：1） 返回类型为包装数据类型，有可能是 null，返回 int 值时注意判空。反例：public int f(){ return Integer 对象}; 如果为 null，自动解箱抛 NPE。2） 数据库的查询结果可能为 null。3） 集合里的元素即使 isNotEmpty，取出的数据元素也可能为 null。4） 远程调用返回对象，一律要求进行 NPE 判断。5） 对于 Session 中获取的数据，建议 NPE 检查，避免空指针。6） 级联调用 obj.getA().getB().getC()；一连串调用，易产生 NPE。 11.【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的 http/api 开放接口必须使用“错误码”；而应用内部推荐异常抛出；跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess、“错误码”、“错误简短信息”。说明：关于 RPC 方法返回方式使用 Result 方式的理由：1）使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。2）如果不加栈信息，只是 new 自定义异常，加入自己的理解的 error message，对于调用端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输的性能损耗也是问题。 12.【推荐】定义时区分 unchecked/checked 异常，避免直接使用 RuntimeException 抛出，更不允许抛出 Exception 或者 Throwable，应使用有业务含义的自定义异常。推荐业界已定义过的自定义异常，如：DAOException/ServiceException 等。 13.【参考】避免出现重复的代码（Don’t Repeat Yourself），即 DRY 原则。说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是共用模块。正例：一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取：private boolean checkParam(DTO dto){…}","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]}]}