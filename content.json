{"meta":{"title":"huzb的小书斋","subtitle":"念念不忘，必有回响","description":"一枚前行的小码农","author":"huzb","url":"http://yoursite.com"},"pages":[{"title":"标签","date":"2019-03-08T00:40:27.822Z","updated":"2019-03-08T00:40:27.822Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2019-03-08T00:40:27.819Z","updated":"2019-03-08T00:40:27.819Z","comments":false,"path":"repository/index.html","permalink":"http://yoursite.com/repository/index.html","excerpt":"","text":""},{"title":"关于","date":"2019-03-08T00:40:27.811Z","updated":"2019-03-08T00:40:27.811Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"个人详细介绍"}],"posts":[{"title":"Spring源码浅析——创建单例bean对象","slug":"Spring源码浅析——创建单例bean对象","date":"2019-03-04T09:52:47.000Z","updated":"2019-03-08T13:16:29.306Z","comments":true,"path":"2019/03/04/Spring源码浅析——创建单例bean对象/","link":"","permalink":"http://yoursite.com/2019/03/04/Spring源码浅析——创建单例bean对象/","excerpt":"","text":"上一篇概览了源码中容器刷新的部分，但是略过了其中最重要的对象初始化和注入部分。这一篇就以一个普通单例对象为例，看它是如何加入到容器中的。 准备首先定义一个类：1234public class Person &#123; int age; String name;&#125; 在配置类中加入如下配置：1234@Beanpublic Person person()&#123; return new Person();&#125; 那么Spring会把一个Person类型的对象注入容器中 源码分析直接进入上一篇中略过的finishBeanFactoryInitialization(beanFactory)方法： 1234567891011121314151617181920212223protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 为容器设置一个全局类型转换器，默认没有；类型转换器可以自定义，常见的有String转Date的类型转换器 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // 检查beanFactory是否没有内嵌值解析器，默认是有的 if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(strVal -&gt; getEnvironment().resolvePlaceholders(strVal)); &#125; // 加载第三方模块，如AspectJ；默认没有 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // 停用临时类加载器 beanFactory.setTempClassLoader(null); // 禁止对除了bean definition metadata缓存之外的配置进行修改 beanFactory.freezeConfiguration(); // 实例化所有剩余的不指定懒加载的单例对象（核心部分） beanFactory.preInstantiateSingletons();&#125; 可以看到核心的函数就是最后一行beanFactory.preInstantiateSingletons();，它会对所有剩余的不指定懒加载的单例对象进行实例化操作。我们进入beanFactory.preInstantiateSingletons();中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public void preInstantiateSingletons() throws BeansException &#123; // 获取所有的bean定义信息 List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); // 开始实例化所有还未实例化的不指定懒加载的单例对象 for (String beanName : beanNames) &#123; // 合并父BeanDefinition与子BeanDefinition，见附录 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // 判断bean是否抽象、是否单例、是否懒加载 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; // FactoryBean类型的bean进行实例化 if (isFactoryBean(beanName)) &#123; // 获取FactoryBean的工厂本身（以&amp;开头），如果没有则创建 Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); if (bean instanceof FactoryBean) &#123; final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean; boolean isEagerInit; // 这里需要判断是不是SmartFactoryBean，因为SmartFactoryBean会定义一个isEagerInit()方法来决定getObject()的实例对象是否懒加载 if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; isEagerInit = AccessController.doPrivileged((PrivilegedAction&lt;Boolean&gt;) ((SmartFactoryBean&lt;?&gt;) factory)::isEagerInit, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; // 对非懒加载的bean实例化 if (isEagerInit) &#123; getBean(beanName); &#125; &#125; &#125; // 没有实现FactoryBean接口的直接实例化 else &#123; getBean(beanName); &#125; &#125; &#125; // 调用所有SmartInitializingSingleton类型的实现类的afterSingletonsInstantiated方法；通过名字可以知道它表示单例对象实例化后需要做的操作 for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125;&#125; 可看出，不管是FactoryBean对象本身、FactoryBean的getObject()创建的对象还是普通单例对象，最终都会依赖getBean(beanName)这个方法来实例化对象。我们进入这个方法来看：```java","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Spring源码浅析——容器刷新流程概览","slug":"Spring源码浅析——容器刷新流程概览","date":"2019-03-03T09:08:49.000Z","updated":"2019-03-08T13:16:37.416Z","comments":true,"path":"2019/03/03/Spring源码浅析——容器刷新流程概览/","link":"","permalink":"http://yoursite.com/2019/03/03/Spring源码浅析——容器刷新流程概览/","excerpt":"","text":"本文是 Spring 源码浅析系列的第一篇。Spring 版本是 Spring Boot 2.1.2.RELEASE （即 Spring 5.1.4），以默认配置启动，分析框架工作的原理。 众所周知，Spring 以容器管理所有的 bean 对象，容器的实体是一个BeanFactory 对象。但我们常用的容器是另一个 ApplicationContext ，它在内部持有了 BeanFactory，所有和 BeanFactory 相关的操作都会委托给内部的 BeanFactory 来完成。 ApplicationContext的继承关系如下图所示： ApplicationContext是一个接口，ClassPathXmlApplicationContext和AnnotationConfigApplicationContext是两个比较常用的实现类，前者基于xml使用，后者基于注解使用。SpringBoot中默认后面一种。 ApplicationContext也继承了BeanFactory接口，BeanFactory的继承关系如下图所示： 从继承关系我们可以获得以下信息： ApplicationContext 继承了 ListableBeanFactory，这个 Listable 的意思就是，通过这个接口，我们可以获取多个 Bean，最顶层 BeanFactory 接口的方法都是获取单个 Bean 的。 ApplicationContext 继承了 HierarchicalBeanFactory，Hierarchical 的意思是说我们可以在应用中起多个 BeanFactory，然后可以将各个 BeanFactory 设置为父子关系。 ApplicationContext非常重要，所以我们第一篇就看一下ApplicationContext初始化的过程。默认配置下，SpringBoot中的ApplicationContext初始化在refresh()方法中，为什么叫refresh()而不是init()呢？因为ApplicationContext建立起来以后，其实我们是可以通过调用refresh()这个方法重建的，这样会将原来的ApplicationContext销毁，然后再重新执行一次初始化操作。 1、容器刷新概览refresh()是个总览全局的方法，我们可以通过这个方法概览容器刷新的过程：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、检验配置文件格式 prepareRefresh(); // ClassPathXmlApplicationContext 会在这里解析 xml 配置；AnnotationConfigApplicationContext 的解析发在初始化，这里只是简单的获取 // 这里的解析是指把配置信息都提取出来了，保存在了一个 Map&lt;String,BeanDefinition&gt; 中 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean 等 prepareBeanFactory(beanFactory); try &#123; // BeanFactory 准备工作完成后进行的后置处理工作，子类可以自定义实现，Spring Boot 中是个空方法 postProcessBeanFactory(beanFactory); //=======以上是BeanFactory的预准备工作======= // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类（注意和之前的 BeanFactoryPostProcessor 的区别） registerBeanPostProcessors(beanFactory); // 初始化 MessageSource 组件（做国际化功能；消息绑定，消息解析） initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器 initApplicationEventMulticaster(); // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前），Spring Boot 中默认没有定义 onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口 registerListeners(); // 初始化所有的 singleton beans（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 容器刷新完成操作 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 通过上面的代码和注释我们总览了容器刷新的整个流程，下面我们来一步步探索每个环节都做了什么。 2、刷新前的准备工作：prepareRefresh()这步比较简单，直接看代码中的注释即可。 1234567891011121314151617protected void prepareRefresh() &#123; // 记录启动时间， // 将 active 属性设置为 true，closed 属性设置为 false，它们都是 AtomicBoolean 类型 this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); // Initialize any placeholder property sources in the context environment // Spring Boot 中是个空方法 initPropertySources(); // 校验配置属性的合法性 getEnvironment().validateRequiredProperties(); // 记录早期的事件 this.earlyApplicationEvents = new LinkedHashSet&lt;&gt;();&#125; 3、获取 Bean 容器前面说过ApplicationContext内部持有了一个BeanFactory，这步就是获取ApplicationContext中的BeanFactory。在ClassPathXmlApplicationContext中会做很多工作，因为一开始ClassPathXmlApplicationContext中的BeanFactory并没有创建，但在AnnotationConfigApplicationContext 比较简单，直接返回即可。 1234567891011protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; // 通过 cas 设置刷新状态 if (!this.refreshed.compareAndSet(false, true)) &#123; throw new IllegalStateException( \"GenericApplicationContext does not support multiple refresh attempts: just call 'refresh' once\"); &#125; // 设置序列号 this.beanFactory.setSerializationId(getId()); // 返回已创建的 BeanFactory return this.beanFactory;&#125; 4、准备 Bean 容器：prepareBeanFactory()BeanFactory获取之后并不能马上使用，还要在BeanFactory中做一些准备工作，包括类加载器、表达式解析器的设置，几个特殊的BeanPostProcessor的添加等。 12 总结我们大致浏览了一遍Spring容器刷新的过程，通过源码我们可以总结出以下几点： Spring容器在启动的时候，先会将所有注册进来的Bean的定义信息保存在beanDefinitionMap中，然后在合适的时机创建这些bean： 用到这个bean的时候，利用getBean()创建bean，创建好以后保存在容器中； finishBeanFactoryInitialization()统一创建剩下所有的bean； Spring的诸多功能都依赖于后置处理器（BeanFactoryPostProcessor和BeanPostProcessor）的实现，后置处理器工作在各种时期（容器准备完成、bean初始化之前、bean初始化之后、容器刷新完成等等），常见的有： AutowiredAnnotationBeanPostProcessor：处理自动注入； AnnotationAwareAspectJAutoProxyCreator：来做AOP功能； InstantiationAwareBeanPostProcessor：对象初始化前后； DestructionAwareBeanPostProcessor：对象销毁前； 容器的刷新过程分为两个阶段：容器准备和对象初始化注入。 容器准备阶段会对BeanFactory进行一些设置，然后执行BeanFactory的后置处理器； 对象初始化注入会依次：注册bean的后置处理器——初始化MessageSource组件——注册事件派发器——注册事件监听器——初始化和注入剩下的单例bean——完成刷新工作； 事件驱动模型： ApplicationListener：事件监听； ApplicationEventMulticaster：事件派发； ApplicationContext.publishEvent()：事件发布；","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}],"keywords":[]},{"title":"Redis高可用和分布式","slug":"Redis高可用和分布式","date":"2019-02-23T08:20:24.000Z","updated":"2019-03-08T13:16:04.648Z","comments":true,"path":"2019/02/23/Redis高可用和分布式/","link":"","permalink":"http://yoursite.com/2019/02/23/Redis高可用和分布式/","excerpt":"","text":"Redis可以单机部署，但会带来单点问题和性能瓶颈。为此Redis提供了主从复制、哨兵和集群的方式来解决这些问题。 一、主从复制主从复制可以将两台或者多台服务器之间的数据同步，这样在主服务器下线后，从服务器可以继续对外提供服务，保证了系统的高可用；另外主从复制也可以进行读写分离，主服务器只提供写操作或少量的读，把多余读请求通过负载均衡算法分流到单个或多个从服务器上。 Redis的复制功能分为同步和命令传播两个操作，同步操作用于将从服务器的状态更新至主服务器当前的状态；命令传播操作用于在主服务状态被修改时，让主从状态重新保持一致。 同步可以通过使用SLAVEOF &lt;host&gt; &lt;port&gt;命令来让一个服务器成为另一个服务器的从服务器。此时从服务器会自动向主服务器发送SYNC的命令进行同步操作，步骤如下： 1）主服务器收到SYNC命令后执行BGSAVE命令，在后台生成一个RDB快照文件，并用一个缓冲区记录从现在开始执行的所有写命令。 2）从服务器从主服务器接收到RDB文件后，丢弃所有旧数据，载入主服务器发来的快照文件。 3）主服务器把记录在缓冲区的命令发送给从服务器，从服务器执行这些命令，同步完成。 但是这种同步方式会带来一个断线后重复值效率低的问题：如果从服务器短暂掉线后重连主服务器，主从之间不得不重新同步所有数据，这是没有必要的，因为从服务器仍然保留了大部分数据。Redis 2.8推出了部分重同步的方式解决了这个问题。 部分重同步部分重同步用于处理断线后复制的情况。Redis部分重同步的实现由以下三个部分构成： 主服务器和从服务器的复制偏移量：复制偏移量表示当前从服务器从主服务器中接收了多少字节的数据。 主服务器的复制积压缓冲区：复制积压缓冲区以FIFO的形式保存了最近的写命令。 服务器运行id：用于标识服务器 当从服务器断线重连之后，会向主服务器发送一个PSYNC &lt;runid&gt; &lt;offset&gt;的命令，offset即当前从服务器的复制偏移量，runid是从服务器记录的掉线之前的主服务器id。主服务器在收到PSYNC后，会首先对比自己的runid和传过来的runid是否一致，如果一致，说明这台从服务器之前和自己同步过数据，然后根据offset的差值把复制积压缓冲区的数据同步给从服务器。这样就实现了增量更新。 命令传播在同步完成之后，主从服务器就进入了命令传播阶段。在这个阶段，主服务器发生写操作后，会把相应的命令发送给从服务器执行，这样主从之间就保持了数据一致性。 从服务器也会向主服务器发送命令REPLCONF ACK &lt;replication_offset&gt;，这是一个心跳检测命令，每隔1秒就会发一次，它有以下两个作用： 检测主从服务器的网络连接状态：主服务器会记录从服务器上次心跳检测的时间，如果超过1秒，就说明连接出了故障。如果主服务器和大量从服务器之间的连接出了故障，比如有3台以上的从服务器超过10秒没有心跳检测，则会拒绝执行写命令。 检测命令丢失：和部分重同步类似，心跳检测也会发送从服务器的当前复制偏移量（replication_offset），主服务器在接收到心跳检测后会检查这个偏移量是否和自己的一致，如果不一致会补发缺失的数据。 二、Sentinel（哨兵）Sentinel（哨兵）是Redis高可用的解决方案：由一个或多个Sentinel实例组成的Sentinel系统可以监视任意多个主服务器以及这些服务器下的所有从服务器。在主服务器进入下线状态时，自动将下线服务器下的某个从服务器升级成主服务器。 Sentinel系统是由Sentinel实例组成的，彼此间通过命令连接相互通信： Sentinel实例会以每秒一次的频率向自己监控的主服务器发送PING命令，主服务器会回复该命令，以此来监控主服务器的在线状态。当Sentinel实例发送PING命令之后等待超过配置指定时间之后，Sentinel实例会判定该主服务器处于主观下线状态。 当一个Sentinel实例判定自己监控的某个主服务器为主观下线之后，它会询问其它监控该服务器的Sentinel实例，当超过配置指定数量的Sentinel实例也认为该服务器已下线时，Sentinel实例会判定该服务器为客观下线。 当有超过配置指定数量的Sentinel实例认为该服务器客观下线之后，监视该服务器的各个Sentinel会进行协商，通过Raft算法选举出一个领头Sentinel对下线主服务器执行故障转移操作。 故障转移操作包括三个部分： 1）在已下线主服务器属下的所有从服务器中挑选一个从服务器，将其转换为主服务器。 2）让其它从服务器改为复制新的主服务器（发送SLAVEOF命令）。 3）将原来的主服务器设置为从服务器。 三、集群Redis集群是Redis提供的分布式数据库方案，集群通过分片的方式进行数据共享，并提供复制和转移功能。 集群中的节点可以通过向其它节点发送CLUSTER MEET &lt;ip&gt; &lt;port&gt;命令邀请其它节点加入集群。 集群的整个数据库被分为16384个槽，数据库中的每一个键都属于这16384个槽中的一个，只有当集群中的每个槽都有节点在处理时，集群才处于上线状态。 我们可以通过CLUSTER ADDSLOTS [slot...]命令将一个或多个槽指派给节点。每个节点会记录自己和集群中其它节点被指派的槽。 节点在接到一个命令请求时，会先检查这个命令请求要处理的键所在的槽是否由自己负责，如果不是的话，节点将向客户端返回一个MOVED错误，MOVED错误携带的信息可以指引客户端转向正确的节点。 除了可以指派槽以外，我们还可以通过redis-trib这个软件将槽重新分片。重新分片的关键是将属于某个槽的所有键值对从一个节点转移至另一个节点。 在重新分片期间，如果客户端向原来的节点请求键k，而k已经被转移到另一个节点时，节点会返回一个ASK错误，指引客户端到新的节点。 MOVED错误表示槽的负责权已经永远转移了，而ASK错误只是两个节点在槽迁移时的临时措施。 故障转移为了集群的高可用，集群中的节点也分主节点和从节点，从节点复制主节点的数据。 主节点之间通过PING消息来互相确认对方的在线状态，如果没有在规定时间内收到对方的PONG，则会把对方标记为疑似下线状态，并将这个消息告知给集群中的其它主节点。当某个主节点发现集群中有超过半数主节点认为某个主节点疑似下线，它就会把这个主节点标记为下线，并广播给集群中其它的节点（主节点+从节点）。 当从节点发现自己正在复制的主节点已下线时，会通过raft算法选举出新的主节点。选举的候选节点是已下线主节点的所有从节点，投票节点是其它所有主节点。最后一个从节点会被推选为新的主节点，接管由已下线节点负责处理的槽。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"CMS,G1和ZGC","slug":"CMS-G1和ZGC","date":"2019-02-21T11:23:54.000Z","updated":"2019-03-08T04:43:24.529Z","comments":true,"path":"2019/02/21/CMS-G1和ZGC/","link":"","permalink":"http://yoursite.com/2019/02/21/CMS-G1和ZGC/","excerpt":"","text":"本文主要介绍比较常用的垃圾收集器：CMS，G1和ZGC。CMS是服务器使用比较多的收集器，侧重点在低停顿；JDK 9以后，G1成为了默认收集器，它的设计目标是停顿可控；最新的JDK 11中，加入了实验性质的ZGC，这个收集器可以将停顿时间降至10ms以下。 1、JDK1.8 之前默认的垃圾收集器JDK1.8 之前（包括1.8），默认的垃圾收集器是Parallel Scavenge（新生代）+Parallel Old（老年代）。 Parallel Scavenge是一款基于复制算法的新生代垃圾处理器，它的设计目标是吞吐量优先。所谓吞吐量就是指：用户运行时间/(用户运行时间+垃圾回收时间)。很显然吞吐量越大越好。用户需要给Parallel Scavenge设置一个吞吐量的目标，然后Parallel Scavenge会自动控制每一次垃圾回收的时间。另外Parallel Scavenge还有自适应调节策略，只要打开-XX:+UseAdaptiveSizePolicy，它就会动态调整新生代大小、Eden与Survivor的比例、晋升老年代对象大小等参数，以达到最大的吞吐量。 和Parallel Scavenge配套使用的老年代收集器是Parallel Old，这是一款基于标记-整理算法的垃圾处理器。 为什么不用CMS作为老年代收集器？ 这是因为Parallel Scavenge的作者没有使用HotSpot VM给定的代码框架，而是自己独立实现了一个。这就导致Parallel Scavenge和当时大部分收集器都不兼容，其中就包括CMS。所以在1.8时代，比较流行的有两套收集器，一套是Parallel Scavenge（新生代）+Parallel Old（老年代），另一套是ParNew（使用复制算法，新生代）+ CMS（老年代） 2、低停顿的CMSCMS，Concurrent Mark Sweep，是一款老年代的收集器，它关注的是垃圾回收最短的停顿时间。命名中Concurrent说明这个收集器是有与工作执行并发的能力的，Mark Sweep则代表算法用的是标记-清除算法。 CMS的工作原理分为四步： 初始标记：单线程执行，仅仅把GC Roots的直接关联可达的对象标记一下，速度很快，需要停顿。 并发标记：对于初始标记过程所标记的初始对象，进行并发追踪标记，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：清除之前标记的垃圾，不需要停顿。 由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 由于CMS以上特性，缺点也是比较明显的： 标记-清除算法会导致内存碎片比较多。 CMS的并发能力依赖于CPU资源，所以在CPU数少和CPU资源紧张的情况下，性能较差。 无法处理浮动垃圾。浮动垃圾是指在并发清除阶段用户线程继续运行而产生的垃圾，这部分垃圾只能等下次GC时处理。由于浮动垃圾的存在，CMS不能等待内存耗尽的时候才进行GC，而要预留一部分内存空间给用户线程。这里会浪费一些空间。 为什么不用标记-整理？ 因为在并发清除阶段，其它用户线程还在工作，要保证它们运行的资源不受影响。而标记-整理算法会移动对象，所以不能使用标记-整理。 3、停顿可控的G1G1是一款可以掌管所有堆内存空间的收集器。G1 把堆划分成多个大小相等的独立区域（Region），新生代和老年代不再物理隔离。 通过引入Region的概念，将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法避免了空间碎片化，也提高了回收的灵活性——G1会根据平均每个 Region回收需要的时间（经验预测）和各个Region的回收收益，制定回收计划。 每个Region都有一个Remembered Set，用来记录该Region对象的引用对象所在的Region。通过使用Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 三种GC模式： Young GC，发生于新生代空间不足时，回收全部新生代，可以通过控制新生代Region的个数来控制Young GC的时间开销。 Mixed GC，当堆中内存使用超过整个堆大小的InitiatingHeapOccupancyPercent（默认45）时启动。 回收全部新生代，并根据预期停顿时间回收部分收益较高的老年代。 Full GC（JDK 9引入），发生于老年代空间不足时，相当于执行一次STW的full gc。 整体的执行流程： 初始标记：标记了从GC Root开始直接关联可达的对象，速度很快，单线程，需停顿。 并发标记：对于初始标记过程所标记的初始对象，进行并发追踪标记，并记录下每个Region中的存活对象信息用于计算收益，不需要停顿。 最终标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 筛选回收：根据GC模式、回收时间和回收收益确定回收计划，回收后的空Region会加入到空闲列表，需要停顿。 由于G1会把存活的对象集中起来放到Survivor Region中，并通过空闲列表整理所有的空Region，所以整体来看是基于“标记 - 整理”算法实现的收集器；但从局部（两个 Region 之间）上来看又是基于“复制”算法实现的。但不论如何，这两种算法都需要移动对象，所以G1的回收阶段是需要停顿的。 4、几乎无停顿的ZGC在JDK 11当中，加入了实验性质的ZGC。它的回收耗时平均不到2毫秒。它是一款低停顿高并发的收集器。ZGC几乎在所有地方并发执行的，除了初始标记的是STW的。所以停顿时间几乎就耗费在初始标记上，这部分的实际是非常少的。那么其他阶段是怎么做到可以并发执行的呢？ZGC主要新增了两项技术，一个是着色指针Colored Pointer，另一个是读屏障Load Barrier。 着色指针Colored Pointer ZGC利用指针的64位中的几位表示Finalizable、Remapped、Marked1、Marked0（ZGC仅支持64位平台），以标记该指向内存的存储状态。相当于在对象的指针上标注了对象的信息。注意，这里的指针相当于Java术语当中的引用。 在这个被指向的内存发生变化的时候（内存在整理时被移动），颜色就会发生变化。 读屏障Load Barrier 由于着色指针的存在，在程序运行时访问对象的时候，可以轻易知道对象在内存的存储状态（通过指针访问对象），若请求读的内存在被着色了。那么则会触发读屏障。读屏障会更新指针再返回结果，此过程有一定的耗费，从而达到与用户线程并发的效果。 把这两项技术联合下理解，引用R大（RednaxelaFX）的话 与标记对象的传统算法相比，ZGC在指针上做标记，在访问指针时加入Load Barrier（读屏障），比如当对象正被GC移动，指针上的颜色就会不对，这个屏障就会先把指针更新为有效地址再返回，也就是，永远只有单个对象读取时有概率被减速，而不存在为了保持应用与GC一致而粗暴整体的Stop The World。 ZGC和G1一样将堆划分为Region来清理、移动，稍有不同的是ZGC中Region的大小是会动态变化的。 ZGC的回收流程如下： 1、初始停顿标记 停顿JVM地标记Root对象，1，2，4三个被标为live。 2、并发标记 并发地递归标记其他对象，5和8也被标记为live。 3、移动对象 对比发现3、6、7是过期对象，也就是中间的两个灰色region需要被压缩清理，所以陆续将4、5、8 对象移动到最右边的新Region。移动过程中，有个forward table纪录这种转向。 活的对象都移走之后，这个region可以立即释放掉，并且用来当作下一个要扫描的region的to region。所以理论上要收集整个堆，只需要有一个空region就OK了。 4、修正指针 最后将指针都妥帖地更新指向新地址。 ZGC虽然目前还在JDK 11还在实验阶段，但由于算法与思想是一个非常大的提升，相信在未来不久会成为主流的GC收集器使用。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"Redis事件、事务和pipeline","slug":"Redis事件、事务和pipeline","date":"2019-02-12T11:34:57.000Z","updated":"2019-03-08T13:16:13.031Z","comments":true,"path":"2019/02/12/Redis事件、事务和pipeline/","link":"","permalink":"http://yoursite.com/2019/02/12/Redis事件、事务和pipeline/","excerpt":"","text":"本文介绍了Redis中事件的类型和事件的调度与执行，以及对批量事件处理的两种方式：事务和pipeline。 一、事件Redis服务器是一个事件驱动程序。Redis的事件有两类： 文件事件：服务器通过套接字与客户端连接，文件事件就是服务器对套接字操作的抽象。 时间事件：服务器对定时操作的抽象。 文件事件Redis包装了底层的select、epoll等来实现自己的网络事件处理器。它使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。 时间事件服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。 时间事件又分为： 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 时间事件中的属性when会记录下次执行的时间，周期性事件在执行后会更新when的值，而定时事件会被删除。 Redis将所有时间事件都放在一个无序链表中，由时间事件执行器通过遍历整个链表查找出已到达的时间事件，并调用相应的事件处理器。 事件的调度与执行服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。 事件调度与执行由 aeProcessEvents 函数负责，伪代码如下：12345678910111213141516def aeProcessEvents(): # 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() # 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms &lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) # 处理所有已产生的文件事件 procesFileEvents() # 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下： 12345678def main(): # 初始化服务器 init_server() # 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() # 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下： 二、事务Redis通过MULTI、EXEC、WATCH等命令来实现事务功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 一个事务包括三个步骤： 事务开始：事务以 MULTI 开始，返回OK命令。 命令入队：每个事务命令成功进入队列后，返回QUEUED。 事务执行：EXEC执行事务。 Redis不支持事务回滚功能，事务中的一个Redis命令执行失败以后，会继续执行后续的命令。 三、pipeline多个命令被一次性发送给服务器，而不是一条一条发送，这种方式被称为流水线，它可以减少客户端与服务器之间的网络通信次数从而提升性能。 可以通过redis-cli --pipe的方式批量发送命令。如cat commands.txt | redis-cli --pipe，commands.txt中的命令会被以RESP协议（这是一个Redis自行规定的协议，用于命令的批量执行）的格式发给服务器，服务器也会返回一个RESP格式的结果。 当然我们不用自己去实现这个协议，Jedis为我们实现好了，我们可以很方便地调用：1234567891011Jedis jedis = new Jedis(\"localhost\", 6379);//使用pipelinePipeline pipeline = jedis.pipelined();//删除listspipeline.del(\"lists\");//循环添加10000个元素for(int i = 0; i &lt; 10000; i++)&#123; pipeline.rpush(\"lists\", i + \"\");&#125;//执行pipeline.sync();","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"Redis持久化和过期机制","slug":"Redis持久化和过期机制","date":"2019-02-08T06:12:30.000Z","updated":"2019-03-08T13:15:54.925Z","comments":true,"path":"2019/02/08/Redis持久化和过期机制/","link":"","permalink":"http://yoursite.com/2019/02/08/Redis持久化和过期机制/","excerpt":"","text":"本文主要介绍了Redis持久化的两种机制：RDB和AOF，以及键过期的策略：惰性删除和定期删除，还有RDB、AOF和复制功能对过期键的处理。 RDBRDB是Redis持久化的第一种方式。有两个Redis命令可以用于生成RDB文件，一个是SAVE，另一个是BGSAVE。SAVE会阻塞Redis服务器进程，执行时Redis服务器会阻塞所有客户端发送的命令。12redis&gt; SAVEOK BGSAVE会派生出一个子进程执行，执行时仍可继续处理客户端的命令，但会拒绝客户端SAVE和BGSAVE的命令，延迟BGREWRITEAOF命令。12redis&gt; BGSAVEBackground saving started 执行条件SAVE命令会阻塞服务器，所以只能手动执行。BGSAVE可以在不阻塞的情况下执行，所以可以配置save选项让服务器每隔一段时间自动执行一次。 比如我们可以向服务器提供以下配置：123save 900 1save 300 10save 60 10000 那么只要满足以下三个条件中的任意一个即可被执行： 服务器在900秒之内对数据库进行了至少1次修改。 服务器在300秒之内对数据库进行了至少10次修改。 服务器在60秒之内对数据库进行了至少10000次修改。 为了实现这一功能，服务器会维持一个记录距离上次保存之后修改的次数的dirty计数器和一个记录上次保存时间的lastsave属性。 周期操作函数serverCron默认每个100毫秒就会执行一次，它的其中一项工作就是检查save选项设置的条件是否满足，如果满足的话就会执行BGSAVE命令。 文件内容RDB文件有多个部分，包括握手字段’REDIS’字符串，版本号，数据库，’EOF’和校验字段。 核心部分是数据库字段，数据库字段包括了握手字段’SELECTDB’，数据库编号和键值对，数据库编号指示了这是第几个数据库，而键值对则保存了各项数据。 键值对中除了类型和数据，还可能会有过期时间。对于不同类型的键值对，RDB文件会用不同的方式来保存它们。 RDB文件本身是一个经过压缩的二进制文件，每次SAVE或者BGSAVE都会创建一个新的RDB文件，不支持追加操作。 AOFAOF是Redis持久化的第二种方式，在AOF和RDB同时开启时，服务器会优先考虑从AOF恢复数据，因为AOF每次记录间隔的时间更短。 和RDB直接记录键值对不同，AOF记录的是命令。服务器在执行完一个写命令以后，会把这条命令追加到服务器aof_buf缓冲区的末尾，并在一个适当的时候写入文件。重建时服务器会创建一个伪客户端，依次执行文件中的命令即可完成数据的载入。 文件的写入与同步AOF在持久化时会调用操作系统的write函数，但通常该函数会把数据保存在一个内存缓冲区里面而不是立刻刷入磁盘。这就带来一个安全问题。为了避免这个问题操作系统又提供了fsync和fdatasync两个强制刷盘的同步函数。我们把write称为写入，把fsync和fdatasync称为同步。 服务器会在每次事件循环结束之前根据appendfsync选项写入和同步aof_buf中的数据： always：写入并同步 everysec：写入，如果距离上次同步超过1秒，则同步 no：只写入，何时同步由操作系统决定 AOF重写随着服务器运行时间的流逝，AOF文件中的内容会越来越多，文件的体积也会越来越大，不仅会对宿主计算机造成影响，也拖慢了数据恢复所需要的时间。 AOF重写是指重新生成一个AOF文件替换原来的AOF文件。但这里的重写不会对原有的文件进行读取、分析或者写入，而是把数据库中的键值对折算成命令，重新写入新的文件。 重写是一个耗时的操作，因此Redis把它放到后台去操作，对应的指令是BGREWRITEAOF。在重写过程中服务器还可能接收新的指令，因此Redis会维护一个AOF重写缓冲区，记录重写期间的写命令，在重写完成后追加到AOF文件末尾。 RDB和AOF对比RDB的优点： RDB 是一个非常紧凑的文件，它的体积更小，且可以选择持久化的时间，适合做备份的文件。比如每天的备份，每月的备份。 RDB 对主进程更友好，父进程只需要fork出一个子进程，无须执行任何磁盘I/O操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 RDB的缺点： 因为RDB 文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少 5 分钟才保存一次 RDB 文件，间隔时间比较长。 RDB 虽然会把持久化的操作交给子进程，但每次都会从头开始，在数据集比较庞大时， fork() 可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 AOF的优点： AOF使用追加的方式，每次写入时间很短，因此可以允许更短间隔的持久化操作，比如1秒。 AOF文件的可读性比较好，如果你不小心执行了一条命令，只要AOF文件未被重写，那么只要停止服务器，移除AOF文件里的该条命令然后重启Redis即可。 AOF的缺点： 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 使用 fsync 会降低Redis的性能，导致AOF 的速度可能会慢于 RDB 。 RDB和AOF各有所长，RDB体积小，恢复速度快，而且可以生成快照；AOF 频率更高，可以保存更新的数据。一般来说，推荐同时使用。 Redis过期机制Redis采取的是惰性删除和定期删除配合使用的方式。 惰性删除是指Redis会在访问某个键的时候检查该键是否过期，如果过期，就会将输入键从数据库中删除。但惰性删除不能及时清理内存，因此Redis还有定期删除的机制。 定期删除是另一种过期键删除方式。Redis会维护一个过期字典（如下图所示），所有声明了过期时间的键都会被添加进这个字典中。周期操作函数serverCron执行时，会在规定时间内随机检查一部分键的过期时间，并删除其中的过期键。 RDB、AOF和复制功能对过期键的处理RDB对过期键的处理RDB文件在生成时会检查每个键的过期时间，过期键不会被添加进RDB文件里。 载入RDB文件时，如果该服务器是主服务器，则不会载入文件中过期的键；如果该服务器是从服务器，则不论过期与否都会被载入。不过，因为主从服务器在同步的时候，从服务器的数据库会被清空，所以一般来讲，过期键对载入RDB文件的从服务器不会造成影响。 AOF对过期键的处理AOF文件写入时，如果数据库中的某个键已过期，但它还没被删除，那么AOF文件不会因为这个键产生任何影响。当它被惰性删除或者定期删除之后，程序会向AOF文件追加一条DEL命令显示记录该键已被删除。 AOF重写时，和生成RDB文件一样，会过滤掉已经过期的键。 复制功能对过期键的处理主服务器在删除一个过期键后，会显式地向所有从服务器发送一个DEL命令，告知从服务器删除这个过期键。 为了保持主从一致性，从服务器在执行客户端发送的读命令时，即使碰到过期键也不会将过期键删除，而是继续像处理未过期键一样处理过期键。 从服务器只有在接到主服务器发来的DEL命令之后，才会删除过期键。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"Redis数据结构和对象","slug":"Redis数据结构和对象","date":"2019-02-05T05:45:44.000Z","updated":"2019-03-08T13:16:19.230Z","comments":true,"path":"2019/02/05/Redis数据结构和对象/","link":"","permalink":"http://yoursite.com/2019/02/05/Redis数据结构和对象/","excerpt":"","text":"本文主要介绍了Redis的6种数据结构：SDS、链表、字典、跳跃表、整数集合、压缩列表，和5种对象：字符串、列表、哈希、集合、有序集合。 一、数据结构SDSSDS是对C字符串的封装，用于表示字符串 SDS使用预分配的方式为字符串分配空间，free字段表示当前未使用的空间，当字符串增长时，会优先使用未使用的空间，如果未使用的空间不足，Redis会为SDS分配额外的空间。分配算法具体为：1）如果增长后的字符串长度小于1MB，Redis将额外分配等同于增长后的字符串长度的空间，此时free和len的大小相等；2）如果增长后的字符串长度大于1MB，Redis将额外分配1MB大小的空间。 分配后的空间不会被回收，如果字符串缩短，缩短的空间会被加入到free空间中。 链表 Redis的链表是一个双向链表，它的特性如下： 双端：链表节点带有prev和next指针，获取某个节点的前置和后置节点的复杂度都是O(1)。 无环：表头节点的prev和表尾节点的next指针都指向NULL。 带表头指针和表尾指针：通过head和tail指针获取表头和表尾节点的复杂度为O(1)。 计数器：获取节点个数复杂度为O(1) 多态：利用dup（复制节点保存的值）、free(释放节点保存的值)和match（比较节点的值和输入的值是否相等），来实现保存不同的值。 字典字典是一个散列表结构，使用MurmurHash2算法计算哈希值，使用拉链法保存哈希冲突。 Redis的字典dict中包含两个哈希表dictht，这是为了方便进行rehash操作。在扩容时，将其中一个dictht上的键值对rehash到另一个dictht上面，完成之后释放空间并交换两个dictht的角色。 渐进式rehash通过记录dict的rehashidx完成，它从0开始，然后每执行一次rehash都会递增。例如在一次rehash中，要把dict[0] rehash到dict[1]，这一次会把dict[0]上table[rehashidx]的键值对rehash到dict[1]上，dict[0]的table[rehashidx]指向null，并令rehashidx++。 在rehash期间，每次对字典执行添加、删除、查找或者更新操作时，都会执行一次渐进式rehash。 采用渐进式rehash会导致字典中的数据分散在两个dictht上，因此对字典的查找操作也需要到对应的dictht去执行。 跳跃表跳跃表是有序集合的底层实现之一。 跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。每个跳跃表节点的层高都是1-32之间的随机数。 跳跃表中的节点按照分数大小排列，当分值相同时，节点按照成员对象的大小进行排序。 在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找。下图演示了查找 22 的过程。 与红黑树等平衡树相比，跳跃表具有以下优点： 插入速度非常快速，因为不需要进行旋转等操作来维护平衡性； 并发插入时只需锁住少数节点 支持范围查找 更容易实现 跳跃表的缺点： 重复存储分层节点，消耗内存 整数集合整数集合是集合键的底层实现之一，它的底层是一个数组，这个数组以有序、无重复的方式保存集合元素。元素的类型可以为int16_t,int32_t或者int64_t。程序会根据新添加元素的类型，改变数组中元素的类型。 encoding表示元素的类型，数组中所有元素的类型是一样的。当有更大的数加入进来的时候，数组会进行升级操作，比如从int16_t升级到int32_t。 整数集合只能升级，不能降级。 压缩列表压缩列表被用作列表和哈希的底层实现之一，是一种为节约内存而开发的，由任意多个节点组成的顺序数据结构。每个节点可以保存一个字节数组或一个整数值。 entry1、entry2…是实际存储数据的节点，除此之外还有些字段记录列表的信息：zlbytes记录整个压缩列表占用的内存字节数，zltail指向压缩列表表尾节点，zllen记录包含的节点数量，zlend是个特殊值字段（0xFF）用于标记末端。 每个节点由previous_entry_length、encoding、content三部分组成。previous_entry_length保存了前一个节点的长度，由此可以实现从表尾向表头的遍历。encoding是个复用字段，记录了content的编码和长度。下图的encoding最高两位00表示节点保存的是一个字节数组，后六位001011记录了字节数组长度11。content保存着节点的值”hello world”。 二、对象Redis并没有使用以上的数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统。 对象类型 底层实现 可以存储的值 操作 STRING int，sds 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作对整数和浮点数执行自增或者自减操作 LIST 链表，压缩列表 列表 从两端压入或者弹出元素 对单个或者多个元素进行修剪，只保留一个范围内的元素 HASH 字典，压缩列表 包含键值对的无序散列表 添加、获取、移除单个键值对获取所有键值对检查某个键是否存在 SET 整数集合，字典 无序集合 添加、获取、移除单个元素检查一个元素是否存在于集合中计算交集、并集、差集从集合里面随机获取元素 ZSET （字典+跳跃表），压缩列表 有序集合 添加、获取、删除元素根据分值范围或者成员来获取元素计算一个键的排名 SET对象使用整数集合保存只包含整数的集合，使用字典保存含有字符串的集合。使用字典保存集合时，字典的键是一个元素的成员，字典的值为NULL。 ZSET对象同时使用字典和跳跃表保存有序集合，使用字典保存有序集合时，字典的键保存了元素的成员，字典的值保存了元素的分值。ZSET集合元素会同时共享在字典和跳跃表中（保存的是指针，不会造成数据的重复） 为什么有序集合需要同时使用字典和跳跃表来实现？ 在理论上，有序集合可以单独使用字典或者跳跃表来实现。但两者都有不可替代的地方：字典可以在O(1)时间复杂度查找成员的分值，跳跃表可以执行范围型操作。因此为了同时获得这两个特性，Redis使用了字典和跳跃表来实现有序集合。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[]},{"title":"HTTP2.0和QUIC","slug":"HTTP2-0和QUIC","date":"2019-02-01T05:23:15.000Z","updated":"2019-03-08T13:14:33.498Z","comments":true,"path":"2019/02/01/HTTP2-0和QUIC/","link":"","permalink":"http://yoursite.com/2019/02/01/HTTP2-0和QUIC/","excerpt":"","text":"HTTP1.1在应用层以纯文本的形式进行通信。每次通信都要带完整的HTTP的头，而且不考虑pipeline模式的话，每次都是一去一回。这样在实时性、并发性上都存在问题。http2.0通过首部压缩、多路复用、二进制分帧、服务端推送等方式获得了更高的并发和更低的延迟。 首部压缩HTTP 2.0 将原来每次都要携带的大量请求头中的key value保存在服务器和客户端两端，对相同的头只发送索引表中的索引。 如果首部发生变化了，那么只需要发送变化了数据在Headers帧里面，新增或修改的首部帧会被追加到“首部表”。首部表在 HTTP 2.0 的连接存续期内始终存在,由客户端和服务器共同渐进地更新 。 多路复用原先的http会为每一个请求建立一个tcp连接。但由于客户端对单个域名的允许的最大连接数有限，以及三次握手和慢启动等问题，导致效率很低。pipeline模式是一个比较好的解决办法，但同样会带来队头阻塞问题：同时发出的请求必须按顺序接收，如果第一个请求被阻塞了，则后面的请求即使处理完毕了，也需要等待。 http2.0的多路复用完美解决了这个问题。一个request对应一个stream并分配一个id，这样一个连接上可以有多个stream，每个stream的frame可以随机的混杂在一起，接收方可以根据stream id将frame再归属到各自不同的request里面。 http2.0还可以为每个stream设置优先级（Priority）和依赖（Dependency）。优先级高的stream会被server优先处理和返回给客户端，stream还可以依赖其它的sub streams。优先级和依赖都是可以动态调整的。动态调整在有些场景下很有用，假想用户在用你的app浏览商品的时候，快速的滑动到了商品列表的底部，但前面的请求先发出，如果不把后面的请求优先级设高，用户当前浏览的图片要到最后才能下载完成，而如果设置了优先级，则可以先加载后面的商品，体验会好很多。 二进制分帧HTTP 1.x在应用层以纯文本的形式进行通信，而HTTP 2.0将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。二进制格式的好处在于解析更快，而且文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合，因此健壮性更好。 服务端推送服务端可以在发送页面HTML时主动推送其它资源，而不用等到浏览器解析到相应位置，发起请求再响应。例如服务端可以主动把JS和CSS文件推送给客户端，而不需要客户端解析HTML时再发送这些请求。 服务端可以主动推送，客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送RST_STREAM帧来拒收。主动推送也遵守同源策略，服务器不会随便推送第三方资源给客户端。 QUICHTTP2.0虽然大大增加了并发性，但还是有问题的。因为HTTP2.0也是基于TCP协议的，TCP协议在处理包时是有严格顺序的。 当其中一个数据包遇到问题，TCP连接需要等待这个包完成重传之后才能继续进行。虽然HTTP2.0通过多个stream，使得逻辑上一个TCP连接上的并行内容，进行多路数据的传输，然而这中间没有关联的数据。一前一后，前面stream2的帧没有收到，后面stream1的帧也会因此阻塞。 于是，就有了从TCP切换到UDP的时候。这就是Google的QUIC协议。 机制一：自定义连接机制我们都知道，一条TCP连接是由四元组标识的，分别是源IP、源端口、目的IP、目的端口。一旦一个元素发生变化时，就需要断开重连，重新连接。在移动互联的情况下，当手机信号不稳定或者在WIFI和移动网络切换时，都会导致重连，从而进行再次的三次握手，导致一定的时延。 QUIC使用一个64位的随机数作为ID来标识，由于UDP是无连接的，所以当IP或者端口发生变化时，只要ID不变，就不需要重新建立连接。 机制二：自定义重传机制TCP为了保证可靠性，通过使用序号和应答机制，来解决顺序问题和丢包问题。 任何一个序号的包发过去，都要在一定的时间内得到应答，否则一旦超时，就会重发。超时时间是通过采样往返时间RTT不断调整的。这就会带来一个采样不准确的问题。例如：发送一个包，序号为100，发现没有返回，于是再发送一个100，过一阵返回一个ACK101。这个时候客户端知道这个包肯定收到了，但往返时间是多少呢？是ACK达到的时间减去后一个100发送的时间，还是减去前一个100发送的时间呢？ QUIC也有个序列号，是递增的。任何一个序列号的包只发送一次，下次就要加一了。例如，发送一个包，序号是100，发现没有返回；再次发送的时候，序号就是101了；如果返回的ACK100，就是对第一个包的响应，如果返回ACK101就是对第二个包的响应，RTT计算相对准确。 但是这里有一个问题，就是这么知道包100和包101发送的是同样的内容呢？QUIC定义了一个stream offset的概念。QUIC既然面向连接，也就像TCP一样，是一个数据流，发送的数据在这个数据流里面有个偏移量stream offset，可以通过stream offset查看数据发送到了哪里，这样只要这个stream offset的包没有来，就要重发；如果来了，按照stream offset拼接，还是能够拼成一个流。 无阻塞的多路复用有了自定义的连接和重传机制，我们可以解决上面HTTP2.0的多路复用问题。 同HTTP2.0一样，同一条QUIC连接上可以创建多个stream，来发送多个HTTP请求。但是，QUIC是基于UDP的，一个连接上的多个stream之间没有依赖。这样，假如stream2丢了一个UDP包，后面跟着stream3的一个UDP包，虽然stream2的那个包需要重传，但是stream3的包无需等待，就可以发给用户。 自定义流量控制TCP的流量控制是通过滑动窗口协议。QUIC的流量控制也是通过window_update，来告诉对端它可以接受的字节数。但是QUIC的窗口是适应自己的多路复用机制的，不但在一个连接上控制窗口，还在一个连接中的每个stream控制窗口。 在TCP协议中，接受端的窗口的起始点是下一个要接收并且ACK的包，即使后来的包都到了，放在缓存里，窗口也不能右移，因为TCP的ACK机制是基于系列号的累计应答，一旦ACK了一个系列号，就说明前面的都到了，所以只要前面的没到，后面的到了也不能ACK，就会导致后面的到了，也有可能超时重传，浪费带宽。 QUIC的ACK是基于offset的，每个offset的包来了，进了缓存，就可以应答，应答后就不会重发，中间的空档会等待到来或者重发即可，而窗口的起始位置为当前收到的最大offset，从这个offset到当前的stream所能容纳的最大缓存，是真正的窗口大小。显然，这样更加准确。 另外，还有整个连接的窗口，需要对于所有的stream的窗口做一个统计。","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/tags/计算机网络/"}],"keywords":[]},{"title":"InnoDB中的锁和MVCC","slug":"InnoDB中的锁和MVCC","date":"2019-01-27T13:20:11.000Z","updated":"2019-03-08T13:14:48.417Z","comments":true,"path":"2019/01/27/InnoDB中的锁和MVCC/","link":"","permalink":"http://yoursite.com/2019/01/27/InnoDB中的锁和MVCC/","excerpt":"","text":"锁和MVCC是MySQL控制并发访问的两种手段。InnoDB在MySQL的基础上提供了更细粒度的行级锁，使用了next-key算法解决了幻读的问题。另外InnoDB提供了一套基于MVCC的一致性非锁定读方式，实现了“读不加锁，读写不冲突”的快照读方式。 锁1、表锁InnoDB直接沿用了ＭySQL提供的表锁。事实上，表锁的加锁和解锁都是在MySQL server层面的，和存储引擎没有关系。加锁的方式如下：123LOCK TABLES orders READ; // 加读锁SELECT SUM(total) FROM orders;UNLOCK TABLES; // 解锁 2、行级锁顾名思义，行级锁锁定的是某一行数据。InnoDB中的所有数据项都保存在聚簇索引中，所以行级锁实质上锁住的是索引项。如果表中有多个索引存在，一行数据会对应到多个索引项，此时行级锁会锁住所有索引上的相应索引项。 行级锁分为共享锁（S锁）和排他锁（X锁）。S锁和S锁可以兼容；S锁和X锁，X锁和X锁不能兼容。（InnoDB存储引擎默认采用行级锁，所以下文如无指明，S锁和X锁均指行锁） 3、意向锁在没有引入意向锁之前，行级锁和表锁之间的兼容有点麻烦：如果要对一张表加X表锁，那么首先要判断这张表是否加了X表锁和S表锁，其次要判断每一行是否加了X锁和S锁，如果表的行数比较多的话，这种判断方式会比较损失性能。因此InnoDB引入了意向锁。 和行锁一样，意向锁也分为意向共享锁（IS锁）和意向排他锁（IX锁）。事务在申请S或X锁之前，必须先申请到IS或IX锁。InnoDB中的意向锁是一种特殊的表锁：意向锁之间互不冲突，意向锁和表锁之间会冲突，此时意向锁相当于同类型的表锁。 MVCCMVCC是一种非锁定读的一致性读机制。它的特点是读不加锁，读写不冲突。InnoDB利用undo log实现了MVCC。undo log的数据结构如图所示： 前四行是数据列，后三列是隐藏列。隐藏列的含义如下： DB_ROW_ID：行ID。占7字节，他就项自增主键一样随着插入新数据自增。如果表中不存主键或者唯一索引，那么数据库就会采用DB_ROW_ID生成聚簇索引。否则DB_ROW_ID不会出现在索引中。 DB_TRX_ID：事务ID。占6字节，表示这一行数据最后插入或修改的事务id。此外删除在内部也被当作一次更新，在行的特殊位置添加一个删除标记（记录头信息有一个字节存储是否删除的标记）。 DB_ROLL_PTR：回滚指针。占7字节，每次对数据进行更新操作时，都会copy当前数据，保存到undo log中。并修改当前行的回滚指针指向undo log中的旧数据行。 MVCC只有在隔离级别是READ COMMITED和REPEATABLE READ两个隔离级别下工作。MVCC可以通过比较数据行的事务ID和当前事务ID来判断该记录是否对当前事务可见。但仅有undo log还不够。试想这样一种情况：事务599开始——事务600开始——事务600查询了表A——事务599更新了表A——事务599提交——事务600再次查询表A。可知事务600的两次查询会得到不同的结果，无法满足RR隔离级别的要求。这是因为事务599的事务ID虽然比事务600小，但事务599还未结束，仍有可能改变数据项的值。 read viewInnoDB使用了read view解决了这个问题。read view是一张表，记录了当前活跃的事务ID。InnoDB在查询时会先对比数据行的事务ID和read view中的事务ID。具体如下： 如果数据行事务ID大于read view中最大的ID，表示数据行一定是在当前事务之后修改的，对当前事务不可见； 如果数据行事务ID小于read view中最小的ID，表示数据行一定是在当前事务开始之前修改并且已提交，所以对当前事务可见。 如果数据行事务ID落在read view最大最小ID的区间中，则要判断数据行事务ID和当前事务ID的关系： 如果数据行事务ID不在活跃事务数组中，表示该事务已提交，此时和当前事务ID比较，若小于则可见，大于则不可见； 如果数据行事务ID在活跃事务数组中，表示该事务未提交，这里要判断一下数据行事务ID是否为当前事务ID，若是，虽然未提交但同一事务内的修改可见，若不是，则不可见。 当前数据行若是不可见，InnoDB会沿着DB_ROLL_PTR往下查找，直到找到第一个可见的数据行或者null。 可见与否只是第一步，实际返回的数据还要经过判断。因为删除和更新共用一个字段，区别只是删除有一个字节的删除标记，那么在返回的时候InnoDB就要判断当前的数据行是否被标记为删除。如果标记了删除，就不会返回。 MVCC在READ COMMITED和REPEATABLE READ两个隔离级别下共用一套逻辑，区别只是在于RC隔离级别是在读操作开始时刻创建read view的，而RR隔离级别是在事务开始时刻，确切地说是第一个读操作创建read view的。由于这一点上的不同，使得MVCC在RC隔离级别下读取的是最新提交的数据，而RR隔离级别下读取的是事务开始前提交的数据。 next-key解决幻读问题幻读是指一个事务内连续进行两次相同的SQL语句可能导致不同的结果，第二次的SQL语句可能会返回之前不存在的行。发生这种现象的原因是事务A两次查找的间隔中事务B插入了一条或多条数据并提交，导致事务A的第二次查询查到了新插入的数据。 InnoDB中有三种锁算法： Record Lock：单个行记录上的锁 Gap Lock：间隙锁，锁住一个范围，但不包含记录本身 Next-Key Lock：Record Lock+Gap Lock，锁定一个范围，并且锁定记录本身 幻读现象问题的根本原因是Record Lock只锁住记录本身而不锁范围，导致其它事务可以在记录间插入数据。InnoDB使用了Next-Key Lock来解决这个问题。Next-Key Lock会锁住一个范围，例如一个索引有10,11,13,20这四个值，那么该索引可能被Next-Key Locking的区间为：(-∞,10](10,11](11,13](13,20](20,+∞) 当然，如果查询的索引是唯一索引的话，就不用担心被插入的问题，InnoDB会对Next-Key Lock进行优化，将其降级为Record Lock。 innodb加锁处理分析以上是理论部分，那么实际innodb怎么加锁呢？我们结合SQL语句来分析。 在支持MVCC并发控制的系统中，读操作可以分成两类：快照读 (snapshot read)与当前读 (current read)。快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。 快照读： select * from table where ?; 当前读： select * from table where ? lock in share mode; select * from table where ? for update; insert into table values (…); update table set ? where ?; delete from table where ?; 简而言之，所有的插入/更新/删除操作都是当前读，且都加了X锁；读取操作默认是快照读，但可以声明加X锁或者S锁。 一条简单SQL的加锁实现分析我们拿一条SQL语句：delete from t1 where id = 10; 来分析innodb的加锁情况。但光有这一条SQL是不够的，我们还需要知道一些前提： 前提1：id列是不是主键？ 前提2：当前系统的隔离级别是什么？ 前提3：如果id列不是主键，那么id列上有索引吗？ 前提4：如果id列上有二级索引，那么这个索引是唯一索引吗？ 基于这些前提的不同，我们可以组合出以下几种情况（隔离级别只考虑RC和RR的情况）： 组合1：id列是主键，RC隔离级别 组合2：id列是二级唯一索引，RC隔离级别 组合3：id列是二级非唯一索引，RC隔离级别 组合4：id列上没有索引，RC隔离级别 组合5：id列是主键，RR隔离级别 组合6：id列是二级唯一索引，RR隔离级别 组合7：id列是二级非唯一索引，RR隔离级别 组合8：id列上没有索引，RR隔离级别 组合1：id列是主键，RC隔离级别这个组合，是最简单，最容易分析的组合。id是主键，Read Committed隔离级别，给定SQL：delete from t1 where id = 10; 只需要将主键上，id = 10的记录加上X锁即可。如下图所示： id是主键时，此SQL只需要在id=10这条记录上加X锁即可。 组合2：id列是二级唯一索引，RC隔离级别这个组合，id不是主键，而是一个unique的二级索引键值。那么在RC隔离级别下，delete from t1 where id = 10; 需要加什么锁呢？见下图： 此组合中，id是unique索引，而主键是name列。此时，加锁的情况由于组合一有所不同。由于id是unique索引，因此delete语句会选择走id列的索引进行where条件的过滤，在找到id=10的记录后，首先会将unique索引上的id=10索引记录加上X锁，同时，会根据读取到的name列，回主键索引(聚簇索引)，然后将聚簇索引上的name = ‘d’ 对应的主键索引项加X锁。为什么聚簇索引上的记录也要加锁？试想一下，如果并发的一个SQL，是通过主键索引来更新：update t1 set id = 100 where name = ‘d’; 此时，如果delete语句没有将主键索引上的记录加锁，那么并发的update就会感知不到delete语句的存在，违背了同一记录上的更新/删除需要串行执行的约束。 组合3：id列是二级非唯一索引，RC隔离级别相对于组合一、二，组合三又发生了变化，隔离级别仍旧是RC不变，但是id列上的约束又降低了，id列不再唯一，只有一个普通的索引。假设delete from t1 where id = 10; 语句，仍旧选择id列上的索引进行过滤where条件，那么此时会持有哪些锁？同样见下图： 可以看到，首先，id列索引上，满足id = 10查询条件的记录，均已加锁。同时，这些记录对应的主键索引上的记录也都加上了锁。与组合二唯一的区别在于，组合二最多只有一个满足等值查询的记录，而组合三会将所有满足查询条件的记录都加锁。 组合4：id列上没有索引，RC隔离级别相对于前面三个组合，这是一个比较特殊的情况。id列上没有索引，where id = 10;这个过滤条件，没法通过索引进行过滤，那么只能走全表扫描做过滤。对应于这个组合，SQL会加什么锁？或者是换句话说，全表扫描时，会加什么锁？这个答案也有很多：有人说会在表上加X锁；有人说会将聚簇索引上，选择出来的id = 10;的记录加上X锁。那么实际情况呢？请看下图： 由于id列上没有索引，因此只能走聚簇索引，进行全部扫描。从图中可以看到，满足删除条件的记录有两条，但是，聚簇索引上所有的记录，都被加上了X锁。无论记录是否满足条件，全部被加上X锁。既不是加表锁，也不是在满足条件的记录上加行锁。 有人可能会问？为什么不是只在满足条件的记录上加锁呢？这是由于MySQL的实现决定的。如果一个条件无法通过索引快速过滤，那么存储引擎层面就会将所有记录加锁后返回，然后由MySQL Server层进行过滤。因此也就把所有的记录，都锁上了。 组合5：id列是主键，RR隔离级别上面的四个组合，都是在Read Committed隔离级别下的加锁行为，接下来的四个组合，是在Repeatable Read隔离级别下的加锁行为。 组合五，id列是主键列，Repeatable Read隔离级别，针对delete from t1 where id = 10; 这条SQL，加锁与组合一：[id主键，Read Committed]一致。 组合6：id列是二级唯一索引，RR隔离级别与组合五类似，组合六的加锁，与组合二：[id唯一索引，Read Committed]一致。两个X锁，id唯一索引满足条件的记录上一个，对应的聚簇索引上的记录一个。 组合7：id列是二级非唯一索引，RR隔离级别组合七，Repeatable Read隔离级别，id上有一个非唯一索引，执行delete from t1 where id = 10; 假设选择id列上的索引进行条件过滤，最后的加锁行为，是怎么样的呢？同样看下面这幅图： 此图，相对于组合三多了一个GAP锁，这是因为RR级别区别于RC级别的一点是RR级别要防止幻读。我们在前一节讲过innodb基于Next-Lock防止幻读，而Next-Lock就是GAP Lock+Record Lock。加在索引上的是Record Lock，而在中间的就是GAP Lock。 那么为什么组合五、组合六，也是RR隔离级别，却不需要加GAP锁呢？这是因为组合五，id是主键；组合六，id是unique键，都能够保证唯一性。一个等值查询，最多只能返回一条记录，而且新的相同取值的记录，一定不会再新插入进来，因此也就避免了GAP锁的使用。 组合8：id列上没有索引，RR隔离级别组合八，Repeatable Read隔离级别下的最后一种情况，id列上没有索引。此时SQL：delete from t1 where id = 10; 没有其他的路径可以选择，只能进行全表扫描。最终的加锁情况，如下图所示：","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"TCP协议回顾","slug":"TCP协议回顾","date":"2019-01-23T13:28:32.000Z","updated":"2019-03-08T13:16:57.782Z","comments":true,"path":"2019/01/23/TCP协议回顾/","link":"","permalink":"http://yoursite.com/2019/01/23/TCP协议回顾/","excerpt":"","text":"本科的时候面向考试学习计算机网络，TCP是重点中的重点，可惜当时不知道TCP在网络世界中的重要，考完试就把知识点扔了。最近在复习计算机网络，看到TCP这章，就像看到了一个老朋友。就想把它记录下来，便于复习。 TCP协议非常复杂，标准也非常多，但核心内容就以下几个部分： 三次握手 四次挥手 可靠性传输 流量控制 拥塞控制 TCP协议头要想了解TCP协议首先就必须了解TCP的协议头： 首先，源端口号和目标端口号是不可少的，这一点和 UDP 是一样的。如果没有这两个端口号。 数据就不知道应该发给哪个应用。 接下来是包的序号和确认序号。为了保证消息的顺序性到达，TCP给每个包编了一个序号。初始序号在建立连接时指定，此后每个包的序号为上一个包的序号+上一个包的字节数。服务器会返回一个确认号，表示这个序号之前的包都收到了。 然后是一些状态位。例如SYN是发起一个连接，ACK是回复，RST是重新连接，FIN是结束连接。TCP是面向连接的，这些带状态位的包可以改变双方的状态。 窗口大小是跟流量控制有关。TCP是全双工的协议，通信双方都会维护一个缓存空间。这个窗口大小就是告诉对方我还有多少剩余的缓存空间。 三次握手TCP是面向连接的协议，所以在建立连接前有一系列的动作，被称为三次握手： 一开始，客户端和服务端都处于CLOSED状态。先是服务端主动监听某个端口，处于LISTEN状态。然后客户端主动发起连接SYN，之后处于SYN-SENT状态。服务端收到发起的连接，返回SYN，并且ACK客户端的SYN，之后处于SYN-RCVD状态。客户端收到服务端发送的SYN和ACK之后，发送ACK的ACK，之后处于ESTABLISHED状态，因为它一发一收成功了。服务端收到ACK的ACK之后，处于 ESTABLISHED状态，因为它也一发一收了。 三次握手除了双方建立连接外，还要沟通一件事情，就是TCP包的序号的问题。 双方在发送SYN包的时候，各自需要指定一个针对这次连接的序号seq。这个seq实质上可以看出一个32位的计时器，每4ms加一。为什么要这么做呢？主要是为了防止在网络中被延迟的分组在以后被重复传输，而导致某个连接的一端对它作错误的判断。如果序号不按这种方式分配，而是从1开始，则会出现这样的情况：AB建立连接之后，A发送了1,2,3三个包，然后掉线了。由于网络的原因三个包没有到达B，在网络中游荡。然后A重连了，序号重新从1开始，他又发送了1,2两个包，但没有发送3号包。此时上一次连接发送的3号包却到达了B，B以为是A这次发送的，就产生了误判。为了避免这种情况的发生，TCP协议规定了这种方式生成初始seq。以这种方式生成的初始seq，需要4个多小时才会重复，此时早已过了3号包的生存时间（TTL）。 为什么要三次握手而不是两次？ 原因一：服务器会收到客户端很早以前发送的，但因为延迟导致现在才到达的SYN报文。如果不采用三次握手，则服务器会认为新的连接已经建立，会白白浪费缓存等资源。原因二：三次握手需要交流双方的初始序号seq，服务器发送的第二次握手是针对客户端在第一次握手中约定的客户端初始seq的确认，客户端的第三次握手是针对服务器在第二次握手中约定的服务器初始seq的确认。如果没有第三次握手，万一服务器的SYN包丢了，那么客户端无法得知服务器的初始序号，此时客户端就没法接收服务器的包，因为客户端没法辨别这个包是本次连接中发送的，还是上一次连接中发送的。 SYN洪泛攻击 因为服务器在收到一个SYN报文后，会初始化连接变量和缓存，如果攻击方会发送大量SYN报文，而不完成第三次握手，那么就会导致服务器的连接资源被消耗殆尽。针对SYN洪泛攻击有一种有效的防御手段，称为SYN cookie：当服务器接收到一个SYN报文时，它不知道这是来自一个合法的请求还是SYN洪泛攻击的一部分。所以它不会为其分配资源，而是将该报文中的源、目的IP地址和端口和服务器自己的秘密数做哈希，将（秒级时间（5位）+最长分段大小（3位）+哈希（24位））作为初始seq返回给客户端。如果是一个合法用户，会返回一个ACK包，服务器可以通过将ACK包中的源、目的IP地址和端口，和ACK-1对比，得知这是否是一个合法的SYN确认包。然后可以通过秒级时间确定这是否是一个新的包。如果是新的包且合法，服务器才会分配资源。这样就有效防止了SYN洪泛攻击的发生。 四次挥手有建立连接，必然也有断开连接。断开连接的动作被称为四次挥手： 区别于三次握手，四次挥手的发起方可以是客户端也可以是服务端。因此不区分客户端和服务端而是用AB代替。 一开始，A和B都处于ESTABLISHED的状态。然后A发送FIN表示请求断开连接，之后B处于FIN-WAIT-1的状态。 B在接收到FIN请求后，会发送一个ACK包表示收到了FIN请求，之后B处于CLOSED-WAIT状态。需要注意的是此时B发送的是ACK包而不是FIN包，之所以不像三次握手一样直接回应一个FIN包，是因为此时B可能还有些事情没有做完，还可能发送数据，所以称为半关闭状态。 这个时候A可以选择不再接收数据，也可以选择最后再接收一段数据，等待B也主动关闭。不论如何，A在收到ACK包后都进入了FIN-WAIT2阶段。此时如果B下线，A将永远在这个状态。TCP协议里没有对这个状态的处理，但Linux有，可以调整tcp_fin_timeout这个参数，设置一个超时时间。 B处理完了所有的事情，终于也准备关闭，此时会发送一个FIN包。之后B进入LAST-ACK状态，等待A的ACK包。 A在收到服务器的FIN包后会发送一个ACK包表示收到了B的FIN包。按理说此时A就可以关闭了，但由于A最后的ACK存在丢包的可能，B没有收到最后的ACK包的话，就会重发一个FIN包，如果这时候A关闭了，B就再也收不到ACK了。因而TCP协议要求A最后等待一段时间TIME_WAIT，这个时间要足够长，长到B没收到ACK的话，重发的FIN包还能到达A。 A直接关闭还有一个问题是，A的端口就直接空出来了，但是B不知道，B原来发过的很多包很可能还在路上，如果A的端口被一个新的应用占用了，这个新的应用会收到上个连接中B发过来的包，虽然序列号是重新生成的，但是这里要上一个双保险，防止产生混乱，因而也需要等足够长的时间，等到原来B发送的所有的包都过期了，再空出端口来。 等待的时间设为2MSL，MSL是Maximum Segment Lifetime（报文最大生存时间），它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为TCP报文基于是IP协议的， 而IP头中有一个TTL域，是IP数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为0则数据报将被丢弃，同时发送ICMP报文通知源主机。协议规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。 服务器大量CLOSE_WAIT\\TIME_WAIT状态的原因 如果服务器出现异常，百分之八九十都是下面两种情况：1、服务器保持了大量CLOSE_WAIT状态。产生的原因在于：TCP Server 已经ACK了过来的FIN数据包，但是上层应用程序迟迟没有发命令关闭Server到client 端的连接。所以TCP一直在那等啊等…..所以说如果发现自己的服务器保持了大量的CLOSE_WAIT，问题的根源十有八九是自己的server端程序代码的问题。2、服务器保持了大量TIME_WAIT状态。产生的原因在于：服务器处理大量高并发短连接并主动关闭连接时容易出现TIME_WAIT积压。这是因为关闭的发起方在TIME_WAIT阶段需要等待1-4分钟才能回收资源。如果连接过多将导致资源来不及回收。解决方案是修改Linux内核，允许将TIME-WAIT sockets重新用于新的TCP连接，并开启TCP连接中TIME-WAIT sockets的快速回收，这些默认都是关闭的。 可靠性传输TCP协议为了保证顺序性，每一个包都有一个ID。在建立连接的时候，会商定起始的ID是什么，然后按照ID一个个发送。为了保证不丢包，对于发送的包都要进行应答，但是这个应答也不是一个一个来的，而是会应答某个之前的ID，表示都收到了，这种模式称为累计确认或者累计应答（cumulative acknowledgment）。 为了记录所有发送的包和接收的包，TCP也需要发送端和接收端分别都有缓存来保存这些记录。发送端的缓存里是按照包的ID一个个排列，根据处理的情况分成四个部分。 第一部分：发送了并且已经确认的。 第二部分：发送了并且尚未确认的。 第三部分：没有发送，但是已经等待发送的。 第四部分：没有发送，并且暂时还不会发送的。 为什么会有三、四部分的区分呢？这是因为接受端有个处理极限，就是剩余缓冲区的大小，如果给接收端发送的包的大小超过了剩余缓冲区的大小，那么有一部分包就会被丢弃，这是不合适的。所以超出剩余缓冲区大小的包，发送端暂时不会发。 于是，发送端需要保持下面的数据结构： 对于接收端来讲，它的缓存里记录的内容要简单一些： 第一部分：接收并且确认过的。 第二部分：还没接收，但尚在接收能力范围之内的。 第三部分：还没接收，超过能力范围的。 对应的数据结构像这样： 顺序与丢包问题还是刚才的图，在发送端来看，1、2、3已经发送并确认；4、5、6、7、8、9都是发送了还没确认；10、11、12是还没发出的；13、14、15是接收方没有空间，不准备发的。 在接收端来看，1、2、3、4、5是已经完成ACK，但是没读取的；6、7是等待接收的；8、9是已经接收，但是没有ACK的。 发送端和接收端当前的状态如下： 1、2、3没有问题，双方达成了一致。 4、5接收方说ACK了，但是发送方还没收到，有可能丢了，有可能在路上。 6、7、8、9肯定都发了，但是8、9已经到了，但是6、7没到，出现了乱序，缓存着但是没办法ACK。 根据这个例子，我们可以知道，顺序问题和丢包问题都有可能发生，所以我们先来看确认与重发的机制。 假设4的确认到了，不幸的是，5的ACK丢了，6、7的数据包丢了，这该怎么办呢？ 一种方法就是超时重试，TCP会为当前最小未应答的包绑定一个定时器，当收到ACK时会重启定时器，并绑定到现在最小未应答的包上。若当前无未应答包，则关闭定时器。等下一个包发出去后再启动并绑定到新的包上。 如何设置往返时间RTT呢？TCP采用了加权的自适应重传算法：EstimatedRTT=0.875EstimatedRTT+0.125SampleRTT 其中EstimatedRTT为平均往返时间，SampleRTT为某次采样的往返时间。 如果过一段时间，5、6、7都超时了，就会重新发送。接收方发现5原来接收过，于是丢弃5；6收到了，发送ACK，要求下一个是7，7不幸又丢了。当7再次超时的时候，有需要重传的时候，TCP的策略是超时间隔加倍。每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。 超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？ 有一个叫快速重传的机制：当接收方收到一个序号大于下一个所期望的报文段时，就检测到了数据流中的一个间格，于是发送三个冗余的ACK，客户端收到后，就在定时器过期之前，重传丢失的报文段。 例如，接收方发现6、8、9都已经接收了，就是7没来，那肯定是丢了，于是发送三个6的ACK，要求下一个是7。客户端收到3个，就会发现7的确又丢了，不等超时，马上重发。 还有一种方式称为Selective Acknowledgment （SACK）。这种方式需要在TCP头里加一个SACK 的东西，可以将缓存的地图发送给发送方。例如可以发送ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是7丢了。 流量控制流量控制是为了平衡发送端与接收端的速度，避免出现包处理不过来的情况。在协议头里面，有一个窗口大小字段，这个就是用来进行流量控制的。 我们先假设窗口不变的情况，窗口始终为9。4的确认来的时候，会右移一个，这个时候第13个包也可以发送了。 这个时候，假设发送端发送过猛，会将第三部分的10、11、12、13全部发送完毕，之后就停止发送了，未发送可发送部分0。 当对于包5的确认到达的时候，在客户端相当于窗口再滑动了一格，这个时候，才可以有更多的包可以发送了，例如第14个包才可以发送。 如果接收方实在处理的太慢，导致缓存中没有空间了，可以通过确认信息修改窗口的大小，甚至可以设置为0，则发送方将暂时停止发送。 我们假设一个极端情况，接收端的应用一直不读取缓存中的数据，当数据包6确认后，窗口大小就不能再是9了，就要缩小一个变为8。 这个新的窗口8通过6的确认消息到达发送端的时候，你会发现窗口没有平行右移，而是仅仅左面的边右移了，窗口的大小从9改成了8。 如果接收端还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，直到为0。 当这个窗口通过包14的确认到达发送端的时候，发送端的窗口也调整为0，停止发送。 如果这样的话，发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满 了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。 拥塞控制拥塞控制是为了避免网络中传输着太多的包导致网络拥挤。这里有一个公式，即：发送但还未确认的包要小于等于滑动窗口（rwnd）和拥塞窗口（cwnd）的最小值。前者在流量控制中已经讲过，剩下的就是后者。 拥塞控制有三个时期：慢启动、拥塞避免和快速恢复。 当开始时，cwnd设置为一个报文段，一次只能发送一个；当收到这一个确认的时候，cwnd加一，于是一次能发送两个；当这两个确认到来的时候，每个确认cwnd加一，两个确认 cwnd 加二，于是一次能发送四个。当这四个的确认到来的时候，每个确认cwnd加一，四个确认cwnd加四，于是一次能够发送八个。依次类推，此时，cwnd的增长速度是指数型的增长。 涨到什么时候是个头呢？有一个值ssthresh初始为65535个字节，当超过这个值的时候，就进入了拥塞避免状态。此时不再是一个确认对应一个cwnd的增长，而是一个确认对应1/cwnd的增长。我们接着上面的过程来，一次发送八个，当八个确认到来的时候，每个确认增加1/8，八个确认一共cwnd增加1，于是一次能够发送九个，变成了线性增长。 但cwnd不可能无限增长，总有一个时候网络会拥挤，拥挤的表现形式是丢包。发送端有两种方式感知到丢包：超时和收到三个相同ACK（快速重传）。针对这两种情况的丢包，发送端的处理方式也不一样。 第一种是超时丢包，这种情况下，发送端会认为当前网络非常拥挤，因此会采取激进的限制措施：将sshresh设为cwnd/2，将cwnd设为1，重新开始慢启动。 第二种是三个相同ACK丢包，发送端会认为这个丢包是个偶然事件，因此网络并不非常拥挤，采取的措施也会温和一些：sshresh设为cwnd/2，cwnd设为cwnd/2，又因为返回了三个确认包，cwnd再加3。之后进入快速恢复阶段，因为当前cwnd仍在比较高的值，这个阶段中cwnd也是线性增长。 两种方式的比较如下： 总结TCP协议的核心部分在于：三次握手、四次挥手、可靠性传输、流量控制、拥塞控制，掌握好这些知识点对网络编程很有帮助。","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/tags/计算机网络/"}],"keywords":[]},{"title":"插入缓冲、两次写和自适应哈希索引","slug":"插入缓冲、两次写和自适应哈希索引","date":"2019-01-14T03:56:25.000Z","updated":"2019-03-08T13:17:07.054Z","comments":true,"path":"2019/01/14/插入缓冲、两次写和自适应哈希索引/","link":"","permalink":"http://yoursite.com/2019/01/14/插入缓冲、两次写和自适应哈希索引/","excerpt":"","text":"InnoDB是MySQL数据库从5.5.8版本开始的默认存储引擎，它将数据放在一个逻辑的表空间中，这个表空间就像黑盒一样由InnoDB存储引擎自身进行管理。从MySQL 4.1开始，它可以将每个InnoDB的表单独存在一个独立的idb文件中。此外，InnoDB支持用裸设备（raw disk，不被操作系统管理的设备）建立其表空间。 InnoDB通过使用多版本并发控制（MVCC）来获得高并发性，并且实现了SQL标准的四种隔离级别，默认为REPEATABLE级别。同时，使用一种称为next-key locking的策略来避免幻读。除此之外，InnoDB还提供了插入缓冲、二次写、自适应哈希索引等高性能和高可用的功能。 缓冲池功能的实现离不开底层的配合。为了协调CPU速度与磁盘速度之间的鸿沟，InnoDB在内存中开辟了一块空间叫内存池，将对数据库的修改首先保存在内存中。比如对于页的操作，首先会在内存中进行，然后后台线程会把脏页（还没有刷入磁盘的页）刷入磁盘。 缓冲池中缓存的数据页类型有：索引页、数据页、undo页、插入缓冲、自适应哈希索引、InnoDB存储的锁信息、数据字典信息等。不能简单地认为，缓冲池只是缓存索引页和数据页，它们只是占缓冲池很大的一部分而已。 中点插入策略缓冲池对于数据页的管理，是使用LRU的方式管理的。和传统的LRU稍有不同的是，InnoDB使用一种称为“中点插入策略”的方式插入数据： 新页的第一次插入只会插入到LRU链表尾端3/8（中点）的位置 页的再次命中（LRU上的页被命中）才会把页插入到链表头部 可以设置一个InnoDB_old_blocks_time的参数，表示新页插入后过多久才有资格被插入到链表头部 这样做的好处是避免了一些冷数据对真正热点数据的干扰。比如进行扫描操作时，需要访问表中的许多页，甚至是全部页，而这些页通常来说只在这次查询中需要，并不是活跃的热点数据。如果页被放入LRU链表头部，那么非常可能将所需的真正热点数据刷出。InnoDB_old_blocks_time也是出于同样的目的。可以避免临近的几次查询把页刷入热数据的情况。 插入缓冲InnoDB底层使用聚簇索引管理数据。在进行插入操作的时候，数据页的存放是按主键顺序存放的，此时磁盘顺序访问，速度会很快。但对于非聚集索引叶子节点的插入则不再是顺序的了，这时需要离散地访问非聚集索引页，磁盘的随机读取效率很低，导致了插入操作的性能下降。 InnoDB存储引擎创造性地设计了Insert Buffer，对于非聚簇索引的插入或更新操作，不是每一次直接插入到索引页中，而是先判断插入的非聚簇索引页是否在缓冲池中，若在，则直接插入；若不在，则先放入到一个Insert Buffer对象中。然后再以一定的频率和情况进行Insert Buffer和辅助索引叶子节点的merge操作，这时通常能将多个插入合并到一个操作中（因为在一个索引页中），这就大大提高了对于非聚簇索引插入的性能。 然而Insert Buffer的使用需要同时满足以下两个条件： 索引是辅助索引。 索引不是唯一的。（因为在插入缓冲时，数据库并不去查找索引页来判断插入的记录的唯一性。如果去查找肯定又会有离散读取的发生，就背离了Insert Buffer的初衷） Insert Buffer的数据结构是一颗B+树，4.1版本之前每张表都有一颗Insert Buffer B+树，4.1版本之后所有表共用一颗B+树。Insert Buffer B+树的非叶子节点存放的是查询的search key，其构造如图： 其中space表示待插入记录所在表的表空间id，在InnoDB存储引擎中，每个表有一个唯一的space id，可以通过space id查询得知是哪张表；maker是用来兼容老版本的Insert Buffer。offerset表示页所在的偏移量。 当一个辅助索引要插入到页（space,offset）时，如果这个页不在缓冲池中，那么InnoDB存储引擎首先根据上述规则构造一个search key，接下来查询Insert Buffer这棵B+树，然后再将记录插入到Insert Buffer B+树的叶子节点中。 两次写如果说Insert Buffer带给InnoDB存储引擎的是性能上的提升，那么doublewrite（两次写）带给InnoDB存储引擎的是数据页的可靠性。 InnoDB中有记录（Row）被更新时，先将其在Buffer Pool中的page更新，并将这次更新记录到Redo Log file中，这时候Buffer Pool中的该page就是被标记为Dirty。在适当的时候（Buffer Pool不够、Redo不够，系统闲置等），这些Dirty Page会被Checkpoint刷新到磁盘进行持久化操作。 但尴尬的地方在于InnoDB的Page Size是16KB，其数据校验也是针对这16KB来计算的，将数据写入到磁盘是以Page为单位进行操作的，而文件系统是以4k为单位写入，磁盘IO的最小单位是512K，因此并不能保证数据页的写入就是原子性的。 那么可不可以通过redo log来进行恢复呢？答案是只能恢复校验完整（还没写）的页，不能恢复已损坏的页。比如某次checkpoint要刷入4个数据页，其中第一页写了2KB，后三页还未写。那么根据redo log可以恢复后三页，但已经写了2KB的页没法恢复，因为没法知道在宕机前第一页到底写了多少。 为什么redo log不需要doublewrite的支持？ 因为redolog写入的单位就是512字节，也就是磁盘IO的最小单位，所以无所谓数据损坏。 double write由两部分组成，一部分是内存中的doublewrite buffer，大小为2MB，另一部分是物理磁盘上共享表空间中连续的128个页，即2个区，大小同样为2MB。在对缓冲池的脏页进行刷新时，并不直接写磁盘，而是会通过memcpy函数将脏页先复制到内存中的doublewrite buffer，之后通过doublewrite buffer再分两次，每次1MB顺序地写入共享表空间的物理磁盘上，然后马上调用fsync函数，同步磁盘。在这个过程中，因为doublewrite页是连续的，因此这个过程是顺序写的，开销不是很大。其工作流程如下图所示： 现在我们来分析一下为什么double write可以生效。当宕机发生时，有那么几种情况：1、磁盘还未写，此时可以通过redo log恢复；2、磁盘正在进行从内存到共享表空间的写，此时数据文件中的页还没开始被写入，因此也同样可以通过redo log恢复；3、磁盘正在写数据文件，此时共享表空间已经写完，可以从共享表空间拷贝页的副本到数据文件实现恢复。 自适应哈希索引哈希是一种非常快的查询方法，一般只需要一次查找就能定位数据。InnoDB存储引擎会监控对表上各索引页的查询，如果观察到建立哈希索引可以带来速度提升，则建立哈希索引，称之为自适应哈希索引。 自适应哈希索引有一个要求，即对这个页的连续访问模式必须是一样的。例如对于（a,b）这样的联合索引页，其访问模式可以是以下情况： WHERE a=xxx WHERE a=xxx and b=xxx 若交替以上两种查询，那么InnoDB存储引擎不会对该页构造哈希索引（这是因为哈希索引是以索引的哈希值为键值存放的，hash(a)和hash(a,b)是两个完全不同的值） 在连续的查询模式一样的条件下，如果能满足以下条件，InnoDB存储引擎就会创建相应的哈希索引： 以该连续模式连续访问了100次 以该模式连续访问了 页中记录总数/16 次 哈希索引只能用来搜索等值的查询，如SELECT * FROM table WHERE index_col=’xxx’。对于其它类型的查找，如范围查找，是不能使用哈希索引的。 InnoDB存储引擎官方文档显示，启用AHI后,读取和写入速度可以提高2倍，辅助索引的连接操作性能可以提高5倍。 总结InnoDB存储引擎在MySQL原有的基础上做了很多优化，主要涉及到的就是缓冲池和磁盘的交互。尽可能多地读缓存，尽量少地读磁盘，于是有了自适应哈希索引；尽量多地顺序写，尽量少地离散写，于是有了插入缓冲；由于缓存的易失性，带来的数据恢复问题，又有了两次写。这些设计思想不只可以用于数据库，也可以用于程序设计的方方面面。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"Java NIO浅析","slug":"Java-NIO浅析","date":"2019-01-09T04:10:18.000Z","updated":"2019-03-08T13:15:07.773Z","comments":true,"path":"2019/01/09/Java-NIO浅析/","link":"","permalink":"http://yoursite.com/2019/01/09/Java-NIO浅析/","excerpt":"","text":"NIO（Non-blocking I/O），是一种同步非阻塞的I/O模型，也是I/O多路复用的基础，已经被越来越多地应用到大型应用服务器，成为解决高并发与大量连接、I/O处理问题的有效方式。Java中的NIO是jdk 1.4之后新出的一套IO接口，相比传统IO(BIO)，两者有如下区别： IO是面向流的，NIO是面向缓冲区的 IO流是同步阻塞的，NIO流是同步非阻塞的 NIO有选择器（Selector），IO没有 IO的流是单向的，NIO的通道（Channel）是双向的 IO基本概念Linux的内核将所有外部设备都可以看做一个文件来操作。那么我们对与外部设备的操作都可以看做对文件进行操作。我们对一个文件的读写，都通过调用内核提供的系统调用；内核给我们返回一个file descriptor（fd,文件描述符）。对一个socket的读写也会有相应的描述符，称为socketfd(socket描述符）。描述符就是一个数字(可以理解为一个索引)，指向内核中一个结构体（文件路径，数据区，等一些属性）。应用程序对文件的读写就通过对描述符的读写完成。 一个基本的IO，它会涉及到两个系统对象，一个是调用这个IO的进程对象，另一个就是系统内核(kernel)。当一个read操作发生时，它会经历四个阶段： 1、通过read系统调用想内核发起读请求。 2、内核向硬件发送读指令，并等待读就绪。 3、内核把将要读取的数据复制到描述符所指向的内核缓存区中。 4、将数据从内核缓存区拷贝到用户进程空间中。 同步和异步同步和异步关注的是消息通信机制 (synchronous communication / asynchronous communication)。所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。 而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 阻塞和非阻塞阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态。阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 常见I/O模型对比所有的系统I/O都分为两个阶段：等待就绪和操作。举例来说，读函数，分为等待系统可读和真正的读；同理，写函数分为等待网卡可以写和真正的写。需要说明的是等待就绪的阻塞是不使用CPU的，是在“空等”；而真正的读写操作的阻塞是使用CPU的，真正在”干活”，而且这个过程非常快，属于memory copy，带宽通常在1GB/s级别以上，可以理解为基本不耗时。 以socket.read()为例子：传统的BIO里面socket.read()，如果TCP RecvBuffer里没有数据，函数会一直阻塞，直到收到数据，返回读到的数据。对于NIO，如果TCP RecvBuffer有数据，就把数据从网卡读到内存，并且返回给用户；反之则直接返回0，永远不会阻塞。最新的AIO(Async I/O)里面会更进一步：不但等待就绪是非阻塞的，就连数据从网卡到内存的过程也是异步的。换句话说，BIO里用户最关心“我要读”，NIO里用户最关心”我可以读了”，在AIO模型里用户更需要关注的是“读完了”。NIO一个重要的特点是：socket主要的读、写、注册和接收函数，在等待就绪阶段都是非阻塞的，真正的I/O操作是同步阻塞的（消耗CPU但性能非常高）。 传统BIO模型分析了解NIO就要从传统BIO的弊端说起。 在传统的BIO中，一旦用户线程发起IO请求，则必须要等内核将数据报准备好，才能将数据从内核复制到用户空间。这是一种效率很低的方式。传统的BIO一般要配合线程池来使用，我们的编程范式（伪代码）一般是这样的： 123456789101112131415161718192021222324ExecutorService executor = Excutors.newFixedThreadPollExecutor(100); // 线程池ServerSocket serverSocket = new ServerSocket();serverSocket.bind(8088);while(!Thread.currentThread.isInturrupted())&#123; // 主线程死循环等待新连接到来 Socket socket = serverSocket.accept(); executor.submit(new ConnectIOnHandler(socket)); // 为新的连接创建新的线程&#125;class ConnectIOnHandler extends Thread&#123; private Socket socket; public ConnectIOnHandler(Socket socket)&#123; this.socket = socket; &#125; public void run()&#123; while(!Thread.currentThread.isInturrupted()&amp;&amp;!socket.isClosed())&#123; // 死循环处理读写事件 String someThing = socket.read()....// 读取数据 if(someThing!=null)&#123; ......//处理数据 socket.write()....// 写数据 &#125; &#125; &#125;&#125; 这是一个经典的每连接每线程的模型，之所以使用多线程，主要原因在于socket.accept()、socket.read()、socket.write()三个主要函数都是同步阻塞的，当一个连接在处理I/O的时候，系统是阻塞的，如果是单线程的话必然就挂死在那里；但CPU是被释放出来的，开启多线程，就可以让CPU去处理更多的事情。其实这也是所有使用多线程的本质： 利用多核。 当I/O阻塞系统，但CPU空闲的时候，可以利用多线程使用CPU资源。 现在的多线程一般都使用线程池，可以让线程的创建和回收成本相对较低。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的I/O并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。 不过，这个模型最本质的问题在于，严重依赖于线程。但线程是很”贵”的资源，主要表现在： 线程的创建和销毁成本很高，在Linux这样的操作系统中，线程本质上就是一个进程。创建和销毁都是重量级的系统函数。 线程本身占用较大内存，像Java的线程栈，一般至少分配512K～1M的空间，如果系统中的线程数过千，恐怕整个JVM的内存都会被吃掉一半。 线程的切换成本是很高的。操作系统发生线程切换的时候，需要保留线程的上下文，然后执行系统调用。如果线程数过高，可能执行线程切换的时间甚至会大于线程执行的时间，这时候带来的表现往往是系统load偏高、CPU sy使用率特别高（超过20%以上)，导致系统几乎陷入不可用的状态。 容易造成锯齿状的系统负载。因为系统负载是用活动线程数或CPU核心数，一旦线程数量高但外部网络环境不是很稳定，就很容易造成大量请求的结果同时返回，激活大量阻塞线程从而使系统负载压力过大。 所以，当面对十万甚至百万级连接的时候，传统的BIO模型是无能为力的。随着移动端应用的兴起和各种网络游戏的盛行，百万级长连接日趋普遍，此时，必然需要一种更高效的I/O处理模型。 NIO是如何工作的 这是一个NIO基本的工作方式（但不常用），我们把一个套接口设置为非阻塞，当所请求的I/O操作不能满足要求时候，不把本进程投入睡眠，而是返回一个错误。也就是说当数据没有到达时并不等待，而是以一个错误返回。 事件驱动的I/O复用模型（常用）在BIO的场景下，为了避免线程长时间阻塞在等待内核准备上，我们选择了每连接每线程的方式。但在NIO的场景下，如果当前的连接没有准备好，可以选择下一个连接。比如我们的聊天程序，我们可以建立两个连接：一个发送端，一个接收端。程序会不断轮询这两个连接，如果接收端有数据达到，那就把它显示在屏幕上；如果发送端有数据发出，那就把它发出。但如果接收端没有数据，或者发送端的网卡没有准备好，程序也不会停下来，而是继续轮询，直到有一方准备好。这种一个进程/线程处理多个IO的方式，被称为I/O复用模型。 而如果我们把发送就绪和接收就绪当成两类事件，只有在这两类事件发生的时候才会触发轮询，其它时候（比如等待请求时），程序不会被唤醒，那么这种方式就被称为事件驱动。 Linux中的select,poll,epoll是典型的事件驱动的I/O复用模型： select()会把所有的I/O请求封装为文件描述符(fd)的形式给操作系统，让操作系统内核去查询这些套接字上是否有事件发生，轮询完后，再将句柄数据复制到用户态，让服务器应用程序轮询处理已发生的网络事件，以此来实现一个线程/进程管理多个I/O的功能。 poll()在select上支持更多数量的fd。因为select中使用数组形式存放文件描述符，数量有限（一般1024个），poll使用链表的形式，理论上支持的描述符数量没有上限。 epoll()在select/poll的基础上有了大幅改进： 它使用红黑树来存储所有需要查询的事件，事件的添加和删除对应红黑树的插入和删除，复杂度从O(N)降为了O(logN)。 它使用双向链表来保存就绪的事件。所有添加到红黑树上的事件都会与设备(网卡)驱动程序建立回调关系，当相应的事件发生时会调用这个回调方法，回调方法会把事件放入双向链表中。 返回时返回的是就绪事件（双向链表）而不是所有事件，既减少了内核到用户空间的拷贝数据量，又省了用户程序筛选就绪事件的时间。 相比select/poll的水平触发模式，epoll也支持边沿触发模式。即用户可以选择到底是接受所有就绪的事件（水平触发），还是只接受上次检查以后新就绪的事件（边沿触发）。 Java中的NIO模型Java中的NIO模型选用了事件驱动的I/O复用模型。事实上，在Linux上Java的NIO就是基于select,poll,epoll来实现的（Linux 2.6之前是select、poll，2.6之后是epoll）。 在Java的NIO中，有4类事件：读就绪（OP_READ），写就绪（OP_WRITE），收到请求（仅服务端有效，OP_ACCEPT），发出请求（仅客户端有效，OP_CONNECT）。我们需要注册当这几个事件到来的时候所对应的处理器。然后在合适的时机告诉事件选择器：我对这个事件感兴趣。对于写操作，就是写不出去的时候对写事件感兴趣；对于读操作，就是完成连接和系统没有办法承载新读入的数据的时；对于accept，一般是服务器刚启动的时候；而对于connect，一般是connect失败需要重连或者直接异步调用connect的时候。新事件到来的时候，会在selector上注册标记位，标示可读、可写或者有连接到来。编程范式（伪代码）一般如下： 12345678910111213141516171819202122232425262728 //处理器抽象接口interface ChannelHandler&#123; void channelReadable(Channel channel); void channelWritable(Channel channel);&#125;class Channel&#123; Socket socket; Event event;//读，写或者连接&#125;Map&lt;Channel，ChannelHandler&gt; handlerMap;//所有channel的对应事件处理器//IO线程主循环:class IoThread extends Thread&#123; public void run()&#123; Channel channel; while(channel=Selector.select())&#123;//选择就绪的事件和对应的连接 if(channel.event==accept)&#123; registerNewChannelHandler(channel);//如果是新连接，则注册一个新的读写处理器 &#125; if(channel.event==write)&#123; getChannelHandler(channel).channelWritable(channel);//如果可以写，则执行写事件 &#125; if(channel.event==read)&#123; getChannelHandler(channel).channelReadable(channel);//如果可以读，则执行读事件 &#125; &#125; &#125;&#125; Buffer的选择Java中的NIO还有一个特点是面向缓冲区的。这一特性其实在传统IO中就有用到，这里不再赘述。但是Buffer的选择也是一个值得注意的地方。 通常情况下，操作系统的一次写操作分为两步： 1. 将数据从用户空间拷贝到系统空间。 2. 从系统空间往网卡写。同理，读操作也分为两步： ① 将数据从网卡拷贝到系统空间； ② 将数据从系统空间拷贝到用户空间。 对于NIO来说，缓存的使用可以使用DirectByteBuffer和HeapByteBuffer。如果使用了DirectByteBuffer，一般来说可以减少一次系统空间到用户空间的拷贝。但Buffer创建和销毁的成本更高，更不宜维护，通常会用内存池来提高性能。如果数据量比较小的中小应用情况下，可以考虑使用heapBuffer；反之可以用directBuffer。 使用NIO != 高性能，当连接数&lt;1000，并发程度不高或者局域网环境下NIO并没有显著的性能优势。 NIO并没有完全屏蔽平台差异，它仍然是基于各个操作系统的I/O系统实现的，差异仍然存在。使用NIO做网络编程构建事件驱动模型并不容易，陷阱重重。 推荐大家使用成熟的NIO框架，如Netty，MINA等。解决了很多NIO的陷阱，并屏蔽了操作系统的差异，有较好的性能和编程模型。 总结最后总结一下Java中的NIO为我们带来了什么： 非阻塞I/O，I/O读写不再阻塞，而是返回0 避免多线程，单个线程可以处理多个任务 事件驱动模型 基于block的传输，通常比基于流的传输更高效 IO多路复用大大提高了Java网络应用的可伸缩性和实用性","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"Java 线程池浅析","slug":"Java-线程池浅析","date":"2018-09-15T05:58:26.000Z","updated":"2019-03-08T13:15:16.376Z","comments":true,"path":"2018/09/15/Java-线程池浅析/","link":"","permalink":"http://yoursite.com/2018/09/15/Java-线程池浅析/","excerpt":"","text":"Java中的线程池是运用场景最多的并发框架，几乎所有需要异步或并发执行任务的程序都可以使用线程池。在开发过程中，合理地使用线程池能够带来3个好处。 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。 线程池的继承关系如下图： 线程池最顶层是Executor，这是只有一个execute方法的接口，也是整个Executor框架的顶层接口，所有Executor框架的组件都要实现这个接口。 ExecutorService继承了Executor，在此基础上增加了submit(Runnable) 和submit(Callable)，表示任务的提交，Runnable和Callable的区别在于Callable的call()方法有返回值，而Runnable的run没有。 ThreadPoolExecutor是线程池的核心实现类，大部分线程池的功能都在这个类中被定义，它有多个参数和构造函数，根据不同的构造参数可以实现不同功能的线程池。线程池的参数会在下文详细介绍。 Executors是ThreadPoolExecutor的工厂类，封装了一些常用的线程池，具体类型也会在下文详细介绍。 线程池基本概念创建一个线程池我们可以通过ThreadPoolExecutor来创建一个线程池： 1new ThreadPoolExecutor(corePoolSize,maximumPoolSize,keepAliveTime,unit,workQueue,ThreadFactory,RejectedExecutionHandler) 线程池的构造函数中需要接收7个参数，它们分别是： corePoolSize 核心线程数，指保留的线程池大小（不超过maximumPoolSize值时，线程池中最多有corePoolSize个线程工作） maximumPoolSize 指的是线程池的最大大小（线程池中最大有maximumPoolSize个线程可运行） keepAliveTime 指的是空闲线程结束的超时时间（当一个线程不工作时，过keepAliveTime长时间将停止该线程） unit 是一个枚举，表示keepAliveTime的单位（有NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS，7个可选值） workQueue 表示存放任务的队列（存放需要被线程池执行的线程队列）。它的类型是BlockingQueue就是阻塞队列，有关阻塞队列的内容可以参考这篇《阻塞队列源码阅读》 threadFactory 是一个线程工厂，负责线程的创建，一般会使用默认的Executors.defaultThreadFactory()。 handler 拒绝策略（添加任务失败后如何处理该任务） 线程池的运行策略线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。我们可以使用execute()方法提交任务到线程池： 123456executorService.execute(new Runnable() &#123; @Override public void run() &#123; //TODO &#125;&#125;); 也可以使用submit()方法提交任务到线程池： 123456Future future = executorService.submit(new Runnable() &#123; @Override public void run() &#123; //TODO &#125;&#125;); 区别在于submit()会返回一个Future对象，通过这个Future对象可以判断任务是否执行成功，并且可以通过Future的get()方法来获取返回值。另外继承了ExecutorService接口的ScheduledExecutorService还可以使用schedule()方法来提交一个定时任务： 123456scheduledExecutorService.schedule(new Runnable() &#123; @Override public void run() &#123; //TODO &#125;&#125;, 1, TimeUnit.SECONDS); 上面代码就会在1秒后执行我们的定时任务。无论是submit()还是schedule()，其底层最后都会调用execute()来提交执行任务。不过，就算队列里面有任务，线程池也不会马上执行它们。 当添加一个任务时，线程池会做如下判断： 如果正在运行的线程数量小于corePoolSize，那么马上创建线程运行这个任务； 如果正在运行的线程数量大于或等于corePoolSize，那么将这个任务放入队列； 如果这时候队列满了，而且正在运行的线程数量小于maximumPoolSize，那么还是要创建线程运行这个任务； 如果队列满了，而且正在运行的线程数量大于或等于maximumPoolSize，那么线程池会调用reject()，这个方法会调用handler.rejectedExecution()方法，根据不同的handler策略会有不同的处理方式。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到corePoolSize的大小。 线程池的拒绝策略上面提到任务添加失败后，线程池会调用reject()方法，这个方法会调用handler.rejectedExecution()方法，根据不同的handler策略会有不同的处理方式。线程池中预设有以下几种处理方式： AbortPolicy：为Java线程池默认的阻塞策略，不执行此任务，而且直接抛出一个运行时异常，切记ThreadPoolExecutor.execute需要try catch，否则程序会直接退出。 DiscardPolicy：直接抛弃，任务不执行，空方法。 DiscardOldestPolicy：从队列里面抛弃head的一个任务，并再次execute此task。 CallerRunsPolicy：还给原线程自己执行，会阻塞入口。 用户自定义拒绝策略：实现RejectedExecutionHandler，并自己定义策略模式。 关闭线程池Java线程池提供了两个方法用于关闭一个线程池，一个是shutdownNow()，另一个是shutdown()。我们可以看一下这两个方法的声明： 12void shutdown();List&lt;Runnable&gt; shutdownNow(); 这两个方法的区别在于： shutdown()：当线程池调用该方法时，线程池的状态则立刻变成SHUTDOWN状态。我们不能再往线程池中添加任何任务，否则将会抛出RejectedExecutionException异常；但是，此时线程池不会立刻退出，直到添加到线程池中的任务都已经处理完成后才会退出。 shutdownNow()：当执行该方法，线程池的状态立刻变成STOP状态，并试图停止所有正在执行的线程，不再处理还在池队列中等待的任务，并以返回值的形式返回那些未执行的任务。此方法会通过调用Thread.interrupt()方法来试图停止正在运行的Worker线程，但是这种方法的作用有限，如果线程中没有sleep 、wait、Condition、定时锁等操作时，interrupt()方法是无法中断当前的线程的。所以，shutdownNow()并不代表线程池就一定立即就能退出，可能必须要等待所有正在执行的任务都执行完成了才能退出。 Executors提供的线程池ThreadPoolExecutor提供的线程创建方式参数太多，对开发人员并不友好。因此Java在Executors类中封装了几种常用的线程池，它们分别是： Executors.newCachedThreadPool 这是一个会根据需要创建线程的线程池，它的corePoolSize被设置为0，maximumPoolSize被设置为Integer.MAX_VALUE，KeepAliveTime被设置为60s，使用没有容量的 SynchronousQueue作为线程池的工作队列。这就意味着，线程池中没有固定的线程数量，任何一个任务被提交时，线程池都会为它创建或者分配一个线程；而任何一个线程空闲时间超过60s，都会关闭它。使用该线程池时要注意主线程提交任务的速度和线程池处理任务的速度，若提交速度大于处理速度，CachedThreadPool会因为创建过多线程而耗尽CPU和内存资源。该线程池的吞吐量在几种预设线程池中是最大的。 Executors.newFixedThreadPool 这是被称为可重用固定线程数的线程池，它的corePoolSize等于 maximumPoolSize，KeepAliveTime被设置为0，使用最大长度的有界队列LinkedBlockingQueue（队列容量为Integer.MAX_VALUE）作为工作队列，这也意味着FixedThreadPool运行稳定后线程数量是不变的，且所有任务都会进入工作队列，不会拒绝任务。 Executors.newSingleThreadExecutor 这是一个只有一个工作线程的线程池，它的corePoolSize和maximumPoolSize都被设置为1，其它参数与FixedThreadPool相同，可以把它理解为newFixedThreadPool(1)，线程执行完任务后会无限反复从LinkedBlockingQueue获取任务来执行。 Executors.newScheduledThreadPool 这是一个可以定时执行的线程池，它的maximumPoolSize被设置为 Integer.MAX_VALUE，KeepAliveTime被设置为10ms，使用DelayedWorkQueue作为阻塞队列，这是一个类似于DelayedQueue和PriorityBlockingQueue的阻塞队列，每次会取出队列中执行时间最早的任务，如果没有到执行之间，则await两者的差值，以此来达到定时执行的目的。同时newScheduledThreadPool的scheduleAtFixedRate和scheduleWithFixedRate方法还可以实现周期执行的功能。两者都是依靠给ScheduledFutureTask（newScheduledThreadPool中被执行的任务）设置下一次执行时间来实现的。区别在于scheduleAtFixedRate中下次执行时间=本次开始时间+间隔时间，而scheduleWithFixedRate中下次执行时间=本次结束时间+间隔时间。 线程池代码分析线程池的属性字段在开始深入了解ThreadPoolExecutor代码之前, 我们先来简单地看一下ThreadPoolExecutor类中到底有哪些重要的字段。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class ThreadPoolExecutor extends AbstractExecutorService &#123; // 这个是一个复用字段, 它复用地表示了当前线程池的状态, 当前线程数信息. private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); // 用于存放提交到线程池中, 但是还未执行的那些任务. private final BlockingQueue&lt;Runnable&gt; workQueue; // 线程池内部锁, 对线程池内部操作加锁, 防止竞态条件 private final ReentrantLock mainLock = new ReentrantLock(); // 一个 Set 结构, 包含了当前线程池中的所有工作线程. // 对 workers 字段的操作前, 需要获取到这个锁. private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); // 条件变量, 用于支持 awaitTermination 操作 private final Condition termination = mainLock.newCondition(); // 记录线程池中曾经到达过的最大的线程数. // 这个字段在获取 mainLock 锁的前提下才能操作. private int largestPoolSize; // 记录已经完成的任务数. 仅仅当工作线程结束时才更新此字段. // 这个字段在获取 mainLock 锁的前提下才能操作. private long completedTaskCount; // 线程工厂. 当需要一个新的线程时, 这里生成. private volatile ThreadFactory threadFactory; // 任务提交失败后的处理 handler private volatile RejectedExecutionHandler handler; // 空闲线程的等待任务时间, 以纳秒为单位. // 当当前线程池中的线程数大于 corePoolSize 时, // 或者 allowCoreThreadTimeOut 为真时, 线程才有 idle 等待超时时间, // 如果超时则此线程会停止.; // 反之线程会一直等待新任务到来. private volatile long keepAliveTime; // 默认为 false. // 当为 false 时, keepAliveTime 不起作用, 线程池中的 core 线程会一直存活, // 即使这些线程是 idle 状态. // 当为 true 时, core 线程使用 keepAliveTime 作为 idle 超时 // 时间来等待新的任务. private volatile boolean allowCoreThreadTimeOut; // 核心线程数. private volatile int corePoolSize; // 最大线程数. private volatile int maximumPoolSize;&#125; ThreadPoolExecutor中, 使用到ctl这个字段来维护线程池中当前线程数和线程池的状态。ctl是一个AtomicInteger类型, 它的低29位用于存放当前的线程数，因此一个线程池在理论上最大的线程数是536870911；高3位是用于表示当前线程池的状态，其中高三位的值和状态对应如下： 111: RUNNING 此时能够接收新任务，以及对已添加的任务进行处理。状态切换：线程池初始化时就是RUNNING状态。 000: SHUTDOWN 此时不接收新任务，但能处理已添加的任务。状态切换：调用线程池的shutdown()接口时，线程池由 RUNNING -&gt; SHUTDOWN。 001: STOP 此时不接收新任务，不处理已添加的任务，并且会中断正在处理的任务。状态切换：调用线程池的shutdownNow()接口时，线程池由 (RUNNING or SHUTDOWN ) -&gt; STOP。 010: TIDYING 当所有的任务已终止，ctl记录的”任务数量”为0，线程池会变为TIDYING状态。当线程池变为TIDYING状态时，会执行钩子函数terminated()。terminated()在ThreadPoolExecutor类中是空的，若用户想在线程池变为TIDYING时，进行相应的处理；可以通过重载terminated()函数来实现。状态切换：当线程池在SHUTDOWN状态下，阻塞队列为空并且线程池中执行的任务也为空时，就会由 SHUTDOWN -&gt; TIDYING。当线程池在STOP状态下，线程池中执行的任务为空时，就会由 STOP -&gt; TIDYING。 011: TERMINATED 线程池彻底终止，就变成TERMINATED状态。状态切换：线程池处在TIDYING状态时，执行完terminated()之后，就会由 TIDYING -&gt; TERMINATED。 提交任务到线程池1234567891011121314151617181920public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command);&#125; 上面的代码有三个步骤，首先第一步是检查当前线程池的线程数是否小于corePoolSize，如果小于，那么由我们前面提到的规则，线程池会创建一个新的线程来执行此任务，因此在第一个if语句中，会调用addWorker(command, true)来创建一个新Worker线程，并执行此任务。 如果当前线程池的线程数不小于corePoolSize，那么会尝试将此任务插入到工作队列中，即workQueue.offer(command)。当插入到workQueue成功后，我们还需要再次检查一下此时线程池是否还是RUNNING状态，如果不是的话就会将原来插入队列中的那个任务删除，然后调用reject方法拒绝此任务的提交；接着考虑到在我们插入任务到workQueue中的同时，如果此时线程池中的线程都执行完毕并终止了，在这样的情况下刚刚插入到workQueue中的任务就永远不会得到执行了。为了避免这样的情况，因此我们要再次检查一下线程池中的线程数，如果为零，则调用addWorker(null, false)来添加一个线程。 最后如果任务插入到工作队列失败了，就会直接调用addWorker(command, false)来新开一个线程。如果失败了，那么我们就知道线程池已经关闭或者饱和了，就拒绝这次添加。 关于addWorker方法前面我们大体分析了一下execute提交任务的流程，不过省略了一个关键步骤，即addWorker方法。现在我们来看看这个方法里究竟发生了什么。 首先看一下addWorker方法的签名：1private boolean addWorker(Runnable firstTask, boolean core) 这个方法接收两个参数，第一个是一个Runnable类型的参数，一般来说是我们调用execute方法所传输的参数，不过也有可能是null值，这样的情况我们在前面一小节中也见到过。那么第二个参数是做什么的呢？第二个参数是一个boolean类型的变量，它的作用是标识是否使用corePoolSize属性。我们知道，ThreadPoolExecutor中，有一个corePoolSize属性，用于规定线程池中的核心线程数。那么当core这个参数是true时，则表示在添加新任务时，需要考虑到corePoolSzie的影响（例如如果此时线程数已经大于corePoolSize了，那么就不能再添加新线程了）；当core为false时，就不考虑corePoolSize的影响，而是以maximumPoolSize代替corePoolSize来做判断条件。 然后是addWorker的源码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (int c = ctl.get();;) &#123; // Check if queue empty only if necessary. if (runStateAtLeast(c, SHUTDOWN) &amp;&amp; (runStateAtLeast(c, STOP) || firstTask != null || workQueue.isEmpty())) return false; for (;;) &#123; // 当 core 为真, 那么就判断当前线程是否大于 corePoolSize // 当 core 为假, 那么就判断当前线程数是否大于 maximumPoolSize // 这里的 for 循环是一个自旋CAS(CompareAndSwap)操作, 用于确保多线程环境下的正确性 if (workerCountOf(c) &gt;= ((core ? corePoolSize : maximumPoolSize) &amp; COUNT_MASK)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateAtLeast(c, SHUTDOWN)) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int c = ctl.get(); if (isRunning(c) || (runStateLessThan(c, STOP) &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 首先在addWorker的一开始，有一个for循环，用于判断当前是否可以添加新的Worker线程。它的逻辑如下： 如果传入的core为真，那么判断当前的线程数是否大于corePoolSize，如果大于，则不能新建Worker线程，返回false。 如果传入的core为假，那么判断当前的线程数是否大于maximumPoolSize，如果大于，则不能新建Worker线程，返回false。 如果条件符合，那么在for循环内，又有一个自旋CAS更新逻辑，用于递增当前的线程数，即compareAndIncrementWorkerCount(c)，这个方法会原子地更新ctl的值，将当前线程数的值+1。addWorker接下来有一个try…finally语句块，这里就是实际上的创建线程、启动线程、添加线程到线程池中的工作了。首先可以看到w = new Worker(firstTask)；这里是实例化一个Worker对象，这个类其实就是ThreadPoolExecutor中对工作线程的封装。Worker类继承于AbstractQueuedSynchronizer并实现了Runnable接口，我们来看一下它的构造器： 12345Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this);&#125; 它会把我们提交的任务（firstTask）设置为自己的内部属性firstTask，然后使用ThreadPoolExecutor中的 threadFactory来创建一个新的线程，并保存在thread字段中，而且注意到，创建线程时，我们传递给新线程的Runnable其实是Worker对象本身（this），因此当这个线程启动时，实际上运行的是Worker.run()中的代码。 回过头来再看一下addWorker方法。当创建好Worker线程后，就会将这个worker线程存放在workers这个HashSet类型的字段中。而且注意到，正如我们在前面所提到的，mainLock是ThreadPoolExecutor的内部锁，我们对ThreadPoolExecutor中的字段进行操作时，为了保证线程安全，需要在获取到mainLock的前提下才能操作。 最后，我们可以看到，在addWorker方法的最后，调用了t.start()；来真正启动这个新建的线程。 任务的分配与调度线程池在执行完firstTask后并不会立即销毁，而是可以根据情况复用。线程的复用就涉及到任务的分配与调度。Java线程池的调度方式很简单，就是执行完之后从workQueue中拿出下一个任务，如果获取到了任务，那就再次执行。 前一小节中，我们看到addWorker中会新建一个Worker对象来代表一个worker线程，接着会调用线程的start()来启动这个线程，我们也提到了当启动这个线程后，会运行到Worker中的run方法，我们来看一下Worker.run具体的实现：123public void run() &#123; runWorker(this);&#125; Worker.run方法很简单，只是调用了ThreadPoolExecutor.runWorker方法而已。runWorker方法比较关键，它是整个线程池任务分配的核心：12345678910111213141516171819202122232425262728293031323334353637383940414243final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125; runWorker方法是整个工作线程的核心循环，在这个循环中，工作线程会不断的从workerQuque中获取新的task，然后执行它。我们注意到在runWorker一开始，有一个w.unlock()，咦, 这是为什么呢? 其实这是Worker类玩的一个小把戏。回想一下，Worker类继承于AQS并实现了Runnable接口，它的构造器如下：12345Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this);&#125; setState(-1) 方法是AQS提供的，初始化Worker时，会先设置state为 -1，根据注释，这样做的原因是为了抑制工作线程的interrupt信号，直到此工作线程开始执行task。那么在addWorker中的w.unlock()就是允许Worker的interrupt信号。 接着在addWorker中会进入一个while循环，在这里此工作线程会不断地从workQueue中取出一个任务，然后调用 task.run()来执行这个任务，因此就执行到了用户所提交的Runnable中的run()方法了。 工作线程的idle超时处理工作线程的idle超出处理在底层依赖于BlockingQueue带超时的poll方法，即工作线程会不断地从workQueue这个BlockingQueue中获取任务，如果allowCoreThreadTimeOut字段为true，或者当前的工作线程数大于corePoolSize，那么线程的idle超时机制就生效了，此时工作线程会以带超时的poll方式从workQueue中获取任务。当超时了还没有获取到任务，那么我们就知道此线程已经到达idle超时时间了，就终止此工作线程。具体源码如下：12345678910111213141516171819202122232425262728293031323334353637private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; 从源码中就可以看到，一开始会判断当前的线程池状态，如果不是SHUTDOWN或STOP之类的状态，那么接着获取当前的工作线程数，然后判断工作线程数量是否已经大于了corePoolSize。当allowCoreThreadTimeOut字段为true，或者当前的工作线程数大于corePoolSize，那么线程的idle超时机制就生效，此时工作线程会以带超时的workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS)方式从workQueue中获取任务；反之会以workQueue.take()方式阻塞等待任务，直到获取一个新的任务。当从workQueue获取新任务超时时，会调用compareAndDecrementWorkerCount将当前的工作线程数-1，并返回null。getTask方法返回null后， runWorker中的while循环自然也就结束了，因此也导致了runWorker方法的返回，最后自然整个工作线程的run() 方法执行完毕，工作线程自然就终止了。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"Java 阻塞队列浅析","slug":"Java-阻塞队列浅析","date":"2018-09-03T13:58:36.000Z","updated":"2019-03-08T13:15:31.946Z","comments":true,"path":"2018/09/03/Java-阻塞队列浅析/","link":"","permalink":"http://yoursite.com/2018/09/03/Java-阻塞队列浅析/","excerpt":"","text":"阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 一、Java中的阻塞队列JDK7中提供了7个阻塞队列，分别是： ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。 LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。 PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。 DelayQueue：一个使用优先级队列实现的无界阻塞队列。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 ArrayBlockingQueueArrayBlockingQueue是一个用数组实现的有界阻塞队列。此队列按照先进先出（FIFO）的原则对元素进行排序。默认情况下不保证访问者公平的访问队列，所谓公平访问队列是指阻塞的所有生产者线程或消费者线程，当队列可用时，可以按照阻塞的先后顺序访问队列，即先阻塞的生产者线程，可以先往队列里插入元素，先阻塞的消费者线程，可以先从队列里获取元素。ArrayBlockingQueue的公平性是由ReentrantLock实现的，公平模式下的ReentrantLock在有一个线程申请锁时，会检测AQS队头是否有等待的线程，如果有则执行队头的线程，而非公平模式下则不会进行检测。通常情况下为了保证公平性会降低吞吐量。 12345678public ArrayBlockingQueue(int capacity, boolean fair) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); // ReentrantLock实现了公平策略 notEmpty = lock.newCondition(); notFull = lock.newCondition();&#125; LinkedBlockingQueueLinkedBlockingQueue是一个用链表实现的有界阻塞队列。此队列的默认最大长度为Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序。 PriorityBlockingQueuePriorityBlockingQueue是一个支持优先级的无界队列，底层由数组实现，数组最大长度为Integer.MAX_VALUE - 8（所以并不是真的无界），默认数组大小为11，如果元素超过数组大小，数组会进行扩容，具体扩容的大小，在数组长度小于64时+2，大于64时变为150%，当数组扩容后大小超过Integer.MAX_VALUE - 8时，不会再用150%扩容，而是使用+1的方式扩容。最后若+1的方式扩容的数组大小也超过了Integer.MAX_VALUE - 8，则报OutOfMemoryError()。默认情况下元素采取自然顺序，排列采用二叉树最小堆实现（所以有序不是数据存储的有序，而是取出数据的有序），也可以通过比较器comparator来指定元素的排序规则。元素按照升序排列。 DelayQueueDelayQueue是一个支持延时获取元素的无界阻塞队列。队列使用PriorityQueue，PriorityQueue和PriorityBlockingQueue一样都是使用小顶堆实现的，唯一的区别在于PriorityQueue是非线程安全的，而PriorityBlockingQueue是线程安全的。队列中的元素必须实现Delayed接口（该接口有两个方法getDelay()和compareTo()），在创建元素时可以指定多久才能从队列中获取当前元素。只有在延迟期满时才能从队列中提取元素。我们可以将DelayQueue运用在以下应用场景： 缓存系统的设计：可以用DelayQueue保存缓存元素的有效期，使用一个线程循环查询DelayQueue，一旦能从DelayQueue中获取元素时，表示缓存有效期到了。 定时任务调度。使用DelayQueue保存当天将会执行的任务和执行时间，一旦从DelayQueue中获取到任务就开始执行，从比如TimerQueue就是使用DelayQueue实现的。 compareTo()接口实现示例队列中的Delayed必须实现compareTo来指定元素的顺序。比如让延时时间最长的放在队列的末尾。实现代码如下：123456789101112131415161718public int compareTo(Delayed other) &#123; if (other == this) // compare zero ONLY if same object return 0; if (other instanceof ScheduledFutureTask) &#123; ScheduledFutureTask x = (ScheduledFutureTask)other; long diff = time - x.time; if (diff &lt; 0) return -1; else if (diff &gt; 0) return 1; &#125;else if (sequenceNumber &lt; x.sequenceNumber)&#123; return -1; &#125;else&#123; return 1; &#125; long d = (getDelay(TimeUnit.NANOSECONDS) - other.getDelay(TimeUnit.NANOSECONDS)); return (d == 0) ? 0 : ((d &lt; 0) ? -1 : 1);&#125; getDelay()接口实现示例getDelay()可以查询当前元素还需要延时多久，需要在对象创建时传入一个开始时间。以ScheduledThreadPoolExecutor里ScheduledFutureTask类为例。这个类实现了Delayed接口。123456ScheduledFutureTask(Runnable r, V result, long ns, long period) &#123; super(r, result); this.time = ns; // 这个time就是开始时间 this.period = period; this.sequenceNumber = sequencer.getAndIncrement();&#125; 然后使用getDelay可以查询当前元素还需要延时多久，代码如下：123public long getDelay(TimeUnit unit) &#123; return unit.convert(time - now(), TimeUnit.NANOSECONDS);&#125; 最后是延时队列的使用，当消费者从队列里获取元素时，如果元素没有达到延时时间，就阻塞当前线程。12345long delay = first.getDelay(TimeUnit.NANOSECONDS);if (delay &lt;= 0) return q.poll();else if (leader != null) available.await(); SynchronousQueueSynchronousQueue是一个不存储元素的阻塞队列。每一个put操作必须等待一个take操作，否则不能继续添加元素。SynchronousQueue可以看成是一个传球手，负责把生产者线程处理的数据直接传递给消费者线程。队列本身并不存储任何元素，非常适合于传递性场景,比如在一个线程中使用的数据，传递给另外一个线程使用，SynchronousQueue的吞吐量高于LinkedBlockingQueue 和 ArrayBlockingQueue。 LinkedTransferQueueLinkedTransferQueue是一个由链表结构组成的无界阻塞TransferQueue队列。TransferQueue队列继承了BlockingQueue，在BlockingQueue的基础上多了tryTransfer和transfer方法。transfer会在元素进入阻塞队列前，先判断有没有消费者线程在等待获取，若有，则直接移交；否则将元素插入到队列尾部。tryTransfer相比transfer少了插入的步骤，多了返回值boolean，即若当前没有消费者线程空闲，则直接返回false，不会插入到阻塞队列。 LinkedBlockingDequeLinkedBlockingDeque是一个由链表结构组成的双向阻塞队列。所谓双向队列指的你可以从队列的两端插入和移出元素。双端队列因为多了一个操作队列的入口，在多线程同时入队时，也就减少了一半的竞争。相比其他的阻塞队列，LinkedBlockingDeque多了addFirst，addLast，offerFirst，offerLast，peekFirst，peekLast等方法，以First单词结尾的方法，表示插入，获取（peek）或移除双端队列的第一个元素。以Last单词结尾的方法，表示插入，获取或移除双端队列的最后一个元素。另外插入方法add等同于addLast，移除方法remove等效于removeFirst。但是take方法却等同于takeFirst，不知道是不是Jdk的bug，使用时还是用带有First和Last后缀的方法更清楚。在初始化LinkedBlockingDeque时可以初始化队列的容量，用来防止其再扩容时过渡膨胀。另外双向阻塞队列可以运用在“工作窃取”模式中。 二、阻塞队列的实现原理阻塞队列的实现需要满足这样两个要求：如果队列为空，消费者会一直等待；如果队列为满，生产者会一直等待。因此，在实现阻塞队列的方式上，消费者和生产者的通信是必不可少的。JDK使用Condition实现了线程间通信，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889final ReentrantLock lock;private final Condition notFull; // 非满条件，插入元素的时候需要满足这个条件private final Condition notEmpty; // 非空条件，取出元素的时候需要满足这个条件public ArrayBlockingQueue(int capacity, boolean fair) &#123; // 初始容量必须大于0 if (capacity &lt;= 0) throw new IllegalArgumentException(); // 初始化数组 this.items = new Object[capacity]; // 初始化可重入锁 lock = new ReentrantLock(fair); // 初始化等待条件 notEmpty = lock.newCondition(); notFull = lock.newCondition();&#125;public void put(E e) throws InterruptedException &#123; checkNotNull(e); // 获取可重入锁 final ReentrantLock lock = this.lock; // 如果当前线程未被中断，则获取锁 lock.lockInterruptibly(); try &#123; while (count == items.length) // 判断元素是否已满 // 若满，则等待 notFull.await(); // 入队列 enqueue(e); &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125;public E take() throws InterruptedException &#123; // 可重入锁 final ReentrantLock lock = this.lock; // 如果当前线程未被中断，则获取锁，中断会抛出异常 lock.lockInterruptibly(); try &#123; while (count == 0) // 元素数量为0，即Object数组为空 // 则等待notEmpty条件 notEmpty.await(); // 出队列 return dequeue(); &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125;private void enqueue(E x) &#123; // assert lock.getHoldCount() == 1; // assert items[putIndex] == null; // 获取数组 final Object[] items = this.items; // 将元素放入 items[putIndex] = x; if (++putIndex == items.length) // 放入后存元素的索引等于数组长度（表示已满） // 重置存索引为0 putIndex = 0; // 元素数量加1 count++; // 唤醒在notEmpty条件上等待的线程 notEmpty.signal();&#125;private E dequeue() &#123; // assert lock.getHoldCount() == 1; // assert items[takeIndex] != null; final Object[] items = this.items; @SuppressWarnings(\"unchecked\") // 取元素 E x = (E) items[takeIndex]; // 该索引的值赋值为null items[takeIndex] = null; // 取值索引等于数组长度 if (++takeIndex == items.length) // 重新赋值取值索引 takeIndex = 0; // 元素个数减1 count--; if (itrs != null) itrs.elementDequeued(); // 唤醒在notFull条件上等待的线程 notFull.signal(); return x;&#125; 当我们往队列里插入一个元素时或者取出一个元素时，如果队列不可用，则会调用Condition的await()方法来阻塞当前线程，该方法主要通过是LockSupport.park(this);来实现的12345678910111213141516171819202122public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 加入等待队列 Node node = addConditionWaiter(); // 释放同步状态（锁） int savedState = fullyRelease(node); int interruptMode = 0; // 判断节点是否在同步队列中 while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); // 核心部分，阻塞（和Object.wait(0)一样进入无限等待状态） if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 退出while循环说明节点被signal()调入同步队列中，调用acquireQueued()加入同步状态竞争，竞争到锁后从await()方法返回 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 函数的核心部分是第8行的LockSupport.park()函数，该函数表示阻塞当前线程，它的源码如下：123456public static void park(Object blocker) &#123; Thread t = Thread.currentThread(); setBlocker(t, blocker); unsafe.park(false, 0L); // 核心部分 setBlocker(t, null);&#125; 函数的核心部分是第4行unsafe.park()函数，这是一个native方法，park这个方法会阻塞当前线程，只有以下四种情况中的一种发生时，该方法才会返回。 与park对应的unpark执行或已经执行时。注意：已经执行是指unpark先执行，然后再执行的park。 线程被中断时。 如果参数中的time不是零，等待了指定的毫秒数时。 发生异常现象时。这些异常事先无法确定。 JVM中park在不同的操作系统使用不同的方式实现，在linux下是使用的是系统方法pthread_cond_wait实现。12345678910111213141516171819202122232425262728293031void os::PlatformEvent::park() &#123; int v ; for (;;) &#123; v = _Event ; if (Atomic::cmpxchg (v-1, &amp;_Event, v) == v) break ; &#125; guarantee (v &gt;= 0, \"invariant\") ; if (v == 0) &#123; // Do this the hard way by blocking ... int status = pthread_mutex_lock(_mutex); assert_status(status == 0, status, \"mutex_lock\"); guarantee (_nParked == 0, \"invariant\") ; ++ _nParked ; while (_Event &lt; 0) &#123; status = pthread_cond_wait(_cond, _mutex); // 核心部分 // for some reason, under 2.7 lwp_cond_wait() may return ETIME ... // Treat this the same as if the wait was interrupted if (status == ETIME) &#123; status = EINTR; &#125; assert_status(status == 0 || status == EINTR, status, \"cond_wait\"); &#125; -- _nParked ; // In theory we could move the ST of 0 into _Event past the unlock(), // but then we'd need a MEMBAR after the ST. _Event = 0 ; status = pthread_mutex_unlock(_mutex); assert_status(status == 0, status, \"mutex_unlock\"); &#125; guarantee (_Event &gt;= 0, \"invariant\") ; &#125; &#125; pthread_cond_wait是一个多线程的条件变量函数，cond是condition的缩写，字面意思可以理解为线程在等待一个条件发生，这个条件是一个全局变量。这个方法接收两个参数，一个共享变量_cond，一个互斥量_mutex。而unpark方法在linux下是使用pthread_cond_signal实现的。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"CAS原理和缺陷","slug":"CAS原理和缺陷","date":"2018-09-03T12:21:30.000Z","updated":"2019-03-08T13:14:03.320Z","comments":true,"path":"2018/09/03/CAS原理和缺陷/","link":"","permalink":"http://yoursite.com/2018/09/03/CAS原理和缺陷/","excerpt":"","text":"JDK1.6以后JVM对synchronize锁机制作了不少优化，加入了偏向锁和自旋锁，在锁的底层实现中或多或少的都借助了CAS操作，其实Java中java.util.concurrent包的实现也是差不多建立在CAS之上，可见CAS在Java同步领域的重要性。 CAS是Compare and Swap的简写形式，可翻译为：比较并交换。用于在硬件层面上提供原子性操作。其实现方式是基于硬件平台的汇编指令，就是说CAS是靠硬件实现的，JVM只是封装了汇编调用。比较是否和给定的数值一致，如果一致则修改，不一致则不修改。 CAS案例分析AtomicInteger的原子特性就是CAS机制的典型使用场景。 其相关的源码片段如下：123456789101112131415161718private volatile int value; public final int get() &#123; return value; &#125; public final int incrementAndGet() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; &#125; &#125; public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125; AtomicInteger在没有锁的机制下借助volatile原语，保证了线程间的数据是可见的（共享的）。其get()方法可以获取最新的内存中的值。 在incrementAndGet()的操作中，使用了CAS操作，每次从内存中读取最新的数据然后将此数据+1，最终写入内存时，先比较内存中最新的值，同累加之前读出来的值是否一致，不一致则写失败，循环重试直到成功为止。 compareAndSet的具体实现调用了unsafe类的compareAndSwapInt方法，它其实是一个Java Native Interface（简称JNI）java本地方法，会根据不同的JDK环境调用不同平台的对应C实现，下面以windows操作系统，X86处理器的实现为例，这个本地方法在openjdk中依次调用的c++代码为：unsafe.cpp，atomic.cpp和atomic_windows_x86.inline.hpp，它的实现代码存在于：openjdk7\\hotspot\\src\\os_cpu\\windows_x86\\vm\\atomic_windows_x86.inline.hpp，下面是相关的代码片段：1234567891011121314151617181920// Adding a lock prefix to an instruction on MP machine // VC++ doesn't like the lock prefix to be on a single line // so we can't insert a label after the lock prefix. // By emitting a lock prefix, we can define a label after it. #define LOCK_IF_MP(mp) __asm cmp mp, 0 \\ __asm je L0 \\ __asm _emit 0xF0 \\ __asm L0: inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) &#123; // alternative for InterlockedCompareExchange int mp = os::is_MP(); __asm &#123; mov edx, dest mov ecx, exchange_value mov eax, compare_value LOCK_IF_MP(mp) cmpxchg dword ptr [edx], ecx &#125; &#125; 由上面源代码可见在该平台的处理器上CAS通过指令cmpxchg（就是x86的比较并交换指令）实现，并且程序会根据当前处理器是否是多处理器(is_MP)来决定是否为cmpxchg指令添加lock前缀(LOCK_IF_MP)，如果是单核处理器则省略lock前缀(单处理器自身会维护单处理器内的顺序一致性，不需要lock前缀提供的内存屏障效果(而在JDK9中，已经忽略了这种判断都会直接添加lock前缀，这或许是因为现代单核处理器几乎已经消亡)。关于Lock前缀指令： Lock前缀指令可以通过对总线或者处理器内部缓存加锁，使得其他处理器无法读写该指令要访问的内存区域，因此能保存指令执行的原子性。 Lock前缀指令将禁止该指令与之前和之后的读和写指令重排序。 Lock前缀指令将会把写缓冲区中的所有数据立即刷新到主内存中。 上面的第1点保证了CAS操作是一个原子操作，第2点和第3点所具有的内存屏障效果，保证了CAS同时具有volatile读和volatile写的内存语义（不过一般还是认为CAS只具有原子性而不具有可见性，因为底层的处理器平台可能不同）。 关于总线锁定和缓存锁定 1、早期的处理器只支持通过总线锁保证原子性。所谓总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。很显然，这会带来昂贵的开销。2、缓存锁定是改进后的方案。在同一时刻我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把CPU和内存之间通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，最近的处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。缓存锁定是指当两个CPU的缓存行同时指向一片内存区域时，如果A CPU希望对该内存区域进行修改并使用了缓存锁定，那么B CPU将无法访问自己缓存中相应的缓存行，自然也没法访问对应的内存区域，这样就A CPU就实现了独享内存。 但是有两种情况下处理器不会使用缓存锁定。第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行（cache line），则处理器会调用总线锁定。第二种情况是：有些处理器不支持缓存锁定。对于Inter486和奔腾处理器，就算锁定的内存区域在处理器的缓存行中也会调用总线锁定。 关于同样使用Lock前缀的volatile却无法保证原子性 volatile和cas都是基于lock前缀实现，但volatile却无法保证原子性这是因为：Lock前缀只能保证缓存一致性，但不能保证寄存器中数据的一致性，如果指令在lock的缓存刷新生效之前把数据写入了寄存器，那么寄存器中的数据不会因此失效而是继续被使用，就好像数据库中的事务执行失败却没有回滚，原子性就被破坏了。以被volatile修饰的i作i++为例，实际上分为4个步骤：mov 0xc(%r10),%r8d ; 把i的值赋给寄存器inc %r8d ; 寄存器的值+1mov %r8d,0xc(%r10) ; 把寄存器的值写回lock addl $0x0,(%rsp) ; 内存屏障，禁止指令重排序，并同步所有缓存 如果两个线程AB同时把i读进自己的寄存器，此时B线程等待，A线程继续工作，把i++后放回内存。按照原子性的性质，此时B应该回滚，重新从内存中读取i，但因为此时i已经拷贝到寄存器中，所以B线程会继续运行，原子性被破坏。 而cas没有这个问题，因为cas操作对应指令只有一个：lock cmpxchg dword ptr [edx], ecx ; 该指令确保了直接从内存拿数据（ptr [edx]），然后放回内存这一系列操作都在lock状态下，所以是原子性的。 总结：volatile之所以不是原子性的原因是jvm对volatile语义的实现只是在volatile写后面加一个内存屏障，而内存屏障前的操作不在lock状态下，这些操作可能会把数据放入寄存器从而导致无法有效同步；cas能保证原子性是因为cas指令只有一个，这个指令从头到尾都是在lock状态下而且从内存到内存，所以它是原子性的。 CAS缺陷1、ABA问题。因为CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么 A－B－A 就会变成 1A - 2B－3A。 从Java1.5开始JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2、循环时间长开销大。自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline），使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3、只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量i＝2，j=a，合并一下ij=2a，然后用CAS来操作ij。从Java1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作。 4、总线风暴带来的本地延迟。在多处理架构中，所有处理器会共享一条总线，靠此总线连接主存，每个处理器核心都有自己的高速缓存，各核相对于BUS对称分布，这种结构称为“对称多处理器”即SMP。当主存中的数据同时存在于多个处理器高速缓存的时候，某一个处理器的高速缓存中相应的数据更新之后，会通过总线使其它处理器的高速缓存中相应的数据失效，从而使其重新通过总线从主存中加载最新的数据，大家通过总线的来回通信称为“Cache一致性流量”，因为总线被设计为固定的“通信能力”，如果Cache一致性流量过大，总线将成为瓶颈。而CAS恰好会导致Cache一致性流量，如果有很多线程都共享同一个对象，当某个核心CAS成功时必然会引起总线风暴，这就是所谓的本地延迟。而偏向锁就是为了消除CAS，降低Cache一致性流量。 关于偏向锁如何消除CAS 试想这样一种情况：线程A：申请锁 - 执行临界区代码 - 释放锁 - 申请锁 - 执行临界区代码 - 释放锁。锁的申请和释放都会执行CAS，一共执行4次CAS。而在偏向锁中，线程A：申请锁 - 执行临界区代码 - 比较对象头 - 执行临界区代码。只执行了1次CAS。 关于总线风暴 其实也不是所有的 CAS 都会导致总线风暴，这跟 Cache 一致性协议有关，具体参考：http://blogs.oracle.com/dave/entry/biased_locking_in_hotspot另外与 SMP 对应还有非对称多处理器架构，现在主要应用在一些高端处理器上，主要特点是没有总线，没有公用主存，每个 Core 有自己的内存。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]},{"title":"《高性能 MySQL》读书笔记——查询优化","slug":"《高性能MySQL》读书笔记——查询优化","date":"2018-08-28T10:34:21.000Z","updated":"2019-03-08T13:13:39.612Z","comments":true,"path":"2018/08/28/《高性能MySQL》读书笔记——查询优化/","link":"","permalink":"http://yoursite.com/2018/08/28/《高性能MySQL》读书笔记——查询优化/","excerpt":"","text":"在设计了最优的库表结构、如何建立最好的索引，这些对于高性能来说必不可少。但这些还不够，还需要合理地设计查询。 一、查询执行的基础当 MySQL 执行一个查询语句时，会经历以下几个步骤： 客户端发送一条查询给服务器。 服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。 服务器端进行 SQL 解析、预处理，再由优化器生成对应的执行计划。 MySQL 根据优化器生成的执行计划，调用存储引擎的 API 来执行查询。 将结果返回给客户端。 二、MySQL 查询优化器MySQL 查询优化器会做大量的工作，这些工作包括但不限于： 1. 重新定义关联表的顺序MySQL 使用一种叫 “嵌套循环关联” 的方式来执行关联查询。顾名思义，这是一种嵌套式的查询。在正常情况下，最左边的表会嵌套在最外层，然后根据表中的每一行数据去遍历内层表，找到所有符合条件的行。如果外层表行数为 m，内层表行数为 n，则总共要遍历 m*n 行数据。 但如果内层表使用了索引，而关联字段恰好就被索引覆盖的话，就只需要几次查询就可以定位内层表的数据行。总行数从 m*n 变为 m*i(i 一般小于 3)。这无疑大大加快了关联查询的速度。MySQL 查询优化器会调整关联表查询的顺序来尽可能使用多的索引查询。 2. 将外连接转化成内连接并不是所有的 OUTER JOIN 语句都必须以外连接的方式执行。诸多原因，例如 WHERE 条件、库表结构都可能让一个外连接等价于一个内连接。MySQL 能够识别这点并重写查询，让其可以调整关联顺序（外连接分左右，所以无法调整顺序）。 3. 使用等价变换规则MySQL 可以合并和减少一些比较，例如（5=5 AND a&gt;5）将被改写成（a&gt;5）。 4. 优化 COUNT()、MIN() 和 MAX()索引和是否可为空可以帮助 MySQL 优化这类表达式。例如要找某一列的最小值，只需查询对应 B-tree 索引最左端的记录，而最大值只需查询对应 B-tree 索引最右端。另外，没有任何 WHERE 条件的 COUNT(*) 查询在 MyISAM 中也是 O(1)，因为 MyISAM 维护了一个变量来存放数据表的行数（不过 innodb 没有，innodb 中执行 COUNT(*) 会做全表查询）。 5. 预估并转化为常数表达式当 MySQL 检测到一个表达式或者一个子查询可以转化为常数的时候，就会一直把该表达式作为常数进行优化处理。 6. 覆盖索引扫描当索引中的列覆盖所有查询中需要使用的列时，MySQL 将直接使用索引返回所需数据而不做回表查询。 7. 提前终止查询在发现已经满足查询要求的时候，MySQL 总是能够立刻终止查询。比如 LIMIT。 8. 子查询优化MySQL 5.6 中处理子查询的思路是，基于查询重写技术的规则，尽可能将子查询转换为连接，并配合基于代价估算的 Materialize、exists 等优化策略让子查询执行更优。 9. 等值传播如果两个列的值通过等式关联，那么 MySQL 能够把其中一个列的 WHERE 语句条件传递到另一列上。例如： SELECT film.film_id FROM film INNER JOIN film_actor USING(film_id) WHERE film.film_id &gt; 500; 优化器会把它优化为： SELECT film.film_id FROM film INNER JOIN film_actor USING(film_id) WHERE film.film_id &gt; 500 AND film_actor.film_id &gt; 500; 10. 列表 IN() 的比较MySQL 会将 IN() 列表中的数据先进行排序，然后通过二分查找的形式来确定取出的值是否在列表中。 11. 索引合并当 WHERE 子句中包含多个复杂条件涉及到多个索引时，MySQL 会先根据不同索引取出多组数据，再将这些数据合并。 三、MySQL 查询优化器的局限1. 关联子查询上一节有提到 MySQL 查询优化器会把 IN 子查询变为 EXISTS 子查询的形式，大部分情况下这种优化会带来性能提升，但某些情况下，会让查询更慢。比如： SELECT * FROM film WHERE film_id IN(SELECT film_id FROM film_actor WHERE actor_id = 1); 假设 film 和 film_actor 表在 film_id 上都有索引，那么这条语句会走 film_actor 和 film 的索引，速度非常快。但查询优化器会把它 “优化” 为： SELECT * FROM film WHERE EXISTS(SELECT 1 FROM film_actor WHERE actor_id = 1 AND film.film_id = actor.film_id); 此时 MySQL 只会走 film_actor 的索引而会对 film 做全表扫描，效率大大下降。 解决的方法就是使用左外连接改造： SELECT film.* FROM film LEFT OUTER JOIN film_actor USING(film_id) WHERE film_actor.actor_id = 1; PS：5.6 及以后版本的 MySQL 对关联子查询做了大量优化，现在的思路是，基于查询重写技术的规则，尽可能将子查询转换为连接，并配合基于代价估算的 Materialize、exists 等优化策略让子查询执行更优。因此 5.6 以后将不存在这个问题。 2.UNION 的限制MySQL 无法将限制条件从外层 “下推” 到内层，其中一个典型就是 UNION： (SELECT first_name FROM actor) UNION ALL (SELECT first_name FROM customer) LIMIT 20; MySQL 会把 actor 和 customer 中的所有记录放在同一张临时表中，然后从临时表中取出前 20 条。可以通过把 LIMIT 放入内部来解决这个问题： (SELECT first_name FROM actor LIMIT 20) UNION ALL (SELECT first_name FROM customer LIMIT 20) LIMIT 20; 这样临时表的规模就缩小到 40 了。 3. 最大值和最小值优化MySQL 优化器会对不加条件的 MAX 和 MIN 做优化，但并没有对加条件的这两个函数做优化。比如： SELECT MIN(actor_id) FROM actor WHERE first_name = “PENELOPE”; actor_id 是主键，严格按照大小排序，那么其实在找到第一个满足条件的记录就可以返回了。而 MySQL 会继续遍历整张表。修改的方式是使用 LIMIT： SELECT actor_id FROM actor WHERE first_name = “PENELOPE” LIMIT 1; 此时会触发提前终止机制，返回最小的 actor_id。 四、优化特定类型的查询1. 优化 COUNT() 查询很多时候一些业务场景并不要求完全精确的 COUNT 值，可以使用 EXPLAIN SELECT * FROM film; 得到的近似值来代替。 innodb 下，如果表中单行数据量很大，且没有二级索引的话，可以对表上较短的且不为空的字段加索引，再执行 count(*），此时优化器会自动选择走二级索引，由于二级索引是短字段，单页存储的数据行数就多，减少了取页的次数，查询时间也就更短了。 2. 优化关联查询确保 ON 或者 USING 子句中的列上有索引，这样只需要全表扫描第一个表，第二个表可以走索引。 确保任何的 GROUP BY 和 ORDER BY 中的表达式只涉及到一个表中的列，这样 MySQL 才可能使用索引来优化这个过程。 3. 优化 LIMIT 分页LIMIT 分页在系统偏移量非常大的时候效果会很差。比如 LIMIT 1000,20 这样的查询，这时 MySQL 需要查询 10020 条记录然后只返回最后 20 条。 优化此类分页查询的一个最简单的办法就是尽可能利用索引覆盖扫描。考虑下面的查询： SELECT film_id, description FROM film ORDER BY title LIMIT 50,5; 这个语句可以被改写成： SELECT film.film_id,film,description FROM film INNER JOIN(SELECT film_id FROM film ORDER BY title LIMIT 50,5)AS lim USING(film_id); 这里的 “延迟关联” 操作将大大提升查询效率，它会利用覆盖索引直接定位到分页所需数据所在的位置，而不需要从头遍历。 记录上一次取数据的位置也是一个不错的主意。比如在频繁使用 “下一页” 这个功能的时候，记录下上次取数据最后的位置，然后可以把查询语句写成： SELECT film_id, description FROM film WHERE film_id&gt;16030 ORDER BY film_id LIMIT 5;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"《高性能MySQL》读书笔记——索引优化","slug":"《高性能MySQL》读书笔记——索引优化","date":"2018-08-25T09:34:28.000Z","updated":"2019-03-08T13:13:54.496Z","comments":true,"path":"2018/08/25/《高性能MySQL》读书笔记——索引优化/","link":"","permalink":"http://yoursite.com/2018/08/25/《高性能MySQL》读书笔记——索引优化/","excerpt":"","text":"索引是数据库得以高效的关键，以最常见的B+Tree索引为例，至少有以下三个优点：1）索引大大减少了服务器需要扫描的数据量；2）索引可以帮助服务器避免排序和临时表；3）索引可以将随机I/O变为顺序I/O。因此，如何使用索引也成了高效MySQL重要的一部分。 一、MySQL中的索引1、B-Tree索引B-Tree和其变体B+Tree是绝大部分数据库引擎默认使用的数据结构，我们在谈到索引时若无特殊指代一般是指B+Tree（B-Tree和B+Tree的区别及选择这种数据结构的原因可以看这里） 优点B-Tree索引适用于全键值、键值范围或键前缀查找。其中键前缀查找只适用于根据最左前缀的查找。以下表为例 family_name first_name birthday 张 三 1960-1-1 李 四 1962-3-2 王 五 1966-4-5 依次在family_name、first_name和birthday上建立B-Tree索引（暂不考虑主键的影响），则上述的索引对如下的索引有效： 全值匹配：全值匹配指的是和索引中的所有列进行匹配，如查找 family_name = 张，first_name = 三，birthday = 1960-1-1 的人。 匹配最左前缀：查找所有 family_name= 张 的人。 匹配列前缀：查找所有 family_name = 张，first_name = 三，birthday = 196X 的人。列前缀的语法有很多，比如 where birthday(3) = 196 或者 where birthday like “196%” 都是合法的匹配列前缀的语法，但 birthday like “%96%” 不是合法的列前缀匹配，不会使用索引查找。 匹配范围值：查找所有 family_name = 张，first_name = 三，birthday &gt; 1960-1-1 and birthday &lt; 1969-12-31 的人。 只访问索引的查询：若查询的内容在索引中便可全部找到，则无需回表查询，可以节省大量磁盘IO，这种索引有个专有名词叫“覆盖索引”。 缺点B-Tree索引的功能强大，但也有局限，仍以上述的索引为例： 如果不是按照索引的最左列开始查找，则无法使用索引。例如我们无法用索引查找 first_name = 四 的人。 不能跳过索引中的列。例如我们无法用索引查找 family_name = 张，birthday = 1960-1-1 的人。 查询中可以使用范围查询，但只能使用一次，且它右边所有的列都无法使用索引优化查找。如查找 family_name = 张，first_name &gt; 三 and first_name &lt; 五（字典序），birthday = 1960-1-1 的人，只会用到姓和名两个索引列，无法使用第三个索引列。（这个涉及到联合索引底层的数据结构，如下图） B-Tree在建立联合索引的时候只会对第一个字段建立B-Tree索引，其它字段会在对应的叶子节点的data域上按给定字段的顺序作为优先级排序后储存。如上图，对id、family_name、first_name三个列建立索引。则底层存储时会先按id构造B-Tree，再在B-Tree的叶子节点上按family_name、first_name的优先级排序后存储对应的地址。对于叶子节点上数据的查找，会采用二分查找的方式。而一旦确定了前一个字段使用范围查找后，得到的一组数据对于后一个字段而言是无序的，无法继续使用二分查找，只能遍历，此时索引失效。除了范围查询，整个B-Tree的最左匹配原则的原因也是和这个数据存储的方式息息相关的，理解了这个数据结构也就理解了B-Tree的最左匹配原则。 2、哈希索引哈希索引基于哈希表实现，使用链表法解决哈希冲突，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码，哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 在MySQL中，只有Memory引擎显式支持哈希索引，这也是Memory引擎表的默认索引类型。 优点 索引的结构十分紧凑，查找的速度非常快。 缺点 哈希索引只包含哈希值和指针，而不存储字段值，所以不能使用覆盖查询。 哈希索引数据并不是按照索引值顺序存储的，所以也就无法用于排序。 哈希索引也不支持部分索引列匹配查找，因为哈希值的计算是使用索引列的全部内容计算的。 哈希索引只支持等值比较，不支持任何范围查询 哈希索引的缺点也决定了哈希索引只适用于某些特定的场合，但一旦适合哈希索引，则它带来的性能提升将非常显著。 3、全文索引在标准的MySQL中只有MyISAM引擎支持全文索引，同时innodb也开始实验性质地支持全文索引。 MyISAM的全文索引作用对象是一个“全文集合”，这可能是某个数据表的一列，也可能使多个列。具体的，对数据表的某一条记录，MySQL会将需要索引的列全部拼接成一个字符串，然后进行索引。 MyISAM的全文索引是一类特殊的B-Tree索引，共有两层。第一层是所有关键字，然后对于每一个关键字的第二层，包含的是一组相关的“文档指针”。 二、innodb中的索引1、聚簇索引聚簇索引不是一种单独的索引类型，而是一种数据存储方式。innodb的聚簇索引是在B-tree的叶子节点上存放了数据行。 innodb所有表中的数据都会以这种形式保存在磁盘，这也意味着innodb每张表中至少要有一个主键。如果没有显式地定义主键，innodb会选择一个唯一的非空索引代替；如果没有这样的索引，innodb会隐式定义一个主键作为聚簇索引。 聚簇索引中，每个叶子节点称为一个数据页，相邻的数据页之间有双向指针相连，范围查找可以直接按顺序读出，速度非常快。 2、非聚簇索引（辅助索引）innodb中每张表有且仅有一个聚簇索引，剩下的都是非聚簇索引。对于非聚簇索引，叶子节点并不会包含行记录的全部数据，而是保存指向聚簇索引中某一条记录的指针。比如user表中使用user_id作为主键，那么在它的非聚簇索引的叶子节点中，保存的就是user_id。对于一次使用了非聚簇索引的查找，数据库引擎会先在非聚簇索引上找到user_id，再根据 user_id在聚簇索引上找到对应的数据行，这也就是innodb中的二次查询。 3、自适应哈希索引自适应哈希索引是innodb上的一种优化措施。InnoDB存储引擎会监控对表上各索引页的查询。如果观察到建立哈希索引可以带来速度提升，则建立哈希索引，称之为自适应哈希索引(Adaptive Hash Index, AHI)。AHI是通过缓冲池的B+树页构造而来，因此建立的速度很快，而且不需要对整张表构建哈希索引。InnoDB存储引擎会自动根据访问的频率和模式来自动地为某些热点页建立哈希索引。 AHI有一个要求，对这个页的连续访问模式必须是一样的。例如对于(a,b)这样的联合索引页，其访问模式可以是下面情况： where a=xxx where a =xxx and b=xxx 访问模式一样是指查询的条件是一样的，若交替进行上述两种查询，那么InnoDB存储引擎不会对该页构造AHI。AHI还有下面几个要求： 以该模式连续访问了100次 以该模式连续访问了 页中记录总数/16次 必须同时满足上述所有要求才会建立AHI。 InnoDB存储引擎官方文档显示，启用AHI后,读取和写入速度可以提高2倍，辅助索引的连接操作性能可以提高5倍。 三、常见索引失效场景1、查询条件包含or SELECT * FROM order WHERE order_id = 1 OR pay_method=’123’; 当or左右查询字段只有一个是索引，该索引失效；只有当or左右查询字段均为索引时，才会生效。 2、索引列上有计算、函数等操作 SELECT * FROM order WHERE order_id +1 = 2; 3、使用负向查询（!=、&lt;&gt;、not in、not exists、not like等） SELECT * FROM order WHERE order_id &lt;&gt; 2; 4、5.7之前的is null和is not null SELECT * FROM order WHERE order_id is not null; 5.7之后is null和is not null也会走索引，但对于使用了声明了NOT NULL的索引行不会。 5、不符合最左前缀原则的组合索引当查询涉及到联合索引时，查询的条件必须是联合索引的一个前缀。比如对于联合索引A/B/C/D，查询的条件可以是A，也可以是A/B/C，但不能是B/C。另外对于范围查询，只能有一个条件是范围查询且必须是最后一个。比如查询A/B/C，只有C可以是范围查询。另外在MySQL中，IN被定义为范围查询，但却是当作多个条件等于来处理，因此IN语句放在中间，也会走索引。 6、like以通配符开头 SELECT * FROM order WHERE pay_method LIKE ‘%23’; 7、字符串不加单引号 SELECT * FROM order WHERE pay_method = 123; 8、当全表扫描速度比索引速度快时，mysql会使用全表扫描，此时索引失效一个有意思的例子是IN的索引失效。MySQL优化器对开销代价的估算方法有两种：index dive和index statistics。前者统计速度慢但是能得到精准的值，后者统计速度快但是数据未必精准。老版本的MySQL默认使用index dive这种统计方式，但在IN()组合条件过多的时候会发生很多问题。查询优化可能需要花很多时间，并消耗大量内存。因此新版本MySQL在组合数超过一定的数量（eq_range_index_dive_limit）就会使用index statistics统计。而index statistics统计的结果不精确，因此可能会出现IN不走索引的情况。此时可以尝试通过增加eq_range_index_dive_limit的值（5.6中默认是10，5.7中默认是200）让IN语句走索引。 四、高性能索引策略1、使用前缀索引在对一个比较长的字符串建立索引的时候，把字符串所有字符放入索引是比较低效的做法。前文对字符串做哈希是一种方式。也可以使用字符串的前缀做索引。比如 CREATE INDEX index_name ON table_name (column_name(10)); 表示将列的前10个字符做索引，这样做的好处是减少索引字段的大小，可以在一页内放入更多的数据，从而降低B-tree的高度，同时更短的索引字段带来更短的匹配时间，提高了查找效率。 2、使用覆盖索引覆盖索引是一种索引包含了查询所需所有数据的情况，在这种情况下，MySQL可以使用索引来直接获取列的数据，这样就不需要再读取数据行。覆盖索引是非常有用的工具，能够极大地提升性能： 索引条目远小于数据行大小，所以如果只需要读取索引，MySQL就会极大地减少数据访问量。这对缓存的负载非常重要，因为这种情况下响应时间大部分花费在数据拷贝上。 因为索引是按值顺序存储的（至少在单个页内如此），对于I/O密集型的范围查询会比随机从磁盘读取每一行数据的I/O要少得多。 对于innodb的聚簇索引，覆盖索引特别有用。innodb的二级索引在叶子节点中保存了行的主键值，所以如果二级主键能够覆盖索引，则可以避免对主键索引的二次查询。 3、延迟关联覆盖索引可以极大地提升查找的效率，但很多时候我们会遇到 select * 这样的需求，这时使用覆盖索引就不可能了。不过我们可以使用延迟关联的方式利用覆盖索引。 比如对于语句： select from t_portal_user where create_time &gt; ‘2012-10:10’ and create_time&lt;’2017:10:10’ LIMIT 5000,10; 可以改写成： SELECT from t_portal_user INNER JOIN (select id from t_portal_user where create_time &gt; ‘2012-10:10’ and create_time&lt;’2017:10:10’ LIMIT 5000,10) as a USING(id); 对于子查询： select id from t_portal_user where create_time &gt; ‘2012-10:10’ and create_time&lt;’2017:10:10’ LIMIT 5000,10; 如果在create_time上做了索引（innodb中主键会被默认添加进索引中），则可以利用覆盖索引找到符合条件的id，再根据id做普通查询。 4、利用索引来做排序MySQL支持二种方式的排序，文件排序和索引，后者效率高，它指MySQL扫描索引本身完成排序。文件排序方式效率较低。ORDER BY满足以下情况，会使用Index方式排序: 使用覆盖索引，即通过扫描索引本身就可完成排序 ORDER BY 语句 或者 WHERE , JOIN 子句和ORDER BY 语句组成的条件组合满足最左前缀（一个例外是IN，IN在没有排序的最左匹配中被视为等值查询，对排序来说是一种范围查询） ORDER BY语句中条件的排序顺序是一样（都为正序或者都为倒序） 5、自定义哈希索引如果存储引擎不支持哈希索引，则可以在B-tree基础上创建一个伪哈希索引。这和真正的哈希索引不是一回事，因为还是使用B-tree进行查找，但它使用的是哈希值而不是键本身进行查找。如 SELECT id FROM user WHERE address = “zhejiang ningbo”; 若删除原来URL列上的索引，而新增一个被索引的address_crc列，使用CRC32做哈希，就可以使用下面的方式查询： SELECT id FROM user WHERE address=”zhejiang ningbo” AND url_crc=CRC32(“zhejiang ningbo”); 这样做的性能会非常高，因为MySQL优化器会使用这个选择性很高而体积很小的基于url_crc列的索引来完成查找。即使有多个记录有相同的索引值，查找仍然很快，只需要根据哈希值做快速的整数比较就能找到索引条目，然后一一比较返回对应的行。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"《高性能MySQL》读书笔记——库表结构优化","slug":"《高性能MySQL》读书笔记——库表结构优化","date":"2018-08-18T10:08:36.000Z","updated":"2019-03-08T13:13:47.256Z","comments":true,"path":"2018/08/18/《高性能MySQL》读书笔记——库表结构优化/","link":"","permalink":"http://yoursite.com/2018/08/18/《高性能MySQL》读书笔记——库表结构优化/","excerpt":"","text":"本文介绍了MySQL中的常用数据类型及其适用场景，以及数据库范式和反范式化的设计。 1、MySQL常用数据类型MySQL常用数据类型分为：整数、实数、字符串、日期和时间、位数据几种。 整数类型整数类型可以分为：TINYINT，SMALLINT，MEDIUMINT，INT，BIGINT，分别使用：8，16，24，32，64位的存储空间。 整数类型有可选的UNSIGNED属性，表示不允许负值，可以把正数的上限提高一倍。 整数可以指定宽度，比如INT(11)，这表示在一些交互工具中INT会显示11个数字，对于存储和计算来说没有意义，INT(1)和INT(20)都是32位。 实数类型实数类型可以是带有小数部分的数字，也可以不是。比如可以使用DECIMAL存储比BIGINT还大的整数。 实数类型分为：FLOAT，DOUBLE和DECIMAL，其中FLOAT和DOUBLE用于存储不精确的小数类型，分别使用32和64位的存储空间；DECIMAL用于存储精确的小数类型，存储空间的大小根据小数的长度决定。5.0版本以后DECIMAL最多允许65个数字。 尽量使用FLOAT和DOUBLE存储小数类型。只有需要精确计算的场合才使用DECIMAL，但在数据量比较大的时候，可以考虑使用BIGINT代替DECIMAL，将需要存储的数字乘以相应的倍数即可。 字符串类型MySQL中的字符串类型有CHAR，VARCHAR，TEXT和BLOB四种，其中TEXT和BLOB两种类型比较特别，MySQL把每个BLOB和TEXT值当作一个独立的对象处理，存储引擎在处理时也通常会做相应的处理。比如当TEXT或BLOB的值较大时，innodb会使用专门的“外部”存储区域来进行存储，此时每个值在行内需要1~4个字节存储一个指针。 TEXT和BLOB之间仅有的不同是BLOB类型存储的是二进制数据，没有排序规则或字符集，而TEXT类型有字符集和排序规则。 MySQL对BLOB和TEXT列进行排序与其他类型是不同的：它只对每个列的最前max_sort_length字节而不是整个字符串做排序。 VARCHAR是变长字符串，而CHAR是定长字符串。VARCHAR比CHAR更省空间，因为它仅使用必要的空间。VARCHAR需要使用1或2个额外字节记录字符串的长度：如果列的最大长度小于或等于255字节，则只使用1个字节表示，否则使用2个。 VARCHAR节省了存储空间，但由于行是变长的，在UPDATE时可能使行变得比原来更长，这就导致需要做额外的工作。如果一个行占用空间增长，超出页的大小，innodb会使用裂页来使行可以放进页内。对于过长的VARCHAR，innodb会将其存储为BLOB。 CHAR值在存储时，MySQL会删除所有的末尾空格。 使用枚举（ENUM）代替字符串类型如果预计字符串可取的值范围确定且数量不大，可以使用枚举的方式替代字符串。比如存储水果，预计种类只有苹果、香蕉、梨。可以把水果种类定义为ENUM(“apple”,”banana”,”pear”)，实际存储时MySQL只会在列表中保存数字，并在.frm文件中保存一个“数字-字符串”的映射关系，可以大大减少存储的空间。 日期和时间类型MySQL能存储的最小时间粒度为秒，但也可以通过使用BIGINT类型存储微秒级别的时间戳等方式绕开这一限制。MySQL中存储时间的数据类型有两种：DATETIME和TIMESTAMP。两种类型的区别如下： DATETIME 占用8个字节 允许为空值，可以自定义值，系统不会自动修改其值。 实际格式储存，格式为YYYYMMDDHHMMSS的整数 与时区无关 不可以设定默认值，所以在不允许为空值的情况下，必须手动指定datetime字段的值才可以成功插入数据。 可以在指定datetime字段的值的时候使用now()变量来自动插入系统的当前时间。 结论：datetime类型适合用来记录数据的原始的创建时间，因为无论你怎么更改记录中其他字段的值，datetime字段的值都不会改变，除非你手动更改它。 TIMESTAMP 占用4个字节，默认为NOT NULL TIMESTAMP值不能早于1970或晚于2037。这说明一个日期，例如’1968-01-01’，虽然对于DATETIME或DATE值是有效的，但对于TIMESTAMP值却无效，如果分配给这样一个对象将被转换为0。 值以UTC格式保存，为从1970年1月1日（格林尼治时间）午夜以来的秒数。 时区转化 ，存储时对当前的时区进行转换，检索时再转换回当前的时区。 默认情况下，如果插入或更新时没有指定第一个TIMESTAMP的值，MySQL会设置这个列的值为当前时间。 结论：timestamp类型适合用来记录数据的最后修改时间，因为只要你更改了记录中其他字段的值，timestamp字段的值都会被自动更新。 位数据类型BIT和SET是MySQL中典型的位数据类型，位数据的本质是一个二进制字符串，使用位数据类型可以在一列中存储多个”true/false”值。 2、范式和反范式数据库中的范式满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多规范要求的称为第二范式（2NF），其余范式以次类推。一般说来，数据库只需满足第三范式 (3NF）就行了。 范式的包含关系。一个数据库设计如果符合第二范式，一定也符合第一范式。如果符合第三范式，一定也符合第二范式。 1NF：属性不可分 2NF：属性完全依赖于主键 [消除部分子函数依赖] 3NF：属性不依赖于其它非主属性 [消除传递依赖] 第一范式 (1NF)符合1NF的关系中的每个属性都不可再分。反例： 第二范式 (2NF)2NF在1NF的基础之上，消除了非主属性对于码（主键）的部分函数依赖 可以通过分解来满足。 分解前 学号 姓名 系名 系主任 课名 分数 1022211101 李小明 经济系 王强 高等数学 95 1022211101 李小明 经济系 王强 大学英语 87 1022211101 李小明 经济系 王强 普通化学 76 1022211102 张莉莉 经济系 王强 高等数学 72 1022211102 张莉莉 经济系 王强 大学英语 98 1022211102 张莉莉 经济系 王强 计算机基础 88 1022511101 高芳芳 法律系 刘玲 高等数学 82 1022511101 高芳芳 法律系 刘玲 法律基础 82 以上学生课程关系中，{学号, 课名} 为键码（主键），有如下函数依赖： （学号，课名） -&gt; 分数 学号 -&gt; 姓名 学号 -&gt; 系名 -&gt;系主任 分数完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。 姓名、系名和系主任都部分依赖于键码，我们需要把部分依赖变成完全依赖。 分解后 关系-1 学号 姓名 系名 系主任 1022211101 李小明 经济系 王强 1022211102 张莉莉 经济系 王强 1022211101 高芳芳 法律系 刘玲 有以下函数依赖： Sno -&gt; Sname, Sdept Sdept -&gt; Mname 关系-2 学号 课名 分数 1022211101 高等数学 95 1022211101 大学英语 87 1022211101 普通化学 76 1022211102 高等数学 72 1022211102 大学英语 98 1022211102 计算机基础 88 1022511101 高等数学 82 1022511101 法学基础 82 有以下函数依赖： Sno, Cname -&gt; Grade 第三范式 (3NF)3NF在2NF的基础之上，消除了非主属性对于码（主键）的传递函数依赖 上面的 关系-1 中存在以下传递函数依赖： Sno -&gt; Sdept -&gt; Mname 可以进行以下分解： 关系-11 学号 姓名 系名 1022211101 李小明 经济系 1022211102 张莉莉 经济系 1022211101 高芳芳 法律系 关系-12 系名 系主任 经济系 王强 法律系 刘玲 范式的优缺点优点： 当数据较好地范式化时，就只有很少或者没有重复数据，所以更新时只需要修改更少的数据。 范式化的表通常更小，可以更好地放在内存里，所以执行操作很更快。 很少有多余的数据意味着检索列表数据时更少需要DISTINCT或者GROUP BY语句。比如前面的例子：在非范式化的结构中必须使用DISTINCT或者GROUP BY才能获得唯一的一张系名列表，但如果使用范式，只需要单独查询系名-系主任表就可以了。 缺点： 范式化的设计通常需要关联。稍微复杂一点的查询语句在符合范式的schema上都有可能需要至少一次关联，这不但代价昂贵，也可能使一些索引无效。例如，范式化可能将列存放在不同的表中，而这些列如果在一个表中本可以属于同一个索引。 常见反范式化设计范式化不一定适合所有场合，很多时候，一些冗余数据有助于我们提升性能。下面列举几个常见的反范式化操作。 缓存表缓存表可以存储那些可以从其他表获取但每次获取的速度比较慢的数据。比如有时可能会需要很多不同的索引组合来加速各种类型的查询。这些矛盾的需求有时需要创建一张只包含主表中部分列的缓存表。有时候我们可能需要不同存储引擎提供的不同特性。例如，如果主表使用innodb，用MyISAM作为缓存表的引擎将会得到更小的索引空间，并且可以做全文索引。 汇总表汇总表保存的是GROUP BY语句聚合数据的表。相比缓存表，汇总表的数据不是逻辑上冗余的，但可以通过其它表计算得到。例如，计算某网站之前24小时内发送的消息数。我们可以通过COUNT()得到，但这样需要检索全表。作为替代方案，可以每小时生成一张汇总表。这样也许一条简单的查询就可以做到，并且比实时维护计数器要高效得多。缺点是计数并不是100% 精确。 某网站之前24小时内发送的消息数的汇总表： CREATE TABLE msg_per_hr (hr DATETIME NOT NULL,cnt INT UNSIGNED NOT NULL,PRIMARY KEY(hr)); 计数器表计数器在应用中很常见。比如网站的点击数，文件下载次数等。 如果其它数据保存在一起，很可能碰到并发问题。创建一张独立的表是个比较好的办法，这样可使计数器表小且快。而且使用独立的表可以帮助避免查询缓存失效。 下面是一张简单的计数器表，只有一行数据，记录网站的点击次数： mysql&gt; CREATE TABLE hit_counter( -&gt; cnt int unsigned not null -&gt; ) ENGINE=InnoDB; 网站的每次点击都会导致对计数器进行更新： mysql&gt; UPDATE hit_counter SET cnt = cnt + 1; 问题在于，对于任何想要更新这一行的事务来说，这条记录上都有一个全局的互斥锁。这会使得这些事务只能串行进行。要获得更高的并发性，可以将计数器保存在多个行中，每次随机选择一行更新： mysql&gt; CREATE TABLE hit_counter( -&gt; slot tinyint unsigned not null primary key, -&gt; cnt int unsigned not null -&gt; ) ENGINE=InnoDB; 然后预先在这张表增加100行数据。现在选择一个随机的槽进行更新： mysql&gt; UPDATE hit_counter SET cnt = cnt + 1 WHERE slot = RAND() * 100; 要获得统计结果，需要使用下面这样的聚合查询： mysql&gt; CREATE TABLE daily_hit_counter( -&gt; day date not null, -&gt; slot tinyint unsigned not null, -&gt; cnt int unsigned not null, -&gt; primary key(day, slot) -&gt; ) ENGINE=InnoDB; 一个常见的需求是每隔一段时间开始一个新的计算器（例如，每天一个）。再作进一步修改： mysql&gt; CREATE TABLE daily_hit_counter( -&gt; day date not null, -&gt; slot tinyint unsigned not null, -&gt; cnt int unsigned not null, -&gt; primary key(day, slot) -&gt; ) ENGINE=InnoDB; 在这个场景中可以不用预告生成行 ，而用ON DUPLICATE KEY UPDATE（对唯一索引或主键字段的值会检查是否已存在，存在则更新，不存在则插入）代替： mysql&gt; INSERT INTO daily_hit_counter(day, slot, cnt) -&gt; VALUES(CURRENT_DATE, RAND()*100, 1) -&gt; ON DUPLICATE KEY UPDATE cnt = cnt + 1; 如果希望减少表的行数，可以写一个周期执行的任务，合并所有结果到0号槽，并且删除所有其他的槽： UPDATE daily_hit_counter as c -&gt; INNER JOIN ( -&gt; SELECT day, SUM(cnt) AS cnt, MIN(slot) AS mslot -&gt; FROM daily_hit_counter -&gt; GROUP BY day -&gt; ) AS X USING(day) -&gt; SET c.cnt = IF(c.slot = x.mslot, x.cnt, 0), -&gt; c.slot = IF(c.slot = x.mslot, 0, c.slot);mysql&gt; DELETE FROM daily_hit_counter WHERE slot &lt;&gt; 0 AND cnt = 0;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"GET与POST的区别","slug":"GET与POST的区别","date":"2018-05-15T06:55:30.000Z","updated":"2019-03-08T13:18:32.530Z","comments":true,"path":"2018/05/15/GET与POST的区别/","link":"","permalink":"http://yoursite.com/2018/05/15/GET与POST的区别/","excerpt":"","text":"HTTP定义了一组请求方法, 以表明要对给定资源执行的操作。而硕果仅存的只剩两个半（笑）。实际开发中我们常用到的一般是POST和GET，极少数情况下会用到PUT。RFC7231规范了GET方法用于请求一个指定资源的表示形式（transfer a current representation of the target resource），而POST方法用于将实体提交到指定的资源（Perform resource-specific processing on the request payload）。但仅仅了解规范是不够的，很多工具比如chrome,nginx有它自己履行规范的方式，从开发角度看，或许这些更有价值。 参数GET传递的参数只能带URL后面，文本格式QueryString，长度受限于浏览器发送长度和服务器接收长度。各家标准不一，作为开发人员宜选取其中最短一个（2083字节）作为开发标准，以避免不必要的兼容问题。 IE(Browser) 2,083 Bytes Firefox(Browser) 65,536 Bytes Safari(Browser) 80,000 Bytes Opera(Browser) 190,000 Bytes Google (chrome) 8,182 Bytes Apache (Server) 8,182 Bytes Microsoft Internet Information Server(IIS) 16,384 Bytes POST的参数就比较灵活，可以传递application/x-www-form-urlencoded的类似QueryString、multipart/form-data的二进制报文格式（支持文件信息嵌入报文传输）、纯文本或二进制的body参数。很多时候我们把参数写在body里，这时参数没有长度上的限制。 幂等幂等是一个计算机术语，表示重复执行某一操作得到的结果总是相同的。在HTTP中，如果我们说一个HTTP方法是幂等的，指的是同样的请求被执行一次与连续执行多次的效果是一样的，服务器的状态也是一样的。 GET是幂等的，我们使用GET获取服务器上同一份数据，拿到的数据应该都是相同的。每次请求后服务器的状态应该也是相同的（统计数据除外）。 POST不是幂等的，多次POST返回的结果不一定相同，每次请求后服务器的状态也是不一样的。 缓存GET时默认可以复用前面的请求数据作为缓存结果返回，此时以完整的URL作为缓存数据的KEY。所以有时候为了强制每次请求都是新数据，我们可以在URL后面加上一个随机参数或时间戳或版本号来避免从缓存中读取，也可以直接设置Cache-Control禁用缓存。 POST则一般不会被这些缓存因素影响。 安全性服务器的日志比如像nginx的access log会自动记录GET和POST的URL，包括其中带的参数，但不会记录请求的报文。对于一些敏感数据，POST更安全一些。 自动化性能测试基于上面提到的nginx日志，可以使用grep GET+日期，awk格式化，然后sort -u去重，从而提取到某天的所有GET请求URL，使用程序模拟登陆，然后请求所有URL即可获取简单的性能测试数据，每个请求是否正确，响应时间多少等等。 但是对于POST请求，因为不知道报文，无法这样简单处理。可以通过nginx-lua获取报文输出到log，这样格式化会麻烦很多，但不失为一个办法。 TCP包的数量GET请求稳定只发送一个包，而POST请求在某些浏览器里会发送两个，具体的原因还在探究。 IE 6 – 2 packets IE 7 – 2 packets IE 8 – 2 packets Firefox 3.0.13 – 1 packet Firefox 3.5.2 – 1 packet Opera 9.27 – 2 packets Safari 4.0.3 – 2 packets Chrome 2.0.172.43 – 2 packets 参考资料1、URL最大长度问题2、xmlhttprequest-xhr-uses-multiple-packets-for-http-post3、comparing-get-and-post","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/tags/计算机网络/"}],"keywords":[]},{"title":"idea常用快捷键备忘","slug":"idea常用快捷键备忘","date":"2018-05-08T07:00:32.000Z","updated":"2019-03-08T13:14:41.060Z","comments":true,"path":"2018/05/08/idea常用快捷键备忘/","link":"","permalink":"http://yoursite.com/2018/05/08/idea常用快捷键备忘/","excerpt":"","text":"记录一些开发中常用但不容易记住的快捷键。 CtrlCtrl + Ｙ 删除行Ctrl + Ｒ 替换Ctrl + Ｆ 当前代码中查找Ctrl + Ｗ 选中光标所在的单词，连按范围扩大Ctrl + －／＝ 折叠／展开当前光标所在代码和注释Ctrl + Ｉ 接口方法补全 Ctrl + AltCtrl + Alt + Ｌ 代码格式化Ctrl + Alt + Ｔ 选中的地方代码环绕提示Ctrl + Alt + 空格 类名或接口名提示Ctrl + Alt + 方向左／右 回退／前进到上一个操作的地方Ctrl + Alt + Ｂ 查看一个方法的实现Ctrl + Alt + Ｏ 清除多余包 Ctrl + ShiftCtrl + Shift + Ｕ 大／小写转换Ctrl + Shift + －／＝ 折叠／展开当前文件下所有代码和注释 AltAlt + 回车 智能提示Alt + 方向上／下 上／下一方法Alt + Insert 类方法补全 Shift双击Shift 在项目的所有目录查找特定内容Shift ＋ F6 重命名类，方法，变量","categories":[],"tags":[{"name":"备忘录","slug":"备忘录","permalink":"http://yoursite.com/tags/备忘录/"}],"keywords":[]},{"title":"Java9新特性概述","slug":"Java9新特性概述","date":"2018-04-11T02:51:48.000Z","updated":"2019-03-08T13:14:56.316Z","comments":true,"path":"2018/04/11/Java9新特性概述/","link":"","permalink":"http://yoursite.com/2018/04/11/Java9新特性概述/","excerpt":"","text":"Java9正式发布于2017年9月21日。作为Java8之后3年半才发布的新版本，Java9带来了很多重大的变化。其中最重要的改动是Java平台模块系统的引入。除此之外，还有一些新的特性。本文对Java9中包含的新特性做了概括性的介绍，可以帮助你快速了解Java9。 Java平台模块系统Java平台模块系统，也就是Project Jigsaw，把模块化开发实践引入到了Java平台中。在引入了模块系统之后，JDK 被重新组织成94个模块。Java应用可以通过新增的jlink工具，创建出只包含所依赖的JDK模块的自定义运行时镜像。这样可以极大的减少Java运行时环境的大小。这对于目前流行的不可变基础设施的实践来说，镜像的大小的减少可以节省很多存储空间和带宽资源。 模块化开发的实践在软件开发领域并不是一个新的概念。Java开发社区已经使用这样的模块化实践有相当长的一段时间。主流的构建工具，包括Apache Maven和Gradle都支持把一个大的项目划分成若干个子项目。子项目之间通过不同的依赖关系组织在一起。每个子项目在构建之后都会产生对应的JAR文件。在Java9中，已有的这些项目可以很容易的升级转换为Java9模块，并保持原有的组织结构不变。 Java9模块的重要特征是在其工件（artifact）的根目录中包含了一个描述模块的module-info.class文件。工件的格式可以是传统的JAR文件或是Java9新增的JMOD文件。这个文件由根目录中的源代码文件module-info.java编译而来。该模块声明文件可以描述模块的不同特征。模块声明文件中可以包含的内容如下： 模块导出的包：使用exports可以声明模块对其他模块所导出的包。包中的public和protected类型，以及这些类型的public和protected成员可以被其他模块所访问。没有声明为导出的包相当于模块中的私有成员，不能被其他模块使用。 模块的依赖关系：使用requires可以声明模块对其他模块的依赖关系。使用requires transitive可以把一个模块依赖声明为传递的。传递的模块依赖可以被依赖当前模块的其他模块所读取。如果一个模块所导出的类型的型构中包含了来自它所依赖的模块的类型，那么对该模块的依赖应该声明为传递的。 服务的提供和使用：如果一个模块中包含了可以被ServiceLocator发现的服务接口的实现，需要使用provides with语句来声明具体的实现类；如果一个模块需要使用服务接口，可以使用uses语句来声明。 代码清单1中给出了一个模块声明文件的示例。在该声明文件中，模块com.mycompany.sample导出了Java包 com.mycompany.sample。该模块依赖于模块 com.mycompany.sample。该模块也提供了服务接口 com.mycompany.common.DemoService的实现类com.mycompany.sample.DemoServiceImpl。 清单1.模块声明示例：123456module com.mycompany.sample &#123; exports com.mycompany.sample; requires com.mycompany.common; provides com.mycompany.common.DemoService with com.mycompany.sample.DemoServiceImpl; &#125; 模块系统中增加了模块路径的概念。模块系统在解析模块时，会从模块路径中进行查找。为了保持与之前Java 版本的兼容性，CLASSPATH依然被保留。所有的类型在运行时都属于某个特定的模块。对于从CLASSPATH中加载的类型，它们属于加载它们的类加载器对应的未命名模块。可以通过Class的getModule()方法来获取到表示其所在模块的Module对象。 在JVM启动时，会从应用的根模块开始，根据依赖关系递归的进行解析，直到得到一个表示依赖关系的图。如果解析过程中出现找不到模块的情况，或是在模块路径的同一个地方找到了名称相同的模块，模块解析过程会终止，JVM也会退出。Java也提供了相应的API与模块系统进行交互。 Jshelljshell是Java9新增的一个实用工具。jshell为Java增加了类似NodeJS和Python中的读取-求值-打印循环（ Read-Evaluation-Print Loop ）。在 jshell中可以直接输入表达式并查看其执行结果。当需要测试一个方法的运行效果，或是快速的对表达式进行求值时，jshell都非常实用。只需要通过jshell命令启动jshell，然后直接输入表达式即可。每个表达式的结果会被自动保存下来，以数字编号作为引用，类似$1和$2这样的名称。可以在后续的表达式中引用之前语句的运行结果。在jshell中，除了表达式之外，还可以创建Java类和方法。jshell也有基本的代码完成功能。 在代码清单2中，我们直接创建了一个方法add。 清单2.在jshell中添加方法：1234jshell&gt; int add(int x, int y) &#123; ...&gt; return x + y; ...&gt; &#125; | created method add(int,int) 接着就可以在jshell中直接使用这个方法，如代码清单3所示。 清单3.在jshell中使用创建的方法：12jshell&gt; add(1, 2) $19 ==&gt; 3 集合、Stream和Optional在集合上，Java9增加了List.of()、Set.of()、Map.of()和Map.ofEntries()等工厂方法来创建不可变集合，如代码清单4所示。 清单4.创建不可变集合：12345678List.of(); List.of(\"Hello\", \"World\"); List.of(1, 2, 3);Set.of(); Set.of(\"Hello\", \"World\"); Set.of(1, 2, 3);Map.of();Map.of(\"Hello\", 1, \"World\", 2); Stream中增加了新的方法ofNullable、dropWhile、takeWhile和iterate。在代码清单5中，流中包含了从1到 5的元素。断言检查元素是否为奇数。第一个元素1被删除，结果流中包含4个元素。 清单5.Stream中的dropWhile方法示例：1234567@Test public void testDropWhile() throws Exception &#123; final long count = Stream.of(1, 2, 3, 4, 5) .dropWhile(i -&gt; i % 2 != 0) .count(); assertEquals(4, count); &#125; Collectors中增加了新的方法filtering和flatMapping。在代码清单6中，对于输入的String流，先通过 flatMapping把String映射成Integer流，再把所有的Integer收集到一个集合中。 清单6.Collectors的flatMapping方法示例：1234567@Test public void testFlatMapping() throws Exception &#123; final Set&lt;Integer&gt; result = Stream.of(\"a\", \"ab\", \"abc\") .collect(Collectors.flatMapping(v -&gt; v.chars().boxed(), Collectors.toSet())); assertEquals(3, result.size()); &#125; Optional类中新增了ifPresentOrElse、or和stream等方法。在代码清单7中，Optional流中包含3个元素，其中只有2个有值。在使用flatMap之后，结果流中包含了2个值。 清单7.Optional的stream方法示例：12345678910@Test public void testStream() throws Exception &#123; final long count = Stream.of( Optional.of(1), Optional.empty(), Optional.of(2) ).flatMap(Optional::stream) .count(); assertEquals(2, count); &#125; 进程APIJava9增加了ProcessHandle接口，可以对原生进程进行管理，尤其适合于管理长时间运行的进程。在使用P rocessBuilder来启动一个进程之后，可以通过Process.toHandle()方法来得到一个ProcessHandle对象的实例。通过ProcessHandle可以获取到由ProcessHandle.Info表示的进程的基本信息，如命令行参数、可执行文件路径和启动时间等。ProcessHandle的onExit()方法返回一个CompletableFuture对象，可以在进程结束时执行自定义的动作。代码清单8中给出了进程API的使用示例。 清单8.进程API示例：123456789final ProcessBuilder processBuilder = new ProcessBuilder(\"top\").inheritIO(); final ProcessHandle processHandle = processBuilder.start().toHandle(); processHandle.onExit().whenCompleteAsync((handle, throwable) -&gt; &#123; if (throwable == null) &#123; System.out.println(handle.pid()); &#125; else &#123; throwable.printStackTrace(); &#125; &#125;); 平台日志API和服务Java9允许为JDK和应用配置同样的日志实现。新增的System.LoggerFinder用来管理JDK使用的日志记录器实现。JVM在运行时只有一个系统范围的LoggerFinder实例。LoggerFinder通过服务查找机制来加载日志记录器实现。默认情况下，JDK使用java.logging模块中的java.util.logging实现。通过LoggerFinder的getLogger()方法就可以获取到表示日志记录器的System.Logger实现。应用同样可以使用System.Logger来记录日志。这样就保证了JDK和应用使用同样的日志实现。我们也可以通过添加自己的System.LoggerFinder实现来让JDK和应用使用SLF4J等其他日志记录框架。代码清单9中给出了平台日志API的使用示例。 清单9.使用平台日志API：123456public class Main &#123; private static final System.Logger LOGGER = System.getLogger(\"Main\"); public static void main(final String[] args) &#123; LOGGER.log(Level.INFO, \"Run!\"); &#125; &#125; 反应式流（Reactive Streams）反应式编程的思想最近得到了广泛的流行。在Java平台上有流行的反应式库RxJava和Reactor。反应式流规范的出发点是提供一个带非阻塞负压（non-blocking backpressure）的异步流处理规范。反应式流规范的核心接口已经添加到了Java9中的java.util.concurrent.Flow类中。 Flow中包含了Flow.Publisher、Flow.Subscriber、Flow.Subscription和Flow.Processor等4个核心接口。Java9还提供了SubmissionPublisher作为Flow.Publisher的一个实现。RxJava2和Reactor都可以很方便的与Flow类的核心接口进行互操作。 变量句柄变量句柄是一个变量或一组变量的引用，包括静态域，非静态域，数组元素和堆外数据结构中的组成部分等。变量句柄的含义类似于已有的方法句柄。变量句柄由Java类java.lang.invoke.VarHandle来表示。可以使用类java.lang.invoke.MethodHandles.Lookup中的静态工厂方法来创建VarHandle对象。通过变量句柄，可以在变量上进行各种操作。这些操作称为访问模式。不同的访问模式尤其在内存排序上的不同语义。目前一共有31种访问模式，而每种访问模式都在VarHandle中有对应的方法。这些方法可以对变量进行读取、写入、原子更新、数值原子更新和比特位原子操作等。VarHandle还可以用来访问数组中的单个元素，以及把byte[]数组和ByteBuffer当成是不同原始类型的数组来访问。 在代码清单10中，我们创建了访问HandleTarget类中的域count的变量句柄，并在其上进行读取操作。 清单10.变量句柄使用示例： 123456789101112131415161718192021public class HandleTarget &#123; public int count = 1; &#125; public class VarHandleTest &#123; private HandleTarget handleTarget = new HandleTarget(); private VarHandle varHandle; @Before public void setUp() throws Exception &#123; this.handleTarget = new HandleTarget(); this.varHandle = MethodHandles .lookup() .findVarHandle(HandleTarget.class, \"count\", int.class); &#125; @Test public void testGet() throws Exception &#123; assertEquals(1, this.varHandle.get(this.handleTarget)); assertEquals(1, this.varHandle.getVolatile(this.handleTarget)); assertEquals(1, this.varHandle.getOpaque(this.handleTarget)); assertEquals(1, this.varHandle.getAcquire(this.handleTarget)); &#125; &#125; 改进方法句柄（Method Handle）类java.lang.invoke.MethodHandles增加了更多的静态方法来创建不同类型的方法句柄。 arrayConstructor：创建指定类型的数组。 arrayLength：获取指定类型的数组的大小。 varHandleInvoker和varHandleExactInvoker：调用VarHandle中的访问模式方法。 zero：返回一个类型的默认值。 empty：返回MethodType的返回值类型的默认值。 loop、countedLoop、iteratedLoop、whileLoop和doWhileLoop：创建不同类型的循环，包括for循环、while循环和do-while循环。 tryFinally：把对方法句柄的调用封装在try-finally语句中。在代码清单11中，我们使用iteratedLoop来创建一个遍历String类型迭代器的方法句柄，并计算所有字符串的长度的总和。 清单11. 循环方法句柄使用示例：123456789101112131415161718192021222324public class IteratedLoopTest &#123; static int body(final int sum, final String value) &#123; return sum + value.length(); &#125; @Test public void testIteratedLoop() throws Throwable &#123; final MethodHandle iterator = MethodHandles.constant( Iterator.class, List.of(\"a\", \"bc\", \"def\").iterator()); final MethodHandle init = MethodHandles.zero(int.class); final MethodHandle body = MethodHandles .lookup() .findStatic( IteratedLoopTest.class, \"body\", MethodType.methodType( int.class, int.class, String.class)); final MethodHandle iteratedLoop = MethodHandles .iteratedLoop(iterator, init, body); assertEquals(6, iteratedLoop.invoke()); &#125; &#125; 并发在并发方面，类CompletableFuture中增加了几个新的方法。completeAsync使用一个异步任务来获取结果并完成该CompletableFuture。orTimeout在CompletableFuture没有在给定的超时时间之前完成，使用TimeoutException异常来完成CompletableFuture。completeOnTimeout与orTimeout类似，只不过它在超时时使用给定的值来完成CompletableFuture。新的Thread.onSpinWait方法在当前线程需要使用忙循环来等待时，可以提高等待的效率。 NashornNashorn是Java8中引入的新的JavaScript引擎。Java9中的Nashorn已经实现了一些ECMAScript6规范中的新特性，包括模板字符串、二进制和八进制字面量、迭代器和for..of循环和箭头函数等。Nashorn还提供了API把ECMAScript源代码解析成抽象语法树（Abstract Syntax Tree，AST），可以用来对ECMAScript源代码进行分析。 I/O 流新特性类java.io.InputStream中增加了新的方法来读取和复制InputStream中包含的数据。 readAllBytes：读取InputStream中的所有剩余字节。readNBytes：从InputStream中读取指定数量的字节到数组中。transferTo：读取InputStream中的全部字节并写入到指定的OutputStream中。代码清单12中给出了这些新方法的使用示例。 清单12.InputStream中的新方法使用示例：1234567891011121314151617181920212223242526public class TestInputStream &#123; private InputStream inputStream; private static final String CONTENT = \"Hello World\"; @Before public void setUp() throws Exception &#123; this.inputStream = TestInputStream.class.getResourceAsStream(\"/input.txt\"); &#125; @Test public void testReadAllBytes() throws Exception &#123; final String content = new String(this.inputStream.readAllBytes()); assertEquals(CONTENT, content); &#125; @Test public void testReadNBytes() throws Exception &#123; final byte[] data = new byte[5]; this.inputStream.readNBytes(data, 0, 5); assertEquals(\"Hello\", new String(data)); &#125; @Test public void testTransferTo() throws Exception &#123; final ByteArrayOutputStream outputStream = new ByteArrayOutputStream(); this.inputStream.transferTo(outputStream); assertEquals(CONTENT, outputStream.toString()); &#125; &#125; ObjectInputFilter可以对ObjectInputStream中包含的内容进行检查，来确保其中包含的数据是合法的。可以使用ObjectInputStream的方法setObjectInputFilter来设置。ObjectInputFilter在进行检查时，可以检查如对象图的最大深度、对象引用的最大数量、输入流中的最大字节数和数组的最大长度等限制，也可以对包含的类的名称进行限制。 改进应用安全性能Java9新增了4个SHA-3哈希算法，SHA3-224、SHA3-256、SHA3-384和SHA3-512。另外也增加了通过java.security.SecureRandom生成使用DRBG算法的强随机数。代码清单13中给出了SHA-3哈希算法的使用示例。 清单13.SHA-3哈希算法使用示例：import org.apache.commons.codec.binary.Hex;public class SHA3 { public static void main(final String[] args) throws NoSuchAlgorithmException { final MessageDigest instance = MessageDigest.getInstance(“SHA3-224”); final byte[] digest = instance.digest(“”.getBytes()); System.out.println(Hex.encodeHexString(digest)); }} 用户界面类java.awt.Desktop增加了新的与桌面进行互动的能力。可以使用addAppEventListener方法来添加不同应用事件的监听器，包括应用变为前台应用、应用隐藏或显示、屏幕和系统进入休眠与唤醒、以及用户会话的开始和终止等。还可以在显示关于窗口和配置窗口时，添加自定义的逻辑。在用户要求退出应用时，可以通过自定义处理器来接受或拒绝退出请求。在AWT图像支持方面，可以在应用中使用多分辨率图像。 统一JVM日志Java9中，JVM有了统一的日志记录系统，可以使用新的命令行选项-Xlog来控制JVM上所有组件的日志记录。该日志记录系统可以设置输出的日志消息的标签、级别、修饰符和输出目标等。Java9移除了在Java8中被废弃的垃圾回收器配置组合，同时把G1设为默认的垃圾回收器实现。另外，CMS垃圾回收器已经被声明为废弃。Java9也增加了很多可以通过jcmd调用的诊断命令。 其他改动方面在Java语言本身，Java9允许在接口中使用私有方法。在try-with-resources语句中可以使用effectively-final变量。类java.lang.StackWalker可以对线程的堆栈进行遍历，并且支持过滤和延迟访问。Java9把对Unicode的支持升级到了8.0。ResourceBundle加载属性文件的默认编码从ISO-8859-1改成了UTF-8，不再需要使用native2ascii命令来对属性文件进行额外处理。注解@Deprecated也得到了增强，增加了since和forRemoval两个属性，可以分别指定一个程序元素被废弃的版本，以及是否会在今后的版本中被删除。 在代码清单14中，buildMessage是接口SayHi中的私有方法，在默认方法sayHi中被使用。 清单14.接口中私有方法的示例：123456789public interface SayHi &#123; private String buildMessage() &#123; return \"Hello\"; &#125; void sayHi(final String message); default void sayHi() &#123; sayHi(buildMessage()); &#125; &#125; 小结作为Java平台最新的一个重大更新，Java9中的很多新特性，尤其模块系统，对于Java应用的开发会产生深远的影响。本文对Java9中的新特性做了概括的介绍，可以作为了解Java9的基础。这些新特性的相关内容，可以通过官方文档来进一步的了解。 参考资源 (resources) 参考Java9官方文档，了解Java9的更多内容。 参考Java9官方Java文档，了解Java API的细节。 了解反应式流规范的更多内容。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"收藏","slug":"收藏","permalink":"http://yoursite.com/tags/收藏/"}],"keywords":[]},{"title":"Centos7下使用systemd管理服务","slug":"Centos7下使用systemd管理服务","date":"2018-03-30T13:09:37.000Z","updated":"2019-03-08T13:18:57.867Z","comments":true,"path":"2018/03/30/Centos7下使用systemd管理服务/","link":"","permalink":"http://yoursite.com/2018/03/30/Centos7下使用systemd管理服务/","excerpt":"","text":"Centos7新增了systemd用于为系统的启动和管理提供一套完整的解决方案，以替代原先的系统管理器system V init（SysVInit）。相比于SysVInit，systemd支持服务并行启动，从而使效率大大提高；同时它还具有日志管理、快速备份与恢复、挂载点管理等多种实用功能，是一套更完善的系统管理方案。服务器后端会经常有将mysql、redis、nginx等部件开机启动的需求，现在可以统一交给Systemd管理，方便许多。 Systemd概述在Linux系统中，我们经常会遇到结尾为’d’的进程，比如initd，mysqld。根据Linux惯例，字母‘d’是守护进程（daemon）的缩写。systemd这个名字的含义，就是它是整个系统的守护进程。在Centos7中，它是系统的第一个进程（PID等于1），创建于系统启动的过程中，其他进程都是它的子进程。 在Centos7中，系统的启动可以汇整成如下几个过程： ①、打开计算机电源，载入BIOS的硬件信息与进行自我测试，并依据设置取得第一个可开机的设备；②、读取并执行第一个开机设备内MBR的boot loader（亦即是grub2，spfdisk等程序）；③、依据boot loader的设置载入Kernel，Kernel会开始侦测硬件与载入驱动程序；④、在硬件驱动成功后，Kernel会主动调用systemd程序，并以default.target流程开机； 1) systemd执行sysinit.target初始化系统及basic.target准备操作系统； 2) systemd启动multi-user.target下的本机与服务器服务； 3) systemd执行multi-user.target下的/etc/rc.d/rc.local文件； 4) systemd执行multi-user.target下的getty.target及登录服务； 5) systemd执行graphical需要的服务（图形化版本特有） 大概就是上面这样子了。你会发现systemd出现的频率很高，这是因为systemd负责了开机时所有的资源（unit）调度任务，操作系统只需要启动systemd，剩下的systemd都会帮它处理。不仅如此，事实上，systemd扮演的就是一个资源管理者的角色，它最主要的功能就是准备软件执行的环境，包括系统的主机名称、网络设置、语系处理、文件系统格式及其他服务的启动等。 Systemd的基本概念和操作一、UnitSystemd可以管理所有系统资源。不同的资源统称为Unit（单元）。systemd 将资源归纳为以下一些不同的类型。然而，systemd正在快速发展，新功能不断增加。所以资源类型可能在不久的将来继续增加。 Service unit：系统服务（最常见）Target unit：多个 Unit 构成的一个组Device Unit：硬件设备Mount Unit：文件系统的挂载点Automount Unit：自动挂载点Path Unit：文件或路径Scope Unit：不是由 Systemd 启动的外部进程Slice Unit：进程组Snapshot Unit：Systemd 快照，可以切回某个快照Socket Unit：进程间通信的 socketSwap Unit：swap 文件Timer Unit：定时器 Unit配置文件每一个unit都有一个配置文件，配置文件一般存放在目录/usr/lib/systemd/system/，它会告诉systemd怎么启动这个unit。配置文件的后缀名，就是该unit的种类，比如sshd.socket。如果省略，systemd默认后缀名为.service，所以sshd会被理解成sshd.service。 上图为我系统中redis.service配置文件的内容。它大致包含了这些信息：1）对这个资源的描述；2）所需的前置资源；3）实际执行此service的指令或脚本程序；4）实际停止此service的指令或脚本程序；5）该资源所在的组（target）。 unit配置文件中常用的字段整理如下： Unit字段 参数意义说明 Description 就是当我们使用systemctl list-units时，会输出给管理员看的简易说明。使用systemctl status输出的此服务的说明，也是这个字段。 After 说明此unit是在哪个daemon启动之后才启动的意思。基本上仅是说明服务启动的顺序而已，并没有强制要求里头的服务一定要启动后此unit才能启动。 Before 与After的意义相反，是在什么服务启动前最好启动这个服务的意思。不过这仅是规范服务启动的顺序，并非强制要求的意思。 Requires 明确地定义此unit需要在哪个daemon启动后才能够启动。如果在此项设置的前导服务没有启动，那么此unit就不会被启动。 Wants 表示这个unit之后最好还要启动什么服务比较好的意思，不过并没有明确的规范。主要的目的是希望创建让使用者比较好操作的环境。因此，这个Wants后面接的服务如果没有启动，其实不会影响到这个unit本身。 Conflicts 代表冲突的服务。表示这个项目后面接的服务如果有启动，那么我们这个unit本身就不能启动！我们unit有启动，则此项目后的服务就不能启动！是一种冲突性的检查。 Service字段（service特有） 参数意义说明 Type Type字段定义启动类型。它可以设置的值如下：simple（默认值）：ExecStart字段启动的进程为主进程。forking：ExecStart字段将以fork()方式启动，此时父进程将会退出，子进程将成为主进程。oneshot：类似于simple，但只执行一次，systemd会等它执行完，才启动其他服务。dbus：类似于simple，但会等待D-Bus信号后启动。notify：类似于simple，启动结束后会发出通知信号，然后 systemd再启动其他服务。idle：类似于simple，但是要等到其他任务都执行完，才会启动该服务。一种使用场合是为让该服务的输出，不与其他服务的输出相混合。 EnvironmentFile 可以指定启动脚本的环境配置文件！例如sshd.service的配置文件写入到 /etc/sysconfig/sshd 当中！你也可以使用Environment=后面接多个不同的Shell变量来给予设置！ ExecStart 就是实际执行此daemon的指令或脚本程序。你也可以使用ExecStartPre（之前）以及ExecStartPost（之后）两个设置项目来在实际启动服务前，进行额外的指令行为。但是你得要特别注意的是，指令串仅接受“指令 参数 参数…”的格式，不能接受&lt;,&gt;,&gt;&gt;,&amp;等特殊字符，很多的bash语法也不支持喔！所以，要使用这些特殊的字符时，最好直接写入到指令脚本里面去！不过，上述的语法也不是完全不能用，亦即，若要支持比较完整的bash语法，那你得要使用Type=oneshot才行喔！其他的Type才不能支持这些字符。 ExecStop 与systemctl stop的执行有关，关闭此服务时所进行的指令。 ExecReload 与systemctl reload有关的指令行为。 Restart 当设置Restart=1时，则当此daemon服务终止后，会再次的启动此服务。 RemainAfterExit 当设置为RemainAfterExit=1时，则当这个daemon所属的所有程序都终止之后，此服务会再尝试启动。这对于 Type=oneshot 的服务很有帮助！ TimeoutSec 若这个服务在启动或者是关闭时，因为某些缘故导致无法顺利“正常启动或正常结束”的情况下，则我们要等多久才进入“强制结束”的状态！ KillMode 可以是process,control-group,none的其中一种，如果是process则daemon终止时，只会终止主要的程序（ExecStart 接的后面那串指令），如果是control-group时，则由此daemon所产生的其他control-group的程序，也都会被关闭。如果是none的话，则没有程序会被关闭。 RestartSec 与Restart有点相关性，如果这个服务被关闭，然后需要重新启动时，大概要sleep多少时间再重新启动的意思。默认是100ms（毫秒）。 Install字段 参数意义说明 WantedBy 这个设置后面接的大部分是*.target unit！意思是，这个unit本身是附挂在哪一个target unit下面的！一般来说，大多的服务性质的unit都是附挂在multi-user.target下面！ Also 当目前这个unit本身被enable时，Also后面接的unit也请enable的意思！也就是具有相依性的服务可以写在这里呢！ Alias 进行一个链接的别名的意思！当systemctl enable相关的服务时，则此服务会进行链接文件的创建/usr/lib/systemd/system/multi-user.target。 配置unit开机启动开机时，systemd默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。systemctl enable命令用于在上面两个目录之间，建立符号链接关系，相当于激活开机启动。与之对应的，systemctl disable命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。 启动和停止unit执行systemctl start命令启动软件，执行systemctl status命令查看该服务的状态 上面的输出结果含义如下： Loaded行：配置文件的位置，是否设为开机启动Drop-In行：符号链接地址Active行：表示正在运行Main PID行：主进程IDCGroup块：应用的所有子进程 当不需要服务继续运行时，可以执行systemctl stop命令终止正在运行的服务。有时候，该命令可能没有响应，服务停不下来，这时候可以执行systemctl kill命令强制终止。另外，需要重启服务时可以执行systemctl restart命令。 二、Target启动计算机的时候，需要启动大量的unit。如果每一次启动，都要一一写明本次启动需要哪些unit，显然非常不方便。Systemd的解决方案就是target。 简单说，target 就是一个unit组，包含许多相关的unit 。启动某个target的时候，systemd就会启动里面所有的unit。从这个意义上说，target这个概念类似于”状态点”，启动某个target就好比启动到某种状态。 传统的init启动模式里面，有runlevel的概念，跟target的作用很类似。不同的是，runlevel是互斥的，不可能多个runlevel同时启动，但是多个target可以同时启动。 查看target我们可以执行指令systemctl list-unit-files --type=target查看当前系统的所有target。也可以执行指令systemctl list-dependencies multi-user.target查看一个target包含的所有unit。 系统启动时systemctl会根据/etc/systemd/system/default.target规划的策略进行启动，我们可以通过执行指令systemctl get-default查看启动时默认的target（一般是multi-user.target）。指令systemctl set-default可以设置启动时的默认target。切换target时，默认不关闭前一个target启动的进程，我们可以通过systemctl isolate关闭前一个target里面所有不属于后一个target的进程。 三、日志管理Systemd 统一管理所有unit的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。 查看日志我们可以通过指令journalctl查看所有日志（默认情况下 ，只保存本次启动的日志）。journalctl -k可以查看内核日志，journalctl -b和journalctl -b -0可以查看系统本次启动的日志，journalctl -b -1可以查看上一次启动的日志，journalctl _PID=X查看指定进程的日志，journalctl /usr/bin/bash查看某个路径的脚本的日志，journalctl -u redis.service和journalctl -u redis.service --since today查看某个unit的日志。 Systemd的特性（对比SysVInit）为了保证运行在先前Linux版本上的应用程序运行稳定，systemd兼容了原先的SysVInit以及LSB initscripts，但也引入了新的特性。这使得系统中已经存在的服务和进程无需修改，降低了系统向systemd迁移的成本。但我们也应该了解systemd所做的改变，以更好的适应当前的版本。大体而言，systemd相比SysVInit更改了以下几个方面： 一、支持并行启动系统启动时，需要启动很多启动项目，在SysVInit中，每一个启动项目都由一个独立的脚本负责，它们由 SysVinit顺序地，串行地调用。因此总的启动时间是各脚本运行时间之和。而systemd通过socket/D-Bus activation等技术，能够将启动项目同时并行启动，大大提高了系统的启动速度。 二、提供按需启动能力当 sysvinit 系统初始化的时候，它会将所有可能用到的后台服务进程全部启动运行。并且系统必须等待所有的服务都启动就绪之后，才允许用户登录。这种做法有两个缺点：首先是启动时间过长；其次是系统资源浪费。 某些服务很可能在很长一段时间内，甚至整个服务器运行期间都没有被使用过。比如 CUPS，打印服务在多数服务器上很少被真正使用到。您可能没有想到，在很多服务器上 SSHD 也是很少被真正访问到的。花费在启动这些服务上的时间是不必要的；同样，花费在这些服务上的系统资源也是一种浪费。 Systemd 可以提供按需启动的能力，只有在某个服务被真正请求的时候才启动它。当该服务结束，systemd 可以关闭它，等待下次需要时再次启动它。 三、采用Linux的Cgroup特性跟踪和管理进程的生命周期Init系统的一个重要职责就是负责跟踪和管理服务进程的生命周期。它不仅可以启动一个服务，也必须也能够停止服务。这看上去没有什么特别的，然而在真正用代码实现的时候，我们会发现有时候停止服务比一开始想的要困难。 服务进程一般都会作为守护进程（daemon）在后台运行，为此服务程序有时候会派生(fork)两次。在SysVInit中，需要在配置文件中正确地配置expect小节。这样SysVInit通过对fork系统调用进行计数，从而获知真正的守护进程的PID号。 还有更加特殊的情况。比如，一个CGI程序会派生两次，从而脱离了和Apache的父子关系。当Apache进程被停止后，该CGI程序还在继续运行。而我们希望服务停止后，所有由它所启动的相关进程也被停止。 为了处理这类问题，SysVInit通过strace来跟踪 fork、exit 等系统调用，但是这种方法很笨拙，且缺乏可扩展性。Systemd则利用了Linux内核的特性即CGroup来完成跟踪的任务。当停止服务时，通过查询CGroup，Systemd可以确保找到所有的相关进程，从而干净地停止服务。 CGroup已经出现了很久，它主要用来实现系统资源配额管理。CGroup提供了类似文件系统的接口，使用方便。当进程创建子进程时，子进程会继承父进程的 CGroup。因此无论服务如何启动新的子进程，所有的这些相关进程都会属于同一个 CGroup，systemd只需要简单地遍历指定的CGroup即可正确地找到所有的相关进程，将它们一一停止即可。 四、启动挂载点和自动挂载的管理传统的Linux系统中，用户可以用/etc/fstab文件来维护固定的文件系统挂载点。这些挂载点在系统启动过程中被自动挂载，一旦启动过程结束，这些挂载点就会确保存在。这些挂载点都是对系统运行至关重要的文件系统，比如HOME目录。和SysVInit 一样，Systemd会管理这些挂载点，以便能够在系统启动时自动挂载它们。Systemd兼容了/etc/fstab文件，我们可以继续使用该文件管理挂载点。 有时候用户还需要动态挂载点，比如打算访问DVD内容时，才临时执行挂载以便访问其中的内容，而不访问光盘时该挂载点被取消(umount)，以便节约资源。传统地，人们依赖autofs服务来实现这种功能。 Systemd内建了自动挂载服务，无需另外安装autofs服务，可以直接使用systemd提供的自动挂载管理能力来实现autofs的功能。 五、实现事务性依赖关系管理系统启动过程是由很多的独立工作共同组成的，这些工作之间可能存在依赖关系，比如挂载一个NFS文件系统必须依赖网络能够正常工作。Systemd 虽然能够最大限度地并发执行很多有依赖关系的工作，但是类似”挂载 NFS”和”启动网络”这样的工作还是存在天生的先后依赖关系，无法并发执行。对于这些任务，systemd 维护一个”事务一致性”的概念，保证所有相关的服务都可以正常启动而不会出现互相依赖，以至于死锁的情况。 六、能够对系统进行快照和恢复Systemd支持按需启动，因此系统的运行状态是动态变化的，人们无法准确地知道系统当前运行了哪些服务。Systemd快照提供了一种将当前系统运行状态保存并恢复的能力。 比如系统当前正运行服务A和B，可以用systemd命令行对当前系统运行状况创建快照。然后将进程A停止，或者做其他的任意的对系统的改变，比如启动新的进程C。在这些改变之后，运行systemd的快照恢复命令，就可立即将系统恢复到快照时刻的状态，即只有服务A，B在运行。一个可能的应用场景是调试：比如服务器出现一些异常，为了调试用户将当前状态保存为快照，然后可以进行任意的操作，比如停止服务等等。等调试结束，恢复快照即可。 这个快照功能目前在systemd中并不完善，似乎开发人员也没有特别关注它，因此有报告指出它还存在一些使用上的问题，使用时尚需慎重。 七、日志服务systemd自带日志服务journald，该日志服务的设计初衷是克服现有的syslog服务的缺点。比如： syslog不安全，消息的内容无法验证。每一个本地进程都可以声称自己是Apache PID 4711，而syslog也就相信并保存到磁盘上。 数据没有严格的格式，非常随意。自动化的日志分析器需要分析人类语言字符串来识别消息。一方面此类分析困难低效；此外日志格式的变化会导致分析代码需要更新甚至重写。 Systemd Journal用二进制格式保存所有日志信息，用户使用journalctl命令来查看日志信息。无需自己编写复杂脆弱的字符串分析处理程序。 Systemd Journal的优点如下： 简单性：代码少，依赖少，抽象开销最小。 零维护：日志是除错和监控系统的核心功能，因此它自己不能再产生问题。举例说，自动管理磁盘空间，避免由于日志的不断产生而将磁盘空间耗尽。 移植性：日志文件应该在所有类型的Linux系统上可用，无论它使用的何种CPU或者字节序。 性能：添加和浏览日志非常快。 最小资源占用：日志数据文件需要较小。 统一化：各种不同的日志存储技术应该统一起来，将所有的可记录事件保存在同一个数据存储中。所以日志内容的全局上下文都会被保存并且可供日后查询。例如一条固件记录后通常会跟随一条内核记录，最终还会有一条用户态记录。重要的是当保存到硬盘上时这三者之间的关系不会丢失。Syslog将不同的信息保存到不同的文件中，分析的时候很难确定哪些条目是相关的。 扩展性：日志的适用范围很广，从嵌入式设备到超级计算机集群都可以满足需求。 安全性：日志文件是可以验证的，让无法检测的修改不再可能。 总结Systemd作为Centos7最新采用的系统管理进程，相比前任有相当多的改变。它的优点是功能强大，使用方便，缺点是过于复杂，与操作系统的其他部分强耦合，可能在某种程度上违背了Linux原本”keep simple, keep stupid”设计哲学。但从一个系统使用者的角度，它的确在很多方面做得都要比它的前任更好。作为一个后端，我们需要对这些改变有所了解，才能将这个系统用得更好。 参考资料1、鸟哥的Linux私房菜：基础学习篇 第四版 第十九章2、Systemd 入门教程 - 阮一峰的网络日志3、CentOS / RHEL 7 : How to set default target (default runlevel)4、IBM developerWorks Systemed","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}],"keywords":[]},{"title":"简单的密码学生成唯一邀请码","slug":"简单的密码学生成唯一邀请码","date":"2018-03-23T09:10:23.000Z","updated":"2019-03-08T13:18:40.929Z","comments":true,"path":"2018/03/23/简单的密码学生成唯一邀请码/","link":"","permalink":"http://yoursite.com/2018/03/23/简单的密码学生成唯一邀请码/","excerpt":"","text":"最近项目需要生成用户邀请码，网上找了几个算法都不太满意，就自己写了一个。里面借鉴了一些密码学里的思路，最后的算法效果还不错。想把思路记录下来，可以用在类似对加密强度要求不高的场合下。 需求分析从业务需求和市面上其它产品邀请码的使用体验上来看，邀请码有以下几个强制性的要求： 不可重复 唯一确定 这两点要求首先就排除了hash code的可能，因为hash code是可以发生碰撞的。然后在强制性要求的基础之上，我们还有一些进一步的需求： 长度不能太长，6-10位是合适的区间 不容易被推测出 资源消耗尽可能小 在这些需求的约束下，我们先来看看常见的通用的序列码生成算法。 通用方案通用方案的解决思路可以分为两种：一种是生成一串不重复随机数，然后将其保存到数据库里。使用邀请码时从数据库里查询就可以得到邀请人；另一种是对身份信息作加密，通常是用户id，将加密后的密文作为邀请码，使用时可以不查询数据库，直接解密得到。理论上说，第二种方式稍好一点，可以少进行一次数据库查询。但是考虑到安全性，我们还是会把解密后的id拿到数据库中查询，防止有人输错或者伪造邀请码产生NPE。因此在选择算法的时候，这两种思路我都有考虑到。 1、UUID谈到不重复的随机数，最先想到的自然是UUID。UUID是一种软件构建的标准，也是开放软件基金会组织（OSF）在分布式计算环境领域的一部分。按照OSF制定的标准计算，它用到了以太网卡地址、纳秒级时间、芯片ID码和许多可能的数字，保证对在同一时空中的所有机器都是唯一的。Java的工具类java.util.UUID是Java提供的一整套UUID生成方案，对于开发者来说可以很方便的调用。然而UUID并不适合用在这里，因为UUID的位数是固定的32位，这个对于我们的邀请码来说显然是太长了（想象一下用户面对面分享邀请码的时候居然需要报一串32位的数字+字母）。 网上也有用UUID的一部分当随机数的，但UUID只能保证完整的32位是不会重复的，不能保证其中的某一段不重复，因此这个方案也行不通。 2、系统当前时间系统当前时间也是一种常见的随机数生成方案。它的做法是先获取到系统当前时间，再用它和某个时间点对比，将这两段时间的间隔以毫秒或者纳秒为单位存到内存中去。最后我们程序获取到的是一串数字。Java提供了两个系统函数用于实现这个功能：System.currentTimeMillis()和System.nanoTime()。然而这两个系统函数在这个业务里都有各自的问题。 System.currentTimeMillis()返回的是从1970.1.1 UTC零点开始到现在的时间，精确到毫秒。它的问题在于不能支持高并发的邀请码生成。在这套方案中，只要我们的系统在某1秒内生成的邀请码超过32个，那么出现相同邀请码的概率就超过50%（详见生日攻击）。显然，这个规模的并发量是不能接受的。 System.nanoTime()返回的是从某个固定但随意的时间点开始的时间，精确到纳秒。这个固定但随意的时间，在不同JVM中是不一样的。这也就是说不同计算机计算出来的nanoTime()是有可能重合的。甚至同一台计算机重启JVM后生成的nanoTime()也是可能重合的。这违背了我们的第一个要求。 3、RC4算法RC4对于学过密码的同学来说肯定不会陌生。它是大名鼎鼎的RSA三人组中的头号人物Ronald Rivest在1987年设计的一种轻量级对称加密算法。它的特点是按字节流加密，也就是说明文多长，密文就多长。这一特点很好避免了UUID只能生成32位字符串的尴尬。而且RC4是一个轻量级加密算法，运行速度快，占用资源少，很好地满足了我们的第5点要求。乍一看RC4似乎是种理想的方案，然而实际一跑就出现了问题： 出现了乱码！这是因为字符的取值在0~255，而我们熟悉的英文和数字只占了其中的62位，其它符号是我们不熟悉的，当然也不能作为邀请码。解决方法也很简单，把字符串转成16进制即可： 由于把8位的字符串转成了4位的16进制，字符串的长度增加了一倍，但长度尚在可接受范围之内。不太满意的一点是加密后的密文都是连续性的，高位的数字基本不变。这也意味着如果被邀请的同学输错了后几位数字，后台大概率检测不到他的这次操作失误，因为他输入的错误邀请码能在数据库里被找到。而且连续的密文容易被找出规律，安全性较低。因此这种方式也不建议。 4、用户身份标志+随机数这种方法是我在网上找到的已经被用于实际业务中的方法，它的大致思路是这样： 获取用户身份的唯一标志，比如用户ID。 将用户ID补全，补全的位数取决于你希望得到的邀请码长度，如：106可以补全为00106. 随机生成一串大写字母串，长度和补全后的用户ID相同，如：SZUDF。 将随机数隔位插入用户ID，得到邀请码：S0Z0U1D0F6。 这种方式得到的邀请码基本能满足我们的要求：由用户ID的唯一性保证了邀请码的唯一性；随机生成的字母串又能保证不容易被找到规律，同时又提高了用户操作的容错率；长度也在可接受范围内。因此第一版的邀请码生成算法我们采用了这种方式。 但是它仍然有改进的空间。①、字母和数字的位置是固定的，有一定的容易被察觉的规律，且对于数字来说，仍然具有连续性；②、用户ID直接暴露在密文中，存在风险；③、没有校验位，邀请码的校验依赖于数据库，无法对恶意伪造大量错误邀请码的攻击进行有效防御。 因此我在这种算法上作了改进，克服了以上的缺点。 我的方案为了让字母和数字的位置不再固定，我将用户ID作了36进制转换，即把用户ID映射为一串字母+数字的组合，高位用0补全。 123456int[] b = new int[CODE_LENGTH];b[0] = id;for (int i = 0; i &lt; 5; ++i) &#123; b[i + 1] = b[i] / 36; b[i] = (b[i]) % 36;&#125; 同时把随机数生成的范围扩大到字母和数字，这样密文中的每一位都可能是数字和字母，规律性就不易察觉得多。 然后是用户ID暴露在密文中的问题。这个问题的解决办法是我们可以加一点盐。盐的取值最好不要太小，太小缺乏隐蔽性；也不能太大，太大会占用过多用户ID的取值空间。具体的取值取决于业务需求。 当然，最后是校验位的问题。这个问题我思考之后决定在随机数上作文章。目前的算法，会生成和补全后用户ID长度相等的随机数。这有两点问题：一是邀请码长度稍显过长，6位用户ID就会产生12位的邀请码；二是随机数没有提供额外的信息，这对密文来说是一种资源浪费。鉴于此，我改变了随机数的生成方式，让它不再随机生成，而是承担起对密文其它部分的校验功能。同时改变了它的长度，把它固定在2位。当然，缩短后的校验码就没有办法隔位插入，我就把它放在了密文尾部。用这一套校验方式，理论上能保证99.9%的误操作可以被后台检测出来而不需要查询数据库。 生成的邀请码如上，相比第一版，可以看到一些很明显的改进。而且理论上可以容纳1000万的用户量，比第一版的10万位有了很大提升。 但是这一版的算法仍有问题，细心的同学会发现6个验证码的2~5位是一样的。这是因为低位的变化不足以影响到高位，导致高位的字符没有发生变化。这样的算法在安全性上是比较薄弱的，攻击人可以利用这一规律大大降低猜测的区间。而且密文和密钥（超参数，本文中就是salt和prime1）之间的关系比较直接，没有进行进一步的处理。现代密码学认为，密码的安全性应该由密钥来保障而不是加密算法，如果密钥和密文之间的联系过于直接，密码的安全性便会削弱。当然，密码学上对这些问题有解决方法，那就是扩散和混淆。 扩散和混淆扩散(diffusion)和混淆(confusion)是C.E.Shannon提出的设计密码体制的两种基本方法，其目的是为了抵抗对手对密码体制的统计分析。在分组密码的设计中，充分利用扩散和混淆，可以有效地抵抗对手从密文的统计特性推测明文或密钥。扩散和混淆是现代分组密码的设计基础。 所谓扩散就是让明文中的每一位影响密文中的许多位，或者说让密文中的每一位受明文中的许多位的影响。这样可以隐蔽明文的统计特性。当然，理想的情况是让明文中的每一位影响密文中的所有位，或者说让密文中的每一位受明文中所有位的影响。 所谓混淆就是将密文与密钥之间的统计关系变得尽可能复杂，使得对手即使获取了关于密文的一些统计特性，也无法推测密钥。使用复杂的非线性代替变换可以达到比较好的混淆效果，而简单的线性代替变换得到的混淆效果则不理想。可以用”揉面团”来形象地比喻扩散和混淆。当然，这个”揉面团”的过程应该是可逆的。乘积和迭代有助于实现扩散和混淆。选择某些较简单的受密钥控制的密码变换，通过乘积和迭代可以取得比较好的扩散和混淆的效果。 改进后的算法我用扩散和混淆的方式对算法进行了改进。 扩散的方式很简单，只需要将个位和其它每一位作和后取余，即可把变化传导到每一位。为了隐蔽，我还把变化进行了放大：1id = id * PRIME1; PRIME1可以为任意随机数，最好和36以及10^n（n为用户id位数）互质。这是因为根据循环群的性质：若 m 和 p 互质，则( id * m ) % p的结果遍历[0, p)的所有整数。保证了放大后结果的分布和原数据的分布同样均匀。为了使结果看起来更随机，我还给每一位分配了不同系数： 12345678910id = id * PRIME1;id = id + SALT;int[] b = new int[CODE_LENGTH];b[0] = id;for (int i = 0; i &lt; 5; ++i) &#123; b[i + 1] = b[i] / ARY; b[i] = (b[i] + b[0] * i) % ARY;&#125;b[5] = (b[0] + b[1] + b[2]) * PRIME1 % ARY;b[6] = (b[3] + b[4] + b[5]) * PRIME1 % ARY; ARY表示进制，这里是36，也可以设置成其它的数，比如62（字母区分大小写）。代码的第7、9、10行中我分别对每一位设置了不同的系数，使得每一次的增量显得更不固定。 然后是混淆。混淆我用了P-box的方式，其实就是将数字洗牌。比如把1234567洗成5237641。这样处理之后可以隐藏密钥和密文之间的关系。洗牌的方式也很简单，选择一个和CODE_LENGTH（本文中为7）互质的数PRIME2，和数组角标相乘取余即可（原理同PRIME1）。最终的代码如下： 1234567891011121314151617public static String inviCodeGenerator(int id) &#123; id = id * PRIME1; id = id + SALT; int[] b = new int[CODE_LENGTH]; b[0] = id; for (int i = 0; i &lt; 5; ++i) &#123; b[i + 1] = b[i] / ARY; b[i] = (b[i] + b[0] * i) % ARY; &#125; b[5] = (b[0] + b[1] + b[2]) * PRIME1 % ARY; b[6] = (b[3] + b[4] + b[5]) * PRIME1 % ARY; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; sb.append(HEX_36_Array.charAt(b[(i * PRIME2) % CODE_LENGTH])); &#125; return sb.toString();&#125; 测试结果如下： 完美符合我们的需求^_^ 邀请码和用户ID的转换也很简单，因为加密的过程都是可逆的，所以只需将加密过程作逆变换即可。这里要提一点就是我们是设置了校验位的，所以可以在解密的过程中对邀请码进行校验，如果是用户的误输入或者有人企图构造邀请码恶意攻击，我们在业务层就可以检测出来，不需要拿到数据层去做校验。具体的解密代码如下： 1234567891011121314151617181920212223242526272829303132public static int inviDecoding(String inviCode) &#123; if (inviCode.length() != CODE_LENGTH) &#123; return -1; &#125; int res = 0; int a[] = new int[CODE_LENGTH]; int b[] = new int[CODE_LENGTH]; char[] c = new char[CODE_LENGTH]; for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; a[(i * PRIME2) % CODE_LENGTH] = i; &#125; for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; c[i] = inviCode.charAt(a[i]); &#125; for (int i = 0; i &lt; CODE_LENGTH; ++i) &#123; a[i] = HEX_36_Array.indexOf(c[i]); &#125; b[5] = (a[0] + a[1] + a[2]) * PRIME1 % ARY; b[6] = (a[3] + a[4] + a[5]) * PRIME1 % ARY; if (a[5] != b[5] || a[6] != b[6]) &#123; return -1; &#125; for (int i = 4; i &gt;= 0; --i) &#123; b[i] = (a[i] - a[0] * i + ARY * i) % ARY; &#125; for (int i = 4; i &gt; 0; --i) &#123; res += b[i]; res *= ARY; &#125; res = ((res + b[0]) - SALT) / PRIME1; return res;&#125; 代码18~22行就是在作校验。 总结不同的业务有不同的需求，市面上通用的方案可能只能满足大部分共性的需求，但对于某些特定的需求，市面上找不到完善的解决方案，这时候就需要我们独立解决问题的能力。本科的时候觉得密码学没用，没想到在这用上了。越来越觉得世上没有无用的知识，多积累一些总是好的^_^","categories":[],"tags":[{"name":"密码学","slug":"密码学","permalink":"http://yoursite.com/tags/密码学/"}],"keywords":[]},{"title":"分布式缓存的一致性Hash算法","slug":"分布式缓存的一致性Hash算法","date":"2018-03-16T03:58:32.000Z","updated":"2019-03-08T13:19:36.902Z","comments":true,"path":"2018/03/16/分布式缓存的一致性Hash算法/","link":"","permalink":"http://yoursite.com/2018/03/16/分布式缓存的一致性Hash算法/","excerpt":"","text":"分布式有利于提高网站的可用性、伸缩性和安全性。分布式缓存，顾名思义，就是将缓存服务器作分布式配置，提高集群性能和可伸缩性。然而对分布式缓存集群而言，不能像应用服务器一样使用简单的负载均衡手段来实现，因为分布式缓存服务器集群中不同服务器中缓存的数据各不相同，缓存访问请求不可以在缓存服务器集群中的任意一台处理，必须先找到缓存有需要数据的服务器，然后才能访问。这个特点会严重制约分布式缓存集群的伸缩性设计，因为新上线的缓存服务器没有缓存任何数据，而已下线的缓存服务器还缓存着网站的许多热点数据。 分布式缓存算法的主要设计目标，就是在保证负载均衡的同时，尽可能让新上线的缓存服务器对整个分布式缓存集群影响最小，也就是说新加入缓存服务器后应使整个缓存服务器集群中已经缓存的数据尽可能还被访问到，同时新增服务器对原有服务器的影响要尽可能均衡。 余数hash算法分布式缓存的算法要保证对于一个确定数据，它所在的服务器也必须是确定的。比如对于键值对&lt;’BEIJING’,DATA&gt;，每一次查找’BEIJING’这个关键词，系统总是访问相同的服务器去读取数据。这样，只要服务器还缓存着该数据，就能保证命中。 余数hash是一个不错的办法:用服务器数目除缓存数据KEY的hash值，余数为服务器列表下标编号。假设我们有三台服务器，要存键值对&lt;’BEIJING’,DATA&gt;到缓存。则先计算’BEIJING’的hash值是490806430（Java中的HashCode()返回值），用服务器数目3除该值，得到余数1，将其保存到NODE1上，以后想要读取数据’BEIJING’的时候，只要服务器数量不变，一定会定位NODE1上。同时，由于HashCode具有随机性，使用余数hash算法可保证缓存数据在整个缓存服务器集群中比较均匀地分布。 对余数Hash算法稍加改进，还能满足对不同硬件性能的服务器集群作负载均衡的需求。比如3台服务器中，第2台服务器的性能是另外2台的2倍，这时我们可以调整算法，把除数设置为4，当余数为1、2的时候，将数据存入NODE2，实现加权负载均衡。 事实上，如果不需要考虑缓存服务器集群伸缩性，余数hash几乎可以满足绝大多数的缓存路由需求。 余数hash算法的不足然而，当考虑到缓存服务器集群伸缩性的时候，余数hash算法的不足就暴露出来了。假设由于业务发展，网站需要将3台缓存服务器扩容至4台。这时用户再次访问’BEIJING’这个数据的时候，除数变成了4，用4除‘BEIJING’的Hash值490806430，余数为2，对应NODE2。由于数据&lt;’BEIJING’,DATA&gt;缓存在NODE1，对NODE2的读缓存操作失败，缓存没有命中。 很容易就可以计算出，3台服务器扩容至4台的时候，大约只能命中25%的缓存（3/4），随着服务器集群规模的增大，这个比例线性上升。当100台服务器的集群加入一台新服务器，不能命中的概率是99%（N/(N+1)）。 这个结果显然是无法接受的，因此我们需要改进这种算法，提高增加新机器后的缓存命中率。 一致性hash算法一致性hash算法通过一个叫作一致性hash环的数据结构提高了新增机器后的缓存命中率，如下图： 具体算法过程为：先构造一个长度为2^32的整数环（这个环被称作一致性hash环），根据节点名称的hash值（其分布范围为[0,2^32-1]）将缓存服务器节点放置在这个hash环上。然后根据需要缓存的数据的KEY值计算得到其hash值（其分布范围也同样为[0,2^32-1]），然后在hash环上顺时针查找距离这个KEY的hash值最近的缓存服务器节点，完成KEY到服务器的hash映射查找。 在上图中，假设NODE1的hash值为3594963423，NODE2的hash值为1845328979，而KEY0的hash值为2534256785，那么KEY0在环上顺时针查找，找到的最近的节点就是NODE1。 当缓存服务器集群需要扩容的时候，只需要将新加入的节点名称（NODE3）的hash值放入一致性hash环中，由于KEY是顺时针查找距离其最近的节点，因此新加入的节点只影响整个环中的一小段，如下图中加粗的一段： 假设NODE3的hash值是2790324235，那么加入NODE3后，KEY0（hash值2534256785）顺时针查找得到的节点就是NODE3。 如上图所示，加入新节点NODE3后，原来的KEY大部分还能继续计算到原来的节点，只有KEY3、KEY0从原来的NODE1重新计算到NODE3。这样就能保证大部分被缓存的数据还可以继续命中。3台服务器扩容至4台，命中率可以达到75%，远高于余数hash的25%，而且随着集群规模增大，继续命中原有缓存数据的概率也逐渐增大，100台服务器扩容增加1台，继续命中的概率是99%。虽然仍有小部分数据缓存在服务器中不能被读到，但这个比例在可接受范围之内。 一致性hash算法的不足虽然上述的算法使缓存服务器集群在增加新服务器后的命中率有了大幅提高，但还存在一个小小的问题。 新加入的节点NODE3只影响了原来的节点NODE1，也就是说一部分原来需要访问NODE1的缓存数据现在需要访问NODE3（概率上是50%）。但是原来的节点NODE0和NODE2不受影响，这也就意味着，新引入的NODE3这个节点只减轻了NODE1的压力，假设原先三个节点的压力是一样大的，那么在引入NODE3这个节点后，NODE0和NODE2的缓存数据量和负载压力是NODE1与NODE3的两倍。这个是有违我们负载均衡的初衷的。 怎么办？ 改！ 改进后的一致性hash算法我们可以通过增加一层虚拟层的方式解决这个问题：将每台物理缓存服务器虚拟为一组虚拟缓存服务器，将虚拟缓存服务器的hash值放置在hash环上，KEY在环上先找到虚拟服务器节点，再得到物理服务器的信息。 这样新加入物理服务器节点时，是将一组虚拟节点加入环中，如果虚拟节点的数目足够多，这组虚拟节点将会影响同样多数目的已经在环上存在的虚拟节点，这些已经存在的虚拟节点又对应不同的物理节点。最终的结果是：新加入一台缓存服务器，将会较为均匀地影响原来集群中已经存在的所有服务器，也就是说分摊原有缓存服务器集群中所有服务器的一小部分负载，其总的影响范围和上面讨论过的相同。如下图所示： 显然每个物理节点对应的虚拟节点越多，各个物理节点之间的负载越均衡，新加入物理服务器对原有的物理服务器的影响越保持一致（这就是一致性hash这个名称的由来）。那么在实践中，一台物理服务器虚拟为多少个虚拟服务器节点合适呢？太多会影响性能，太少又会导致负载不均衡，一般说来，经验值是150，当然根据集群规模和负载均衡的精度需求，这个值应该根据具体情况具体对待。","categories":[],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/tags/读书笔记/"}],"keywords":[]},{"title":"MySQL索引原理","slug":"MySQL索引原理","date":"2018-03-13T02:48:28.000Z","updated":"2019-03-08T13:15:38.304Z","comments":true,"path":"2018/03/13/MySQL索引原理/","link":"","permalink":"http://yoursite.com/2018/03/13/MySQL索引原理/","excerpt":"","text":"数据库索引是面试中的常考项，也是日常开发中提高程序可用性的实用技巧。我在美团技术点评团队中找到了这篇文章《MySQL索引原理及慢查询优化》，摘录了其中专讲索引原理的部分，以供随时复习。 索引目的索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到m字母，然后从下往下找到y字母，再找到剩下的sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到m开头的单词呢？或者ze开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？ 索引原理除了词典，生活中随处可见索引的例子，如火车站的车次表、图书的目录等。它们的原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询(&gt;、&lt;、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段……这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。但如果是1千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。 磁盘IO与预读前面提到了访问磁盘，那么这里先简单介绍一下磁盘IO和预读，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分，寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。下图是计算机硬件延迟的对比图，供大家参考：考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。 索引的数据结构前面讲了生活中索引的例子，索引的基本原理，数据库的复杂性，又讲了操作系统的相关知识，目的就是让大家了解，任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘IO次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。 详解b+树如上图，是一颗b+树，关于b+树的定义可以参见B+树，这里只说一些重点，浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块1包含数据项17和35，包含指针P1、P2、P3，P1表示小于17的磁盘块，P2表示在17和35之间的磁盘块，P3表示大于35的磁盘块。真实的数据存在于叶子节点即3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如17、35并不真实存在于数据表中。 b+树的查找过程如图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。 b+树性质1.通过上面的分析，我们知道IO次数取决于b+数的高度h，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有h=㏒(m+1)N，当数据量N一定的情况下，m越大，h越小；而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。2.当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"},{"name":"收藏","slug":"收藏","permalink":"http://yoursite.com/tags/收藏/"}],"keywords":[]},{"title":"MySQL作图建库","slug":"MySQL作图建库","date":"2018-02-03T03:59:36.000Z","updated":"2019-03-08T13:19:14.116Z","comments":true,"path":"2018/02/03/MySQL作图建库/","link":"","permalink":"http://yoursite.com/2018/02/03/MySQL作图建库/","excerpt":"","text":"workbench是MySQL自带的数据库可视化工具，相比另一款常用的数据库可视化工具Navicat最大的优势就是它是免费的。两者的操作逻辑没有太大区别，不过workbench英文的界面对初学者不是太友好。我花了一点时间学习了如何作图，希望能为以后节省时间。 E-R图是描述数据库关系最常用的方式。优点是直观、详尽。workbench当然也提供了这种图表的绘制方式，不过在里面叫EER图。经过我的一番考证，这个EER图就是我们常说的E-R图。 一、已有数据库，自动生成EER图：①、首先在mysql workbench里选中Database——&gt; reverse engineering ②、然后选择你建立的连接（也就是数据库） ③、接下来一路next，直到最后选择导出的数据库 ④、自动生成的 E-R 图大概长相如图： 二、先画EER图，然后自动生成数据库：①、启动软件过后，注意不需要连接数据库（我第一次就是直接连接数据库了所以找不到设计ER模型的地方） ②、点击”+” ,进入模型设计界面 ③、双击 Add Diagram 进入如下设计界面 ④、点击工具栏表格，并在设计区域点击，就会出现一个table1 并双击它 ⑤、最后 执行 “File”-&gt;”Export” 按钮，选择 Forward Engineer SQL CREATE Script (ctrl+shift+G). 这样就可以把模型导出为SQL脚本文件。现在执行这个SQL文件就OK了","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}],"keywords":[]},{"title":"Java中的异常","slug":"Java中的异常","date":"2017-12-29T11:43:59.000Z","updated":"2019-03-08T13:19:50.574Z","comments":true,"path":"2017/12/29/Java中的异常/","link":"","permalink":"http://yoursite.com/2017/12/29/Java中的异常/","excerpt":"","text":"异常是Java中非常基础但重要的一块内容，异常的概念理解起来简单，但系统讲起来却并不容易。专门抽了一个下午将它整理出来，以备所需。 一、异常的分类Java中的异常是指程序在编译或者运行中遇到的问题。 Java中的异常都继承自Throwable类。Throwable是java语言中所有错误和异常的超类，它表示可抛（万物皆可抛）。它有两个子类：Error和Exception。 Error：Error为错误，是程序无法处理的。如OutOfMemoryError、ThreadDeath等，出现这种情况只能听之任之，交由JVM处理。一般情况下JVM也没法子，只好终止线程。 Exception：Exception是程序可以处理的异常。它有很多子类，比如IOException,RuntimeException,SQLException等等。其中RuntimeException比较特殊，它表示程序运行中 发生的异常，在编译时可以不接受检查。而其它异常编译时就要接受检查，对于抛出异常的部分，要么throw给子类，要么用try…catch处理。 常见异常继承关系： 二、常见的异常记住常见的异常可以让我们更高效地调试程序代码，有助于提高开发效率。 runtimeException子类 ArrayIndexOutOfBoundsException：数组索引越界异常。当对数组的索引值为负数或大于等于数组大小时抛出 ArithmeticException：算术条件异常。譬如：整数除零等 NullPointerException：空指针异常。当应用试图在要求使用对象的地方使用了null时，抛出该异常。譬如：调用null对象的实例方法、访问 null对象的属性、计算null对象的长度、使用throw语句抛出null等等 ClassNotFoundException：找不到类异常。当应用试图根据字符串形式的类名构造类，而在遍历CLASSPAH之后找不到对应名称的class文件时，抛出该异常 NegativeArraySizeException：数组长度为负异常 IOException子类 IOException：操作输入流和输出流时可能出现的异常 EOFException：文件已结束异常 FileNotFoundException：文件未找到异常 其它Exception子类 ClassCastException：类型转换异常类 ArrayStoreException：数组中包含不兼容的值抛出的异常 SQLException：操作数据库异常类 三、异常处理的机制Java中异常抛出后有两种处理方式：try…catch…finally机制和throws继续抛出机制。 try…catch…finally机制try…catch…finally是java中的关键字，使用方式如下：1234567try&#123; 抛出异常的代码&#125;catch(异常类)&#123; 处理语句&#125;finally&#123; 处理完后执行语句&#125; 在使用try…catch…finally时，若try中某一语句抛出了异常，则try后面的代码会被屏蔽，直接进行catch中的语句。catch中语句执行到return(返回语句)时，会先看看有没有finally块，若有，则优先执行finally块中语句。如果catch块和finally块都有return语句，则执行finally块中的。 抛出异常的代码可能抛出多种异常，处理异常的catch也可以有多个catch，分别处理不同的异常。在执行时，会依次查找catch语句，直到找到第一个能catch某异常的代码块。 throw和throws机制throws用在方法声明中，表示这个方法将会抛出某一异常。使用该方法的时候必须对该异常进行处理（try…catch或throw）throw用在语句中，表示抛出一个异常。抛出异常后方法会出栈，方法中后面的代码将不会执行。 四、注意事项 throws只是再次抛出了某个异常，并没有真正处理异常。在使用中，需要有代码去真正处理抛出的异常 如果子类重写了父类的方法，则子类能够抛出的异常只能是父类的子集（父类所有异常类及它们的子类集合） 对于runtimeException异常及其子类，程序可以选择显式处理也可以不处理，交给程序调用者去处理；对于其它exception（编译时异常），程序必须要显式处理（try…catch）或抛出（throws），交给调用者处理 五、异常处理规约摘自阿里JAVA开发手册，个人觉得有助于良好的代码风格形成。 1.【强制】不要捕获Java类库中定义的继承自RuntimeException的运行时异常类，如：IndexOutOfBoundsException/NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。正例：if(obj != null) {…}反例：try { obj.method() } catch(NullPointerException e){…} 2.【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。 3.【强制】对大段代码进行try-catch，这是不负责任的表现。catch时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的catch尽可能进行区分异常类型，再做对应的异常处理。 4.【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。 5.【强制】有try块放到了事务代码中，catch异常后，如果需要回滚事务，一定要注意手动回滚事务。 6.【强制】finally块必须对资源对象、流对象进行关闭，有异常也要做try-catch。说明：如果JDK7，可以使用try-with-resources方式。 7.【强制】不能在finally块中使用return，finally块中的return返回后方法结束执行，不会再执行try块中的return语句。 8.【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。 9.【推荐】方法的返回值可以为null，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回null值。调用方需要进行null判断防止NPE问题。说明：本规约明确防止NPE是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败，运行时异常等场景返回null的情况。 10.【推荐】防止NPE，是程序员的基本修养，注意NPE产生的场景：1） 返回类型为包装数据类型，有可能是null，返回int值时注意判空。反例：public int f(){ return Integer对象}; 如果为null，自动解箱抛NPE。2） 数据库的查询结果可能为null。3） 集合里的元素即使isNotEmpty，取出的数据元素也可能为null。4） 远程调用返回对象，一律要求进行NPE判断。5） 对于Session中获取的数据，建议NPE检查，避免空指针。6） 级联调用obj.getA().getB().getC()；一连串调用，易产生NPE。 11.【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的http/api开放接口必须使用“错误码”；而应用内部推荐异常抛出；跨应用间RPC调用优先考虑使用Result方式，封装isSuccess、“错误码”、“错误简短信息”。说明：关于 RPC方法返回方式使用 Result方式的理由：1）使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。2）如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输的性能损耗也是问题。 12.【推荐】定义时区分unchecked/checked异常，避免直接使用RuntimeException抛出，更不允许抛出Exception或者Throwable，应使用有业务含义的自定义异常。推荐业界已定义过的自定义异常，如：DAOException/ServiceException等。 13.【参考】避免出现重复的代码（Don’t Repeat Yourself），即DRY原则。说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是共用模块。正例：一个类中有多个public方法，都需要进行数行相同的参数校验操作，这个时候请抽取：private boolean checkParam(DTO dto){…}","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[]}]}